<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>블로그 홈</title><meta name=keywords content><meta name=description content="VAE (송경우교수님 딥러닝강의 2022) https://www.youtube.com/watch?v=V-lWbJtNzTc&amp;list=PLeiav_J6JcY8iFItzNZ_6PMlz9W4_jz5J&amp;index=58
Generative model MLE의 관점에서 보았을 때, $p_\theta(x)$를 최대화 하는 파라미터를 찾는 것이다! 결국 목적함수는,
$$\theta^* = \arg\max\limits_{\theta}\cfrac{1}{N}\sum_{i=1}^N\log{p_\theta(x_i)}$$
이다.
Variational Inference 어떤 조건이 주어졌을 때의 확률($p(z|x)$)을 다루기 쉬운 확률분포($q(z)$)로 근사하는 것. 즉, log-likelihood($\log{p(x_i)}$)의 lower bound인 $L$를 maximize하면 결국에 $q_i(z)$가 $p(z|x_i)$와 가까워져, 실제 샘플에 대응하는 latent를 더 잘 뽑아줄 수 있게 된다.
이를 학습하기 위해 elbo를 maximize 하는 과정을 살펴보면,
i번째 샘플 데이터학습할 때 마다, $q_i$로부터 $\mu_i, \Sigma_i$를 얻어내고, 여기로부터의 분포에서 다시 $\hat{x}$를 뽑아내서($\theta$), $x_i$와 닮도록 학습한다."><meta name=author content="Me"><link rel=canonical href=https://dobe0715.github.io/posts/vae/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content><meta property="og:description" content="VAE (송경우교수님 딥러닝강의 2022) https://www.youtube.com/watch?v=V-lWbJtNzTc&amp;list=PLeiav_J6JcY8iFItzNZ_6PMlz9W4_jz5J&amp;index=58
Generative model MLE의 관점에서 보았을 때, $p_\theta(x)$를 최대화 하는 파라미터를 찾는 것이다! 결국 목적함수는,
$$\theta^* = \arg\max\limits_{\theta}\cfrac{1}{N}\sum_{i=1}^N\log{p_\theta(x_i)}$$
이다.
Variational Inference 어떤 조건이 주어졌을 때의 확률($p(z|x)$)을 다루기 쉬운 확률분포($q(z)$)로 근사하는 것. 즉, log-likelihood($\log{p(x_i)}$)의 lower bound인 $L$를 maximize하면 결국에 $q_i(z)$가 $p(z|x_i)$와 가까워져, 실제 샘플에 대응하는 latent를 더 잘 뽑아줄 수 있게 된다.
이를 학습하기 위해 elbo를 maximize 하는 과정을 살펴보면,
i번째 샘플 데이터학습할 때 마다, $q_i$로부터 $\mu_i, \Sigma_i$를 얻어내고, 여기로부터의 분포에서 다시 $\hat{x}$를 뽑아내서($\theta$), $x_i$와 닮도록 학습한다."><meta property="og:type" content="article"><meta property="og:url" content="https://dobe0715.github.io/posts/vae/"><meta property="og:image" content="https://dobe0715.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dobe0715.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content><meta name=twitter:description content="VAE (송경우교수님 딥러닝강의 2022) https://www.youtube.com/watch?v=V-lWbJtNzTc&amp;list=PLeiav_J6JcY8iFItzNZ_6PMlz9W4_jz5J&amp;index=58
Generative model MLE의 관점에서 보았을 때, $p_\theta(x)$를 최대화 하는 파라미터를 찾는 것이다! 결국 목적함수는,
$$\theta^* = \arg\max\limits_{\theta}\cfrac{1}{N}\sum_{i=1}^N\log{p_\theta(x_i)}$$
이다.
Variational Inference 어떤 조건이 주어졌을 때의 확률($p(z|x)$)을 다루기 쉬운 확률분포($q(z)$)로 근사하는 것. 즉, log-likelihood($\log{p(x_i)}$)의 lower bound인 $L$를 maximize하면 결국에 $q_i(z)$가 $p(z|x_i)$와 가까워져, 실제 샘플에 대응하는 latent를 더 잘 뽑아줄 수 있게 된다.
이를 학습하기 위해 elbo를 maximize 하는 과정을 살펴보면,
i번째 샘플 데이터학습할 때 마다, $q_i$로부터 $\mu_i, \Sigma_i$를 얻어내고, 여기로부터의 분포에서 다시 $\hat{x}$를 뽑아내서($\theta$), $x_i$와 닮도록 학습한다."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://dobe0715.github.io/posts/"},{"@type":"ListItem","position":3,"name":"","item":"https://dobe0715.github.io/posts/vae/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"","name":"","description":"VAE (송경우교수님 딥러닝강의 2022) https://www.youtube.com/watch?v=V-lWbJtNzTc\u0026amp;list=PLeiav_J6JcY8iFItzNZ_6PMlz9W4_jz5J\u0026amp;index=58\nGenerative model MLE의 관점에서 보았을 때, $p_\\theta(x)$를 최대화 하는 파라미터를 찾는 것이다! 결국 목적함수는,\n$$\\theta^* = \\arg\\max\\limits_{\\theta}\\cfrac{1}{N}\\sum_{i=1}^N\\log{p_\\theta(x_i)}$$\n이다.\nVariational Inference 어떤 조건이 주어졌을 때의 확률($p(z|x)$)을 다루기 쉬운 확률분포($q(z)$)로 근사하는 것. 즉, log-likelihood($\\log{p(x_i)}$)의 lower bound인 $L$를 maximize하면 결국에 $q_i(z)$가 $p(z|x_i)$와 가까워져, 실제 샘플에 대응하는 latent를 더 잘 뽑아줄 수 있게 된다.\n이를 학습하기 위해 elbo를 maximize 하는 과정을 살펴보면,\ni번째 샘플 데이터학습할 때 마다, $q_i$로부터 $\\mu_i, \\Sigma_i$를 얻어내고, 여기로부터의 분포에서 다시 $\\hat{x}$를 뽑아내서($\\theta$), $x_i$와 닮도록 학습한다.","keywords":[],"articleBody":"VAE (송경우교수님 딥러닝강의 2022) https://www.youtube.com/watch?v=V-lWbJtNzTc\u0026list=PLeiav_J6JcY8iFItzNZ_6PMlz9W4_jz5J\u0026index=58\nGenerative model MLE의 관점에서 보았을 때, $p_\\theta(x)$를 최대화 하는 파라미터를 찾는 것이다! 결국 목적함수는,\n$$\\theta^* = \\arg\\max\\limits_{\\theta}\\cfrac{1}{N}\\sum_{i=1}^N\\log{p_\\theta(x_i)}$$\n이다.\nVariational Inference 어떤 조건이 주어졌을 때의 확률($p(z|x)$)을 다루기 쉬운 확률분포($q(z)$)로 근사하는 것. 즉, log-likelihood($\\log{p(x_i)}$)의 lower bound인 $L$를 maximize하면 결국에 $q_i(z)$가 $p(z|x_i)$와 가까워져, 실제 샘플에 대응하는 latent를 더 잘 뽑아줄 수 있게 된다.\n이를 학습하기 위해 elbo를 maximize 하는 과정을 살펴보면,\ni번째 샘플 데이터학습할 때 마다, $q_i$로부터 $\\mu_i, \\Sigma_i$를 얻어내고, 여기로부터의 분포에서 다시 $\\hat{x}$를 뽑아내서($\\theta$), $x_i$와 닮도록 학습한다.\n이 때, 각 데이터 샘플마다 뮤와 시그마에 대응시키는 파라미터가 필요하다.\n-\u003e 너무 많다… 새로운 데이터 들어올때마다 또 추가해야한다.\nAmortized Variational Inference $x_i$에 대응되는 $z_i$가 존재하는 상황 N개의 데이터가 있다면,,, 원래는 N개의 파라미터를 대응시켜서 학습시켰다..(n개의 튜플만들어서 각 튜플마다 파라미터로서 바꾸는느낌) 이걸 차라리 하나의 네트워크를 통해 mapping을 시켜주겠다!!(보통 DNN에서 입력, 출력 원하는 방향으로 하듯이) 수식으로 보면, $q_i(z) \\approx q(z|x_i)$ 왼쪽거 대신 오른쪽거로 VI를 하겠다는 뜻 데이터 하나마다 학습되는 과정을 수식으로 바라보면,\n$x_i$-\u003e NN($q_\\phi(z|x)$) -\u003e $\\mu(x_i), \\sigma(x_i)$ -\u003e $z=\\mu(x_i) + \\epsilon\\sigma(x_i)$ -\u003e NN($p_\\theta(x|z)$) -\u003e $\\hat{x} \\approx x$\n코드 실습 import torch import torch.nn as nn from torch.nn import functional as F import torchvision import torchvision.transforms as transforms from torchvision.utils import save_image from PIL import Image device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") device device(type='cuda') data_set = torchvision.datasets.MNIST('./data', train=True, transform=transforms.Compose([ transforms.Resize((32, 32)), transforms.ToTensor() ]), download=True, ) data_set Dataset MNIST Number of datapoints: 60000 Root location: ./data Split: Train StandardTransform Transform: Compose( Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=warn) ToTensor() ) data_loader = torch.utils.data.DataLoader(dataset=data_set, batch_size=100) 모델구성 mnist data : 28x28 network : channel : 4, 8, 16, 32 / WxH : 28-14-7-4-2 -\u003e fc1 4 -\u003e 1(mu), fc2 4 -\u003e 1(var) class VAE(nn.Module): def __init__(self, in_channels, latent_dim, hidden_dims = None, **kwargs): super(VAE, self).__init__() self.latent_dim = latent_dim modules = [] if hidden_dims is None: hidden_dims = [16, 32, 64, 128] # Build Encoder for h_dim in hidden_dims: modules.append( nn.Sequential( nn.Conv2d(in_channels, out_channels=h_dim, kernel_size=3, stride=2, padding=1), nn.BatchNorm2d(h_dim), nn.LeakyReLU() ) ) in_channels = h_dim self.encoder = nn.Sequential(*modules) self.fc_mu = nn.Linear(hidden_dims[-1]*4, latent_dim) self.fc_var = nn.Linear(hidden_dims[-1]*4, latent_dim) # Build Decoder modules = [] self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1]*4) hidden_dims.reverse() for i in range(len(hidden_dims) - 1): modules.append( nn.Sequential( nn.ConvTranspose2d(hidden_dims[i], hidden_dims[i+1], kernel_size=3, stride=2, padding=1, output_padding=1), nn.BatchNorm2d(hidden_dims[i+1]), nn.LeakyReLU() ) ) self.decoder = nn.Sequential(*modules) self.final_layer = nn.Sequential( nn.ConvTranspose2d(hidden_dims[-1], hidden_dims[-1], kernel_size=3, stride=2, padding=1, output_padding=1), nn.BatchNorm2d(hidden_dims[-1]), nn.LeakyReLU(), nn.Conv2d(hidden_dims[-1], out_channels=1, kernel_size=3, padding=1), nn.Sigmoid() ) def encode(self, input): result = self.encoder(input) result = torch.flatten(result, start_dim=1) mu = self.fc_mu(result) log_var = self.fc_var(result) return [mu, log_var] def decode(self, z): result = self.decoder_input(z) result = result.view(-1, 128, 2, 2) result = self.decoder(result) result = self.final_layer(result) return result def reparameterize(self, mu, logvar): std = torch.exp(0.5*logvar) eps = torch.randn_like(std) return eps*std + mu def loss_function(self, *args): recons = args[0] input = args[1] mu = args[2] log_var = args[3] recons_loss = F.mse_loss(recons, input) kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu**2 - log_var.exp(), dim=1), dim=0) loss = recons_loss + kld_loss return loss def sample(self, num_samples, current_device, **kwargs): z = torch.randn(num_samples, self.latent_dim) z = z.to(current_device) samples = self.decode(z) return samples def generate(self, x, **kwargs): return self.forward(x)[0] model = VAE(in_channels=1, latent_dim=200).to(device) optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.5, 0.999)) num = 0 for epoch in range(30): for i, (images, _) in enumerate(data_loader): # forward x = images.to(device) mu, log_var = model.encode(x) z = model.reparameterize(mu, log_var) x_rec = model.decode(z) # compute loss loss = model.loss_function(x_rec, x, mu, log_var) # backward optimizer.zero_grad() loss.backward() optimizer.step() if i % 100 == 0: print(f\"epoch : {epoch+1} | iter : {i} | loss : {loss:.4}\") if i % 200 == 0: samples = model.sample(1, device) save_image(samples, f\"./vae_samples/sample{num}.png\") num += 1 epoch : 1 | iter : 0 | loss : 25.12 epoch : 1 | iter : 100 | loss : 0.2628 epoch : 1 | iter : 200 | loss : 0.2504 epoch : 1 | iter : 300 | loss : 0.1924 epoch : 1 | iter : 400 | loss : 0.1042 epoch : 1 | iter : 500 | loss : 0.2347 epoch : 2 | iter : 0 | loss : 0.08283 epoch : 2 | iter : 100 | loss : 0.06895 epoch : 2 | iter : 200 | loss : 0.07953 epoch : 2 | iter : 300 | loss : 0.07523 epoch : 2 | iter : 400 | loss : 0.07775 epoch : 2 | iter : 500 | loss : 0.08221 epoch : 3 | iter : 0 | loss : 0.0736 epoch : 3 | iter : 100 | loss : 0.06282 epoch : 3 | iter : 200 | loss : 0.07073 epoch : 3 | iter : 300 | loss : 0.05984 epoch : 3 | iter : 400 | loss : 0.06127 epoch : 3 | iter : 500 | loss : 0.06909 epoch : 4 | iter : 0 | loss : 0.06461 epoch : 4 | iter : 100 | loss : 0.06142 epoch : 4 | iter : 200 | loss : 0.07161 epoch : 4 | iter : 300 | loss : 0.05889 epoch : 4 | iter : 400 | loss : 0.05847 epoch : 4 | iter : 500 | loss : 0.06437 epoch : 5 | iter : 0 | loss : 0.06036 epoch : 5 | iter : 100 | loss : 0.06125 epoch : 5 | iter : 200 | loss : 0.07911 epoch : 5 | iter : 300 | loss : 0.06195 epoch : 5 | iter : 400 | loss : 0.05938 epoch : 5 | iter : 500 | loss : 0.06209 epoch : 6 | iter : 0 | loss : 0.07897 epoch : 6 | iter : 100 | loss : 0.06053 epoch : 6 | iter : 200 | loss : 0.07334 epoch : 6 | iter : 300 | loss : 0.07139 epoch : 6 | iter : 400 | loss : 0.06082 epoch : 6 | iter : 500 | loss : 0.05961 epoch : 7 | iter : 0 | loss : 0.0998 epoch : 7 | iter : 100 | loss : 0.05946 epoch : 7 | iter : 200 | loss : 0.06843 epoch : 7 | iter : 300 | loss : 0.06251 epoch : 7 | iter : 400 | loss : 0.05751 epoch : 7 | iter : 500 | loss : 0.05803 epoch : 8 | iter : 0 | loss : 0.09001 epoch : 8 | iter : 100 | loss : 0.05908 epoch : 8 | iter : 200 | loss : 0.06798 epoch : 8 | iter : 300 | loss : 0.05804 epoch : 8 | iter : 400 | loss : 0.05706 epoch : 8 | iter : 500 | loss : 0.05436 epoch : 9 | iter : 0 | loss : 0.07401 epoch : 9 | iter : 100 | loss : 0.0589 epoch : 9 | iter : 200 | loss : 0.06516 epoch : 9 | iter : 300 | loss : 0.05802 epoch : 9 | iter : 400 | loss : 0.07783 epoch : 9 | iter : 500 | loss : 0.05494 epoch : 10 | iter : 0 | loss : 0.06096 epoch : 10 | iter : 100 | loss : 0.05923 epoch : 10 | iter : 200 | loss : 0.06546 epoch : 10 | iter : 300 | loss : 0.05692 epoch : 10 | iter : 400 | loss : 0.05633 epoch : 10 | iter : 500 | loss : 0.05238 epoch : 11 | iter : 0 | loss : 0.05586 epoch : 11 | iter : 100 | loss : 0.05847 epoch : 11 | iter : 200 | loss : 0.0642 epoch : 11 | iter : 300 | loss : 0.05614 epoch : 11 | iter : 400 | loss : 0.05614 epoch : 11 | iter : 500 | loss : 0.05214 epoch : 12 | iter : 0 | loss : 0.05507 epoch : 12 | iter : 100 | loss : 0.05837 epoch : 12 | iter : 200 | loss : 0.06383 epoch : 12 | iter : 300 | loss : 0.05589 epoch : 12 | iter : 400 | loss : 0.05605 epoch : 12 | iter : 500 | loss : 0.05205 epoch : 13 | iter : 0 | loss : 0.05482 epoch : 13 | iter : 100 | loss : 0.05831 epoch : 13 | iter : 200 | loss : 0.06379 epoch : 13 | iter : 300 | loss : 0.05581 epoch : 13 | iter : 400 | loss : 0.05601 epoch : 13 | iter : 500 | loss : 0.05198 epoch : 14 | iter : 0 | loss : 0.05462 epoch : 14 | iter : 100 | loss : 0.05825 epoch : 14 | iter : 200 | loss : 0.06375 epoch : 14 | iter : 300 | loss : 0.05574 epoch : 14 | iter : 400 | loss : 0.05597 epoch : 14 | iter : 500 | loss : 0.052 epoch : 15 | iter : 0 | loss : 0.05447 epoch : 15 | iter : 100 | loss : 0.05825 epoch : 15 | iter : 200 | loss : 0.06369 epoch : 15 | iter : 300 | loss : 0.05568 epoch : 15 | iter : 400 | loss : 0.05597 epoch : 15 | iter : 500 | loss : 0.05191 epoch : 16 | iter : 0 | loss : 0.0544 epoch : 16 | iter : 100 | loss : 0.05823 epoch : 16 | iter : 200 | loss : 0.06365 epoch : 16 | iter : 300 | loss : 0.0556 epoch : 16 | iter : 400 | loss : 0.05592 epoch : 16 | iter : 500 | loss : 0.0519 epoch : 17 | iter : 0 | loss : 0.0543 epoch : 17 | iter : 100 | loss : 0.05822 epoch : 17 | iter : 200 | loss : 0.06361 epoch : 17 | iter : 300 | loss : 0.05559 epoch : 17 | iter : 400 | loss : 0.0559 epoch : 17 | iter : 500 | loss : 0.0519 epoch : 18 | iter : 0 | loss : 0.05427 epoch : 18 | iter : 100 | loss : 0.05824 epoch : 18 | iter : 200 | loss : 0.06361 epoch : 18 | iter : 300 | loss : 0.05553 epoch : 18 | iter : 400 | loss : 0.05594 epoch : 18 | iter : 500 | loss : 0.05185 epoch : 19 | iter : 0 | loss : 0.05425 epoch : 19 | iter : 100 | loss : 0.05821 epoch : 19 | iter : 200 | loss : 0.06357 epoch : 19 | iter : 300 | loss : 0.05552 epoch : 19 | iter : 400 | loss : 0.05591 epoch : 19 | iter : 500 | loss : 0.0519 epoch : 20 | iter : 0 | loss : 0.05424 epoch : 20 | iter : 100 | loss : 0.05821 epoch : 20 | iter : 200 | loss : 0.06349 epoch : 20 | iter : 300 | loss : 0.0555 epoch : 20 | iter : 400 | loss : 0.0559 epoch : 20 | iter : 500 | loss : 0.05188 epoch : 21 | iter : 0 | loss : 0.05432 epoch : 21 | iter : 100 | loss : 0.05819 epoch : 21 | iter : 200 | loss : 0.06353 epoch : 21 | iter : 300 | loss : 0.0555 epoch : 21 | iter : 400 | loss : 0.05591 epoch : 21 | iter : 500 | loss : 0.05189 epoch : 22 | iter : 0 | loss : 0.05439 epoch : 22 | iter : 100 | loss : 0.05821 epoch : 22 | iter : 200 | loss : 0.06355 epoch : 22 | iter : 300 | loss : 0.05548 epoch : 22 | iter : 400 | loss : 0.0559 epoch : 22 | iter : 500 | loss : 0.05187 epoch : 23 | iter : 0 | loss : 0.05434 epoch : 23 | iter : 100 | loss : 0.0582 epoch : 23 | iter : 200 | loss : 0.06354 epoch : 23 | iter : 300 | loss : 0.05552 epoch : 23 | iter : 400 | loss : 0.05592 epoch : 23 | iter : 500 | loss : 0.05189 epoch : 24 | iter : 0 | loss : 0.0545 epoch : 24 | iter : 100 | loss : 0.05819 epoch : 24 | iter : 200 | loss : 0.06357 epoch : 24 | iter : 300 | loss : 0.05547 epoch : 24 | iter : 400 | loss : 0.05589 epoch : 24 | iter : 500 | loss : 0.05189 epoch : 25 | iter : 0 | loss : 0.05434 epoch : 25 | iter : 100 | loss : 0.05818 epoch : 25 | iter : 200 | loss : 0.06356 epoch : 25 | iter : 300 | loss : 0.05543 epoch : 25 | iter : 400 | loss : 0.0559 epoch : 25 | iter : 500 | loss : 0.05189 epoch : 26 | iter : 0 | loss : 0.0545 epoch : 26 | iter : 100 | loss : 0.05818 epoch : 26 | iter : 200 | loss : 0.06354 epoch : 26 | iter : 300 | loss : 0.05548 epoch : 26 | iter : 400 | loss : 0.05589 epoch : 26 | iter : 500 | loss : 0.05191 epoch : 27 | iter : 0 | loss : 0.05421 epoch : 27 | iter : 100 | loss : 0.05817 epoch : 27 | iter : 200 | loss : 0.06355 epoch : 27 | iter : 300 | loss : 0.0555 epoch : 27 | iter : 400 | loss : 0.05591 epoch : 27 | iter : 500 | loss : 0.05192 epoch : 28 | iter : 0 | loss : 0.05429 epoch : 28 | iter : 100 | loss : 0.05816 epoch : 28 | iter : 200 | loss : 0.06354 epoch : 28 | iter : 300 | loss : 0.05548 epoch : 28 | iter : 400 | loss : 0.0559 epoch : 28 | iter : 500 | loss : 0.0519 epoch : 29 | iter : 0 | loss : 0.05418 epoch : 29 | iter : 100 | loss : 0.05819 epoch : 29 | iter : 200 | loss : 0.06356 epoch : 29 | iter : 300 | loss : 0.05547 epoch : 29 | iter : 400 | loss : 0.05594 epoch : 29 | iter : 500 | loss : 0.05189 epoch : 30 | iter : 0 | loss : 0.05412 epoch : 30 | iter : 100 | loss : 0.05817 epoch : 30 | iter : 200 | loss : 0.06355 epoch : 30 | iter : 300 | loss : 0.05546 epoch : 30 | iter : 400 | loss : 0.05589 epoch : 30 | iter : 500 | loss : 0.05189 # class VAE(nn.Module): # def __init__(self, # input_feature, # latent_dim, # hidden_dim, # **kwargs): # super(VAE, self).__init__() # # Build Encoder # self.e_fc1 = nn.Sequential( # nn.Linear(input_feature, hidden_dim), # nn.BatchNorm2d(hidden_dim), # nn.LeakyReLU(0.2) # ) # self.e_fc2 = nn.Sequential( # nn.Linear(hidden_dim, hidden_dim), # nn.BatchNorm2d(hidden_dim), # nn.LeakyReLU(0.2) # ) # # output layer # self.mu = nn.Linear(hidden_dim, latent_dim) # self.var = nn.Linear(hidden_dim, latent_dim) # # Build Decoder # self.d_fc1 = nn.Sequential( # nn.Linear(latent_dim, hidden_dim), # nn.BatchNorm2d(hidden_dim), # nn.LeakyReLU(0.2) # ) # self.d_fc2 = nn.Sequential( # nn.Linear(hidden_dim, hidden_dim), # nn.BatchNorm2d(hidden_dim), # nn.LeakyReLU(0.2) # ) # self.fianl_layer = nn.Sequential( # nn.Linear(hidden_dim, input_feature), # nn.Tanh() # ) # def encode(self, input): # result = self.e_fc1(input) # result = self.e_fc2(result) # mu = self.mu(result) # log_var = self.var(result) # return [mu, log_var] # def decode(self, z): # result = self.d_fc1(z) # result = self.d_fc2(result) # result = self.final_layer(result) # return result # def reparameterize(self, mu, logvar): # std = torch.exp(0.5*logvar) # eps = torch.randn_like(std) # return eps*std + mu # def loss_function(self, # *args): # recons = args[0] # input = args[1] # mu = args[2] # log_var = args[3] # recons_loss = F.mse_loss(recons, input) # kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu**2 - log_var.exp(), dim=1), dim=0) # loss = recons_loss + kld_loss # return loss # def sample(self, # num_samples, # current_device, # **kwargs): # z = torch.randn(num_samples, # self.latent_dim) # z = z.to(current_device) # samples = self.decode(z) # samples = samples.view(-1, 28, 28) # return samples # def generate(self, x, **kwargs): # return self.forward(x)[0] ","wordCount":"2805","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://dobe0715.github.io/posts/vae/"},"publisher":{"@type":"Organization","name":"블로그 홈","logo":{"@type":"ImageObject","url":"https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dobe0715.github.io accesskey=h title="Home (Alt + H)"><img src=https://dobe0715.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dobe0715.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://dobe0715.github.io/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dobe0715.github.io>Home</a>&nbsp;»&nbsp;<a href=https://dobe0715.github.io/posts/>Posts</a></div><h1 class=post-title></h1><div class=post-meta>14 min&nbsp;·&nbsp;2805 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/VAE.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><h1 id=vae>VAE<a hidden class=anchor aria-hidden=true href=#vae>#</a></h1><p>(송경우교수님 딥러닝강의 2022) <a href="https://www.youtube.com/watch?v=V-lWbJtNzTc&amp;list=PLeiav_J6JcY8iFItzNZ_6PMlz9W4_jz5J&amp;index=58">https://www.youtube.com/watch?v=V-lWbJtNzTc&amp;list=PLeiav_J6JcY8iFItzNZ_6PMlz9W4_jz5J&amp;index=58</a></p><h2 id=generative-model>Generative model<a hidden class=anchor aria-hidden=true href=#generative-model>#</a></h2><ul><li>MLE의 관점에서 보았을 때, $p_\theta(x)$를 최대화 하는 파라미터를 찾는 것이다!</li></ul><p>결국 목적함수는,<br>$$\theta^* = \arg\max\limits_{\theta}\cfrac{1}{N}\sum_{i=1}^N\log{p_\theta(x_i)}$$</p><p>이다.</p><h4 id=variational-inference>Variational Inference<a hidden class=anchor aria-hidden=true href=#variational-inference>#</a></h4><ul><li>어떤 조건이 주어졌을 때의 확률($p(z|x)$)을 다루기 쉬운 확률분포($q(z)$)로 근사하는 것.</li></ul><p>즉, log-likelihood($\log{p(x_i)}$)의 lower bound인 $L$를 maximize하면 결국에 $q_i(z)$가 $p(z|x_i)$와 가까워져, 실제 샘플에 대응하는 latent를 더 잘 뽑아줄 수 있게 된다.</p><p>이를 학습하기 위해 elbo를 maximize 하는 과정을 살펴보면,<br>i번째 샘플 데이터학습할 때 마다, $q_i$로부터 $\mu_i, \Sigma_i$를 얻어내고, 여기로부터의 분포에서 다시 $\hat{x}$를 뽑아내서($\theta$), $x_i$와 닮도록 학습한다.</p><p>이 때, 각 데이터 샘플마다 뮤와 시그마에 대응시키는 파라미터가 필요하다.<br>-> 너무 많다&mldr; 새로운 데이터 들어올때마다 또 추가해야한다.</p><h4 id=amortized-variational-inference>Amortized Variational Inference<a hidden class=anchor aria-hidden=true href=#amortized-variational-inference>#</a></h4><ul><li>$x_i$에 대응되는 $z_i$가 존재하는 상황<ul><li>N개의 데이터가 있다면,,, 원래는 N개의 파라미터를 대응시켜서 학습시켰다..(n개의 튜플만들어서 각 튜플마다 파라미터로서 바꾸는느낌)</li><li>이걸 차라리 하나의 네트워크를 통해 mapping을 시켜주겠다!!(보통 DNN에서 입력, 출력 원하는 방향으로 하듯이)</li></ul></li></ul><p>수식으로 보면, $q_i(z) \approx q(z|x_i)$ 왼쪽거 대신 오른쪽거로 VI를 하겠다는 뜻
데이터 하나마다 학습되는 과정을 수식으로 바라보면,<br>$x_i$-> NN($q_\phi(z|x)$) -> $\mu(x_i), \sigma(x_i)$ -> $z=\mu(x_i) + \epsilon\sigma(x_i)$ -> NN($p_\theta(x|z)$) -> $\hat{x} \approx x$<br></p><h2 id=코드-실습>코드 실습<a hidden class=anchor aria-hidden=true href=#코드-실습>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.nn</span> <span class=kn>import</span> <span class=n>functional</span> <span class=k>as</span> <span class=n>F</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torchvision</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torchvision.transforms</span> <span class=k>as</span> <span class=nn>transforms</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torchvision.utils</span> <span class=kn>import</span> <span class=n>save_image</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>PIL</span> <span class=kn>import</span> <span class=n>Image</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s2>&#34;cpu&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>device</span>
</span></span></code></pre></div><pre><code>device(type='cuda')
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>data_set</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>datasets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                      <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                      <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
</span></span><span class=line><span class=cl>                                          <span class=n>transforms</span><span class=o>.</span><span class=n>Resize</span><span class=p>((</span><span class=mi>32</span><span class=p>,</span> <span class=mi>32</span><span class=p>)),</span>
</span></span><span class=line><span class=cl>                                          <span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                                          <span class=p>]),</span>
</span></span><span class=line><span class=cl>                                      <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                      <span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>data_set</span>
</span></span></code></pre></div><pre><code>Dataset MNIST
    Number of datapoints: 60000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=warn)
               ToTensor()
           )
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>data_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>data_set</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=모델구성>모델구성<a hidden class=anchor aria-hidden=true href=#모델구성>#</a></h3><ul><li>mnist data : 28x28</li><li>network :
channel : 4, 8, 16, 32 /
WxH : 28-14-7-4-2 -> fc1 4 -> 1(mu), fc2 4 -> 1(var)</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>VAE</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>in_channels</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>latent_dim</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>hidden_dims</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>VAE</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>latent_dim</span> <span class=o>=</span> <span class=n>latent_dim</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>modules</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>hidden_dims</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>hidden_dims</span> <span class=o>=</span> <span class=p>[</span><span class=mi>16</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>128</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Build Encoder</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>h_dim</span> <span class=ow>in</span> <span class=n>hidden_dims</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>in_channels</span><span class=p>,</span> <span class=n>out_channels</span><span class=o>=</span><span class=n>h_dim</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                              <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                    <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=n>h_dim</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                    <span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>in_channels</span> <span class=o>=</span> <span class=n>h_dim</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=n>modules</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc_mu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dims</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>*</span><span class=mi>4</span><span class=p>,</span> <span class=n>latent_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc_var</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dims</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>*</span><span class=mi>4</span><span class=p>,</span> <span class=n>latent_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Build Decoder</span>
</span></span><span class=line><span class=cl>        <span class=n>modules</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>decoder_input</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>latent_dim</span><span class=p>,</span> <span class=n>hidden_dims</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>*</span><span class=mi>4</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>hidden_dims</span><span class=o>.</span><span class=n>reverse</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>hidden_dims</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=n>nn</span><span class=o>.</span><span class=n>ConvTranspose2d</span><span class=p>(</span><span class=n>hidden_dims</span><span class=p>[</span><span class=n>i</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                                       <span class=n>hidden_dims</span><span class=p>[</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                                       <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                       <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                       <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                       <span class=n>output_padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                    <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=n>hidden_dims</span><span class=p>[</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>                    <span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=n>modules</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>final_layer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ConvTranspose2d</span><span class=p>(</span><span class=n>hidden_dims</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                               <span class=n>hidden_dims</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                               <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                               <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                               <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                               <span class=n>output_padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=n>hidden_dims</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>hidden_dims</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=n>out_channels</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                      <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Sigmoid</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>encode</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=nb>input</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>result</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=p>(</span><span class=nb>input</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>result</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>flatten</span><span class=p>(</span><span class=n>result</span><span class=p>,</span> <span class=n>start_dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>mu</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc_mu</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>log_var</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc_var</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>[</span><span class=n>mu</span><span class=p>,</span> <span class=n>log_var</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>decode</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>z</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>result</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>decoder_input</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>result</span> <span class=o>=</span> <span class=n>result</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>result</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>result</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>final_layer</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>result</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>reparameterize</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>mu</span><span class=p>,</span> <span class=n>logvar</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>std</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=mf>0.5</span><span class=o>*</span><span class=n>logvar</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>eps</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn_like</span><span class=p>(</span><span class=n>std</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>eps</span><span class=o>*</span><span class=n>std</span> <span class=o>+</span> <span class=n>mu</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>loss_function</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                      <span class=o>*</span><span class=n>args</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>recons</span> <span class=o>=</span> <span class=n>args</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=nb>input</span> <span class=o>=</span> <span class=n>args</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>mu</span> <span class=o>=</span> <span class=n>args</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>log_var</span> <span class=o>=</span> <span class=n>args</span><span class=p>[</span><span class=mi>3</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>recons_loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>mse_loss</span><span class=p>(</span><span class=n>recons</span><span class=p>,</span> <span class=nb>input</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>kld_loss</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=o>-</span><span class=mf>0.5</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>log_var</span> <span class=o>-</span> <span class=n>mu</span><span class=o>**</span><span class=mi>2</span> <span class=o>-</span> <span class=n>log_var</span><span class=o>.</span><span class=n>exp</span><span class=p>(),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>recons_loss</span> <span class=o>+</span> <span class=n>kld_loss</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>loss</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>sample</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>               <span class=n>num_samples</span><span class=p>,</span>
</span></span><span class=line><span class=cl>               <span class=n>current_device</span><span class=p>,</span>
</span></span><span class=line><span class=cl>               <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>z</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>num_samples</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=bp>self</span><span class=o>.</span><span class=n>latent_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>z</span> <span class=o>=</span> <span class=n>z</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>current_device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>samples</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>samples</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>generate</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>x</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>VAE</span><span class=p>(</span><span class=n>in_channels</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>latent_dim</span><span class=o>=</span><span class=mi>200</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span> <span class=n>betas</span><span class=o>=</span><span class=p>(</span><span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.999</span><span class=p>))</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>num</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>30</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>images</span><span class=p>,</span> <span class=n>_</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>data_loader</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># forward</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>images</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>mu</span><span class=p>,</span> <span class=n>log_var</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>z</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>reparameterize</span><span class=p>(</span><span class=n>mu</span><span class=p>,</span> <span class=n>log_var</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>x_rec</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># compute loss</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>loss_function</span><span class=p>(</span><span class=n>x_rec</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mu</span><span class=p>,</span> <span class=n>log_var</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># backward</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>i</span> <span class=o>%</span> <span class=mi>100</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;epoch : </span><span class=si>{</span><span class=n>epoch</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2> | iter : </span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s2> | loss : </span><span class=si>{</span><span class=n>loss</span><span class=si>:</span><span class=s2>.4</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>i</span> <span class=o>%</span> <span class=mi>200</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>samples</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>save_image</span><span class=p>(</span><span class=n>samples</span><span class=p>,</span> <span class=sa>f</span><span class=s2>&#34;./vae_samples/sample</span><span class=si>{</span><span class=n>num</span><span class=si>}</span><span class=s2>.png&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>num</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span></code></pre></div><pre><code>epoch : 1 | iter : 0 | loss : 25.12
epoch : 1 | iter : 100 | loss : 0.2628
epoch : 1 | iter : 200 | loss : 0.2504
epoch : 1 | iter : 300 | loss : 0.1924
epoch : 1 | iter : 400 | loss : 0.1042
epoch : 1 | iter : 500 | loss : 0.2347
epoch : 2 | iter : 0 | loss : 0.08283
epoch : 2 | iter : 100 | loss : 0.06895
epoch : 2 | iter : 200 | loss : 0.07953
epoch : 2 | iter : 300 | loss : 0.07523
epoch : 2 | iter : 400 | loss : 0.07775
epoch : 2 | iter : 500 | loss : 0.08221
epoch : 3 | iter : 0 | loss : 0.0736
epoch : 3 | iter : 100 | loss : 0.06282
epoch : 3 | iter : 200 | loss : 0.07073
epoch : 3 | iter : 300 | loss : 0.05984
epoch : 3 | iter : 400 | loss : 0.06127
epoch : 3 | iter : 500 | loss : 0.06909
epoch : 4 | iter : 0 | loss : 0.06461
epoch : 4 | iter : 100 | loss : 0.06142
epoch : 4 | iter : 200 | loss : 0.07161
epoch : 4 | iter : 300 | loss : 0.05889
epoch : 4 | iter : 400 | loss : 0.05847
epoch : 4 | iter : 500 | loss : 0.06437
epoch : 5 | iter : 0 | loss : 0.06036
epoch : 5 | iter : 100 | loss : 0.06125
epoch : 5 | iter : 200 | loss : 0.07911
epoch : 5 | iter : 300 | loss : 0.06195
epoch : 5 | iter : 400 | loss : 0.05938
epoch : 5 | iter : 500 | loss : 0.06209
epoch : 6 | iter : 0 | loss : 0.07897
epoch : 6 | iter : 100 | loss : 0.06053
epoch : 6 | iter : 200 | loss : 0.07334
epoch : 6 | iter : 300 | loss : 0.07139
epoch : 6 | iter : 400 | loss : 0.06082
epoch : 6 | iter : 500 | loss : 0.05961
epoch : 7 | iter : 0 | loss : 0.0998
epoch : 7 | iter : 100 | loss : 0.05946
epoch : 7 | iter : 200 | loss : 0.06843
epoch : 7 | iter : 300 | loss : 0.06251
epoch : 7 | iter : 400 | loss : 0.05751
epoch : 7 | iter : 500 | loss : 0.05803
epoch : 8 | iter : 0 | loss : 0.09001
epoch : 8 | iter : 100 | loss : 0.05908
epoch : 8 | iter : 200 | loss : 0.06798
epoch : 8 | iter : 300 | loss : 0.05804
epoch : 8 | iter : 400 | loss : 0.05706
epoch : 8 | iter : 500 | loss : 0.05436
epoch : 9 | iter : 0 | loss : 0.07401
epoch : 9 | iter : 100 | loss : 0.0589
epoch : 9 | iter : 200 | loss : 0.06516
epoch : 9 | iter : 300 | loss : 0.05802
epoch : 9 | iter : 400 | loss : 0.07783
epoch : 9 | iter : 500 | loss : 0.05494
epoch : 10 | iter : 0 | loss : 0.06096
epoch : 10 | iter : 100 | loss : 0.05923
epoch : 10 | iter : 200 | loss : 0.06546
epoch : 10 | iter : 300 | loss : 0.05692
epoch : 10 | iter : 400 | loss : 0.05633
epoch : 10 | iter : 500 | loss : 0.05238
epoch : 11 | iter : 0 | loss : 0.05586
epoch : 11 | iter : 100 | loss : 0.05847
epoch : 11 | iter : 200 | loss : 0.0642
epoch : 11 | iter : 300 | loss : 0.05614
epoch : 11 | iter : 400 | loss : 0.05614
epoch : 11 | iter : 500 | loss : 0.05214
epoch : 12 | iter : 0 | loss : 0.05507
epoch : 12 | iter : 100 | loss : 0.05837
epoch : 12 | iter : 200 | loss : 0.06383
epoch : 12 | iter : 300 | loss : 0.05589
epoch : 12 | iter : 400 | loss : 0.05605
epoch : 12 | iter : 500 | loss : 0.05205
epoch : 13 | iter : 0 | loss : 0.05482
epoch : 13 | iter : 100 | loss : 0.05831
epoch : 13 | iter : 200 | loss : 0.06379
epoch : 13 | iter : 300 | loss : 0.05581
epoch : 13 | iter : 400 | loss : 0.05601
epoch : 13 | iter : 500 | loss : 0.05198
epoch : 14 | iter : 0 | loss : 0.05462
epoch : 14 | iter : 100 | loss : 0.05825
epoch : 14 | iter : 200 | loss : 0.06375
epoch : 14 | iter : 300 | loss : 0.05574
epoch : 14 | iter : 400 | loss : 0.05597
epoch : 14 | iter : 500 | loss : 0.052
epoch : 15 | iter : 0 | loss : 0.05447
epoch : 15 | iter : 100 | loss : 0.05825
epoch : 15 | iter : 200 | loss : 0.06369
epoch : 15 | iter : 300 | loss : 0.05568
epoch : 15 | iter : 400 | loss : 0.05597
epoch : 15 | iter : 500 | loss : 0.05191
epoch : 16 | iter : 0 | loss : 0.0544
epoch : 16 | iter : 100 | loss : 0.05823
epoch : 16 | iter : 200 | loss : 0.06365
epoch : 16 | iter : 300 | loss : 0.0556
epoch : 16 | iter : 400 | loss : 0.05592
epoch : 16 | iter : 500 | loss : 0.0519
epoch : 17 | iter : 0 | loss : 0.0543
epoch : 17 | iter : 100 | loss : 0.05822
epoch : 17 | iter : 200 | loss : 0.06361
epoch : 17 | iter : 300 | loss : 0.05559
epoch : 17 | iter : 400 | loss : 0.0559
epoch : 17 | iter : 500 | loss : 0.0519
epoch : 18 | iter : 0 | loss : 0.05427
epoch : 18 | iter : 100 | loss : 0.05824
epoch : 18 | iter : 200 | loss : 0.06361
epoch : 18 | iter : 300 | loss : 0.05553
epoch : 18 | iter : 400 | loss : 0.05594
epoch : 18 | iter : 500 | loss : 0.05185
epoch : 19 | iter : 0 | loss : 0.05425
epoch : 19 | iter : 100 | loss : 0.05821
epoch : 19 | iter : 200 | loss : 0.06357
epoch : 19 | iter : 300 | loss : 0.05552
epoch : 19 | iter : 400 | loss : 0.05591
epoch : 19 | iter : 500 | loss : 0.0519
epoch : 20 | iter : 0 | loss : 0.05424
epoch : 20 | iter : 100 | loss : 0.05821
epoch : 20 | iter : 200 | loss : 0.06349
epoch : 20 | iter : 300 | loss : 0.0555
epoch : 20 | iter : 400 | loss : 0.0559
epoch : 20 | iter : 500 | loss : 0.05188
epoch : 21 | iter : 0 | loss : 0.05432
epoch : 21 | iter : 100 | loss : 0.05819
epoch : 21 | iter : 200 | loss : 0.06353
epoch : 21 | iter : 300 | loss : 0.0555
epoch : 21 | iter : 400 | loss : 0.05591
epoch : 21 | iter : 500 | loss : 0.05189
epoch : 22 | iter : 0 | loss : 0.05439
epoch : 22 | iter : 100 | loss : 0.05821
epoch : 22 | iter : 200 | loss : 0.06355
epoch : 22 | iter : 300 | loss : 0.05548
epoch : 22 | iter : 400 | loss : 0.0559
epoch : 22 | iter : 500 | loss : 0.05187
epoch : 23 | iter : 0 | loss : 0.05434
epoch : 23 | iter : 100 | loss : 0.0582
epoch : 23 | iter : 200 | loss : 0.06354
epoch : 23 | iter : 300 | loss : 0.05552
epoch : 23 | iter : 400 | loss : 0.05592
epoch : 23 | iter : 500 | loss : 0.05189
epoch : 24 | iter : 0 | loss : 0.0545
epoch : 24 | iter : 100 | loss : 0.05819
epoch : 24 | iter : 200 | loss : 0.06357
epoch : 24 | iter : 300 | loss : 0.05547
epoch : 24 | iter : 400 | loss : 0.05589
epoch : 24 | iter : 500 | loss : 0.05189
epoch : 25 | iter : 0 | loss : 0.05434
epoch : 25 | iter : 100 | loss : 0.05818
epoch : 25 | iter : 200 | loss : 0.06356
epoch : 25 | iter : 300 | loss : 0.05543
epoch : 25 | iter : 400 | loss : 0.0559
epoch : 25 | iter : 500 | loss : 0.05189
epoch : 26 | iter : 0 | loss : 0.0545
epoch : 26 | iter : 100 | loss : 0.05818
epoch : 26 | iter : 200 | loss : 0.06354
epoch : 26 | iter : 300 | loss : 0.05548
epoch : 26 | iter : 400 | loss : 0.05589
epoch : 26 | iter : 500 | loss : 0.05191
epoch : 27 | iter : 0 | loss : 0.05421
epoch : 27 | iter : 100 | loss : 0.05817
epoch : 27 | iter : 200 | loss : 0.06355
epoch : 27 | iter : 300 | loss : 0.0555
epoch : 27 | iter : 400 | loss : 0.05591
epoch : 27 | iter : 500 | loss : 0.05192
epoch : 28 | iter : 0 | loss : 0.05429
epoch : 28 | iter : 100 | loss : 0.05816
epoch : 28 | iter : 200 | loss : 0.06354
epoch : 28 | iter : 300 | loss : 0.05548
epoch : 28 | iter : 400 | loss : 0.0559
epoch : 28 | iter : 500 | loss : 0.0519
epoch : 29 | iter : 0 | loss : 0.05418
epoch : 29 | iter : 100 | loss : 0.05819
epoch : 29 | iter : 200 | loss : 0.06356
epoch : 29 | iter : 300 | loss : 0.05547
epoch : 29 | iter : 400 | loss : 0.05594
epoch : 29 | iter : 500 | loss : 0.05189
epoch : 30 | iter : 0 | loss : 0.05412
epoch : 30 | iter : 100 | loss : 0.05817
epoch : 30 | iter : 200 | loss : 0.06355
epoch : 30 | iter : 300 | loss : 0.05546
epoch : 30 | iter : 400 | loss : 0.05589
epoch : 30 | iter : 500 | loss : 0.05189
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># class VAE(nn.Module):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#     def __init__(self,</span>
</span></span><span class=line><span class=cl><span class=c1>#                  input_feature,</span>
</span></span><span class=line><span class=cl><span class=c1>#                  latent_dim,</span>
</span></span><span class=line><span class=cl><span class=c1>#                  hidden_dim,</span>
</span></span><span class=line><span class=cl><span class=c1>#                  **kwargs):</span>
</span></span><span class=line><span class=cl><span class=c1>#         super(VAE, self).__init__()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#         # Build Encoder</span>
</span></span><span class=line><span class=cl><span class=c1>#         self.e_fc1 = nn.Sequential(</span>
</span></span><span class=line><span class=cl><span class=c1>#             nn.Linear(input_feature, hidden_dim),</span>
</span></span><span class=line><span class=cl><span class=c1>#             nn.BatchNorm2d(hidden_dim),</span>
</span></span><span class=line><span class=cl><span class=c1>#             nn.LeakyReLU(0.2)</span>
</span></span><span class=line><span class=cl><span class=c1>#             )</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#         self.e_fc2 = nn.Sequential(</span>
</span></span><span class=line><span class=cl><span class=c1>#             nn.Linear(hidden_dim, hidden_dim),</span>
</span></span><span class=line><span class=cl><span class=c1>#             nn.BatchNorm2d(hidden_dim),</span>
</span></span><span class=line><span class=cl><span class=c1>#             nn.LeakyReLU(0.2)</span>
</span></span><span class=line><span class=cl><span class=c1>#             )</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#         # output layer</span>
</span></span><span class=line><span class=cl><span class=c1>#         self.mu = nn.Linear(hidden_dim, latent_dim)</span>
</span></span><span class=line><span class=cl><span class=c1>#         self.var = nn.Linear(hidden_dim, latent_dim)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#         # Build Decoder</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#         self.d_fc1 = nn.Sequential(</span>
</span></span><span class=line><span class=cl><span class=c1>#             nn.Linear(latent_dim, hidden_dim),</span>
</span></span><span class=line><span class=cl><span class=c1>#             nn.BatchNorm2d(hidden_dim),</span>
</span></span><span class=line><span class=cl><span class=c1>#             nn.LeakyReLU(0.2)</span>
</span></span><span class=line><span class=cl><span class=c1>#             )</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#         self.d_fc2 = nn.Sequential(</span>
</span></span><span class=line><span class=cl><span class=c1>#             nn.Linear(hidden_dim, hidden_dim),</span>
</span></span><span class=line><span class=cl><span class=c1>#             nn.BatchNorm2d(hidden_dim),</span>
</span></span><span class=line><span class=cl><span class=c1>#             nn.LeakyReLU(0.2)</span>
</span></span><span class=line><span class=cl><span class=c1>#             )</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#         self.fianl_layer = nn.Sequential(</span>
</span></span><span class=line><span class=cl><span class=c1>#             nn.Linear(hidden_dim, input_feature),</span>
</span></span><span class=line><span class=cl><span class=c1>#             nn.Tanh()</span>
</span></span><span class=line><span class=cl><span class=c1>#         )</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#     def encode(self, input):</span>
</span></span><span class=line><span class=cl><span class=c1>#         result = self.e_fc1(input)</span>
</span></span><span class=line><span class=cl><span class=c1>#         result = self.e_fc2(result)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#         mu = self.mu(result)</span>
</span></span><span class=line><span class=cl><span class=c1>#         log_var = self.var(result)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#         return [mu, log_var]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#     def decode(self, z):</span>
</span></span><span class=line><span class=cl><span class=c1>#         result = self.d_fc1(z)</span>
</span></span><span class=line><span class=cl><span class=c1>#         result = self.d_fc2(result)</span>
</span></span><span class=line><span class=cl><span class=c1>#         result = self.final_layer(result)</span>
</span></span><span class=line><span class=cl><span class=c1>#         return result</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#     def reparameterize(self, mu, logvar):</span>
</span></span><span class=line><span class=cl><span class=c1>#         std = torch.exp(0.5*logvar)</span>
</span></span><span class=line><span class=cl><span class=c1>#         eps = torch.randn_like(std)</span>
</span></span><span class=line><span class=cl><span class=c1>#         return eps*std + mu</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#     def loss_function(self,</span>
</span></span><span class=line><span class=cl><span class=c1>#                       *args):</span>
</span></span><span class=line><span class=cl><span class=c1>#         recons = args[0]</span>
</span></span><span class=line><span class=cl><span class=c1>#         input = args[1]</span>
</span></span><span class=line><span class=cl><span class=c1>#         mu = args[2]</span>
</span></span><span class=line><span class=cl><span class=c1>#         log_var = args[3]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#         recons_loss = F.mse_loss(recons, input)</span>
</span></span><span class=line><span class=cl><span class=c1>#         kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu**2 - log_var.exp(), dim=1), dim=0)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#         loss = recons_loss + kld_loss</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#         return loss</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#     def sample(self,</span>
</span></span><span class=line><span class=cl><span class=c1>#                num_samples,</span>
</span></span><span class=line><span class=cl><span class=c1>#                current_device,</span>
</span></span><span class=line><span class=cl><span class=c1>#                **kwargs):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#         z = torch.randn(num_samples,</span>
</span></span><span class=line><span class=cl><span class=c1>#                         self.latent_dim)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#         z = z.to(current_device)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#         samples = self.decode(z)</span>
</span></span><span class=line><span class=cl><span class=c1>#         samples = samples.view(-1, 28, 28)</span>
</span></span><span class=line><span class=cl><span class=c1>#         return samples</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#     def generate(self, x, **kwargs):</span>
</span></span><span class=line><span class=cl><span class=c1>#         return self.forward(x)[0]</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://dobe0715.github.io/posts/stylegan2-ada/><span class=title>« Prev</span><br><span></span></a>
<a class=next href=https://dobe0715.github.io/posts/variational-diffusion-models/><span class=title>Next »</span><br><span></span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share  on twitter" href="https://twitter.com/intent/tweet/?text=&amp;url=https%3a%2f%2fdobe0715.github.io%2fposts%2fvae%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share  on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdobe0715.github.io%2fposts%2fvae%2f&amp;title=&amp;summary=&amp;source=https%3a%2f%2fdobe0715.github.io%2fposts%2fvae%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share  on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdobe0715.github.io%2fposts%2fvae%2f&title="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share  on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdobe0715.github.io%2fposts%2fvae%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share  on whatsapp" href="https://api.whatsapp.com/send?text=%20-%20https%3a%2f%2fdobe0715.github.io%2fposts%2fvae%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share  on telegram" href="https://telegram.me/share/url?text=&amp;url=https%3a%2f%2fdobe0715.github.io%2fposts%2fvae%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share  on ycombinator" href="https://news.ycombinator.com/submitlink?t=&u=https%3a%2f%2fdobe0715.github.io%2fposts%2fvae%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://dobe0715.github.io>블로그 홈</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
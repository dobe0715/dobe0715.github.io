<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Zero-shot Image-to-Image Translation(pix2pix zero) review | 블로그 홈</title><meta name=keywords content><meta name=description content="Review : Zero-shot Image-to-Image Translation(pix2pix zero) pre-train된 모델만으로 pix2pix 작업을 완성도 있게 수행한 모델 1. 핵심내용 traing free, prompt free input text prompting 없이 자동으로 direction editing cross-attention guidance를 통해 content 보존 auto correlation regularization conditional GAN distillation 2. Related Works Deep image editing with GANs 기존의 연구들은 target imgae의 latent vector를 disentangle한 space로 보냄으로써, latent vector를 조작하여 image translation을 수행하였다. ex) stylegan에서의 W space single category에 대해 효과적, high quality inversion 단점은 image set을 내가 잘 사전에 준비해놔야한다&mldr; pre trained GAN이용!"><meta name=author content="Me"><link rel=canonical href=https://dobe0715.github.io/posts/pix2pix-zero/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Zero-shot Image-to-Image Translation(pix2pix zero) review"><meta property="og:description" content="Review : Zero-shot Image-to-Image Translation(pix2pix zero) pre-train된 모델만으로 pix2pix 작업을 완성도 있게 수행한 모델 1. 핵심내용 traing free, prompt free input text prompting 없이 자동으로 direction editing cross-attention guidance를 통해 content 보존 auto correlation regularization conditional GAN distillation 2. Related Works Deep image editing with GANs 기존의 연구들은 target imgae의 latent vector를 disentangle한 space로 보냄으로써, latent vector를 조작하여 image translation을 수행하였다. ex) stylegan에서의 W space single category에 대해 효과적, high quality inversion 단점은 image set을 내가 잘 사전에 준비해놔야한다&mldr; pre trained GAN이용!"><meta property="og:type" content="article"><meta property="og:url" content="https://dobe0715.github.io/posts/pix2pix-zero/"><meta property="og:image" content="https://dobe0715.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dobe0715.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Zero-shot Image-to-Image Translation(pix2pix zero) review"><meta name=twitter:description content="Review : Zero-shot Image-to-Image Translation(pix2pix zero) pre-train된 모델만으로 pix2pix 작업을 완성도 있게 수행한 모델 1. 핵심내용 traing free, prompt free input text prompting 없이 자동으로 direction editing cross-attention guidance를 통해 content 보존 auto correlation regularization conditional GAN distillation 2. Related Works Deep image editing with GANs 기존의 연구들은 target imgae의 latent vector를 disentangle한 space로 보냄으로써, latent vector를 조작하여 image translation을 수행하였다. ex) stylegan에서의 W space single category에 대해 효과적, high quality inversion 단점은 image set을 내가 잘 사전에 준비해놔야한다&mldr; pre trained GAN이용!"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://dobe0715.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Zero-shot Image-to-Image Translation(pix2pix zero) review","item":"https://dobe0715.github.io/posts/pix2pix-zero/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Zero-shot Image-to-Image Translation(pix2pix zero) review","name":"Zero-shot Image-to-Image Translation(pix2pix zero) review","description":"Review : Zero-shot Image-to-Image Translation(pix2pix zero) pre-train된 모델만으로 pix2pix 작업을 완성도 있게 수행한 모델 1. 핵심내용 traing free, prompt free input text prompting 없이 자동으로 direction editing cross-attention guidance를 통해 content 보존 auto correlation regularization conditional GAN distillation 2. Related Works Deep image editing with GANs 기존의 연구들은 target imgae의 latent vector를 disentangle한 space로 보냄으로써, latent vector를 조작하여 image translation을 수행하였다. ex) stylegan에서의 W space single category에 대해 효과적, high quality inversion 단점은 image set을 내가 잘 사전에 준비해놔야한다\u0026hellip; pre trained GAN이용!","keywords":[],"articleBody":"Review : Zero-shot Image-to-Image Translation(pix2pix zero) pre-train된 모델만으로 pix2pix 작업을 완성도 있게 수행한 모델 1. 핵심내용 traing free, prompt free input text prompting 없이 자동으로 direction editing cross-attention guidance를 통해 content 보존 auto correlation regularization conditional GAN distillation 2. Related Works Deep image editing with GANs 기존의 연구들은 target imgae의 latent vector를 disentangle한 space로 보냄으로써, latent vector를 조작하여 image translation을 수행하였다. ex) stylegan에서의 W space single category에 대해 효과적, high quality inversion 단점은 image set을 내가 잘 사전에 준비해놔야한다… pre trained GAN이용! Text2Image models 기존의 모델들은 text로 입력한 부분 이외의 부분에서도 변경이 많이 일어난다.\n이러한 부분을 제어하는 것이 중요한데, mask기법을 이용한 기술이 많이사용되었다. 또한, diffusion기반 translation모델들은 추가학습이 필요하다.\nex) Palette, InstructPix2Pix, PITI pix2pix zero : mask x, 추가학습x\nImage editing with diffusion models Imagic : 성능은 훌륭 but, 모델에 맞는 fine-tune 필요 Prompt2Prompt : cross-attention map 이용. fine-tune 필요 x pix2pix zero : 성능훌륭, fine-tune 필요 x! 3. Method 목적 : 주어진 image에서 cat -\u003e dog의 변환과정 수행 Inverting Real Images 사전 준비. image가 $\\tilde{x} \\in R^{5125123}$이면, stable diffusion을 통해 $x_0 \\in R^{64644}$ 으로 encoding 해놓고 시작 Deterministic inversion DDPM과 DDIM의 가장 큰 차이점은 inversion과정이다. ddpm은 마르코프 연쇄 룰 ($q(x_{t}|x_{t-1}, …, x_0)=q(x_t|x_{t-1})$ “다음것은 오직 바로 직전 것으로 유도된다” 라고 가정하여 inversion 수식유도 ddim은 $q(x_{t}|x_{t-1}, x_0)$ “다음것은 처음과 직전 이미지에의해 결정된다\"를 가정해서 inversion 수식유도 $\\sigma$에 따라 inversion 모델이 ddpm이 되기도하고 deterministic 해지기도 한다. 여기선 0이라고 하여 inversion $\\epsilon$은 각t번째 time step에서의 noise라고 생각할 수 있다. 즉, $x_t$번째 이미지를 통해 처음 이미지와 $t$번째 noise를 추출하여 $\\alpha_{t-1}$번째 가중치로 interpolation해서 $x_{t-1}$번째 이미지를 추측한다는 의미 Noise regularization 위와같이 DDIM inversion하면서 추출한 Noise map($\\epsilon$)이 가우스 distribution을 따르지 않는 경우, image editing이 잘 수행되지 않게 된다. 즉, noise는 다음 조건을 따를 수록 좋다\n임의의 두 위치 pair에 대해, 그 corelation 값이 낮아야 한다 각 spatial위치에 대해 평균 : 0, 분산 : 1 을 만족해야한다. Loss\n$L_{auto} = L_{pair} + \\lambda L_{KL}$\n$L_{pair}$\n보통은 $\\delta=1$에 대해서만 계산하지만, 여기선 더 넓은 range에 대해 적용\n모든 위치에 대해 안하는건 연산량 너무 많아서인듯.. 대충 생각해도 시간복잡도가 $O(n^4)$가 나온다\n$L_{KL}$\nmean=0, var=1에 너무 fit하게 맞춰버리면, denoise 과정에서 발산하는 경우가 있어, KL-divergence를 이용해 부드럽게 균형맞춰준다. VAE에서 사용한 idea로, $N(0, 1)$과 $\\epsilon(t)$의 KL값을 편하게 계산하도록 유도한 공식이 있다. 이걸 사용한다. $L_{KL} = \\sigma^2 + \\mu^2 - 1 - \\log{\\sigma^2}$ Discovering Edit Directions 위의 Method부분을 보면 좋다. source domain과 target domain의 단어가 포함된 문장을 gpt등의 모델을 통해 생성 (논문에서는 각각 1000개) CLIP기술을 통해 vector로 embedding한 후, 각 단어 벡터의 평균의 차를 구한다. Editing via Cross-Attention Guidance 차후 채울 예정.. ","wordCount":"404","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://dobe0715.github.io/posts/pix2pix-zero/"},"publisher":{"@type":"Organization","name":"블로그 홈","logo":{"@type":"ImageObject","url":"https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dobe0715.github.io accesskey=h title="Home (Alt + H)"><img src=https://dobe0715.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dobe0715.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://dobe0715.github.io/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dobe0715.github.io>Home</a>&nbsp;»&nbsp;<a href=https://dobe0715.github.io/posts/>Posts</a></div><h1 class=post-title>Zero-shot Image-to-Image Translation(pix2pix zero) review</h1><div class=post-meta>2 min&nbsp;·&nbsp;404 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/pix2pix%20zero.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><h1 id=review--zero-shot-image-to-image-translationpix2pix-zero>Review : Zero-shot Image-to-Image Translation(pix2pix zero)<a hidden class=anchor aria-hidden=true href=#review--zero-shot-image-to-image-translationpix2pix-zero>#</a></h1><ul><li>pre-train된 모델만으로 pix2pix 작업을 완성도 있게 수행한 모델</li></ul><h2 id=1-핵심내용>1. 핵심내용<a hidden class=anchor aria-hidden=true href=#1-핵심내용>#</a></h2><ul><li><strong>traing free, prompt free</strong></li><li>input text prompting 없이 자동으로 direction editing</li><li>cross-attention guidance를 통해 content 보존</li><li>auto correlation regularization</li><li>conditional GAN distillation</li></ul><h2 id=2-related-works>2. Related Works<a hidden class=anchor aria-hidden=true href=#2-related-works>#</a></h2><h3 id=deep-image-editing-with-gans>Deep image editing with GANs<a hidden class=anchor aria-hidden=true href=#deep-image-editing-with-gans>#</a></h3><ul><li>기존의 연구들은 target imgae의 latent vector를 disentangle한 space로 보냄으로써, latent vector를 조작하여 image translation을 수행하였다.<ul><li>ex) stylegan에서의 W space</li><li>single category에 대해 효과적, high quality inversion</li><li>단점은 image set을 내가 잘 사전에 준비해놔야한다&mldr;</li><li>pre trained GAN이용!</li></ul></li></ul><p></p><h3 id=text2image-models>Text2Image models<a hidden class=anchor aria-hidden=true href=#text2image-models>#</a></h3><ul><li><p>기존의 모델들은 text로 입력한 부분 이외의 부분에서도 변경이 많이 일어난다.</p><ul><li>이러한 부분을 제어하는 것이 중요한데, mask기법을 이용한 기술이 많이사용되었다.</li></ul></li><li><p>또한, diffusion기반 translation모델들은 추가학습이 필요하다.</p><ul><li>ex) Palette, InstructPix2Pix, PITI</li></ul></li><li><p><strong>pix2pix zero : mask x, 추가학습x</strong></p></li></ul><h3 id=image-editing-with-diffusion-models>Image editing with diffusion models<a hidden class=anchor aria-hidden=true href=#image-editing-with-diffusion-models>#</a></h3><ul><li>Imagic : 성능은 훌륭 but, 모델에 맞는 fine-tune 필요</li><li>Prompt2Prompt : cross-attention map 이용. fine-tune 필요 x</li><li><strong>pix2pix zero : 성능훌륭, fine-tune 필요 x!</strong></li></ul><h2 id=3-method>3. Method<a hidden class=anchor aria-hidden=true href=#3-method>#</a></h2><ul><li>목적 : 주어진 image에서 cat -> dog의 변환과정 수행</li></ul><h3 id=inverting-real-images>Inverting Real Images<a hidden class=anchor aria-hidden=true href=#inverting-real-images>#</a></h3><ul><li>사전 준비.</li><li>image가 $\tilde{x} \in R^{512<em>512</em>3}$이면, stable diffusion을 통해 $x_0 \in R^{64<em>64</em>4}$ 으로 encoding 해놓고 시작</li></ul><h4 id=deterministic-inversion>Deterministic inversion<a hidden class=anchor aria-hidden=true href=#deterministic-inversion>#</a></h4><ul><li>DDPM과 DDIM의 가장 큰 차이점은 inversion과정이다.</li><li>ddpm은 마르코프 연쇄 룰 ($q(x_{t}|x_{t-1}, &mldr;, x_0)=q(x_t|x_{t-1})$ &ldquo;다음것은 오직 바로 직전 것으로 유도된다&rdquo; 라고 가정하여 inversion 수식유도</li><li>ddim은 $q(x_{t}|x_{t-1}, x_0)$ &ldquo;다음것은 처음과 직전 이미지에의해 결정된다"를 가정해서 inversion 수식유도</li></ul><ul><li>$\sigma$에 따라 inversion 모델이 ddpm이 되기도하고 deterministic 해지기도 한다. 여기선 0이라고 하여 inversion</li><li>$\epsilon$은 각t번째 time step에서의 noise라고 생각할 수 있다.</li><li>즉, $x_t$번째 이미지를 통해 처음 이미지와 $t$번째 noise를 추출하여 $\alpha_{t-1}$번째 가중치로 interpolation해서 $x_{t-1}$번째 이미지를 추측한다는 의미</li></ul><h4 id=noise-regularization>Noise regularization<a hidden class=anchor aria-hidden=true href=#noise-regularization>#</a></h4><ul><li><p>위와같이 DDIM inversion하면서 추출한 Noise map($\epsilon$)이 가우스 distribution을 따르지 않는 경우, image editing이 잘 수행되지 않게 된다. 즉, noise는 다음 조건을 따를 수록 좋다</p><ol><li>임의의 두 위치 pair에 대해, 그 corelation 값이 낮아야 한다</li><li>각 spatial위치에 대해 평균 : 0, 분산 : 1 을 만족해야한다.</li></ol></li><li><p><strong>Loss</strong><br>$L_{auto} = L_{pair} + \lambda L_{KL}$</p></li><li><p>$L_{pair}$</p></li></ul><ul><li><p>보통은 $\delta=1$에 대해서만 계산하지만, 여기선 더 넓은 range에 대해 적용</p></li><li><p>모든 위치에 대해 안하는건 연산량 너무 많아서인듯.. 대충 생각해도 시간복잡도가 $O(n^4)$가 나온다</p></li><li><p>$L_{KL}$</p><ul><li>mean=0, var=1에 너무 fit하게 맞춰버리면, denoise 과정에서 발산하는 경우가 있어, KL-divergence를 이용해 부드럽게 균형맞춰준다.</li><li>VAE에서 사용한 idea로, $N(0, 1)$과 $\epsilon(t)$의 KL값을 편하게 계산하도록 유도한 공식이 있다. 이걸 사용한다.
$L_{KL} = \sigma^2 + \mu^2 - 1 - \log{\sigma^2}$</li></ul></li></ul><h3 id=discovering-edit-directions>Discovering Edit Directions<a hidden class=anchor aria-hidden=true href=#discovering-edit-directions>#</a></h3><ul><li>위의 Method부분을 보면 좋다.</li><li>source domain과 target domain의 단어가 포함된 문장을 gpt등의 모델을 통해 생성 (논문에서는 각각 1000개) CLIP기술을 통해 vector로 embedding한 후, 각 단어 벡터의 평균의 차를 구한다.</li></ul><h3 id=editing-via-cross-attention-guidance>Editing via Cross-Attention Guidance<a hidden class=anchor aria-hidden=true href=#editing-via-cross-attention-guidance>#</a></h3><ul><li>차후 채울 예정..</li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://dobe0715.github.io/posts/wgan/><span class=title>« Prev</span><br><span>Wasserstein GAN review</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Zero-shot Image-to-Image Translation(pix2pix zero) review on twitter" href="https://twitter.com/intent/tweet/?text=Zero-shot%20Image-to-Image%20Translation%28pix2pix%20zero%29%20review&amp;url=https%3a%2f%2fdobe0715.github.io%2fposts%2fpix2pix-zero%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Zero-shot Image-to-Image Translation(pix2pix zero) review on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdobe0715.github.io%2fposts%2fpix2pix-zero%2f&amp;title=Zero-shot%20Image-to-Image%20Translation%28pix2pix%20zero%29%20review&amp;summary=Zero-shot%20Image-to-Image%20Translation%28pix2pix%20zero%29%20review&amp;source=https%3a%2f%2fdobe0715.github.io%2fposts%2fpix2pix-zero%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Zero-shot Image-to-Image Translation(pix2pix zero) review on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdobe0715.github.io%2fposts%2fpix2pix-zero%2f&title=Zero-shot%20Image-to-Image%20Translation%28pix2pix%20zero%29%20review"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Zero-shot Image-to-Image Translation(pix2pix zero) review on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdobe0715.github.io%2fposts%2fpix2pix-zero%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Zero-shot Image-to-Image Translation(pix2pix zero) review on whatsapp" href="https://api.whatsapp.com/send?text=Zero-shot%20Image-to-Image%20Translation%28pix2pix%20zero%29%20review%20-%20https%3a%2f%2fdobe0715.github.io%2fposts%2fpix2pix-zero%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Zero-shot Image-to-Image Translation(pix2pix zero) review on telegram" href="https://telegram.me/share/url?text=Zero-shot%20Image-to-Image%20Translation%28pix2pix%20zero%29%20review&amp;url=https%3a%2f%2fdobe0715.github.io%2fposts%2fpix2pix-zero%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Zero-shot Image-to-Image Translation(pix2pix zero) review on ycombinator" href="https://news.ycombinator.com/submitlink?t=Zero-shot%20Image-to-Image%20Translation%28pix2pix%20zero%29%20review&u=https%3a%2f%2fdobe0715.github.io%2fposts%2fpix2pix-zero%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://dobe0715.github.io>블로그 홈</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
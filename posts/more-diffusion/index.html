<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>블로그 홈</title><meta name=keywords content><meta name=description content="참고자료 blogs
(lil log, what are Diffusion Models?) : https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ (Yang song, Generative Modeling by Estimating gradients of the data distribution) : https://yang-song.net/blog/2021/score/ 한국 블로그 : https://deepseow.tistory.com/61 improved DDPM notion : https://sang-yun-lee.notion.site/Improved-Denoising-Diffusion-Probabilistic-Models-efa847335aef4163bfd3ee96c176f659 Diffusion models beats GANs : https://sang-yun-lee.notion.site/Diffusion-Models-Beat-GANs-on-Image-Synthesis-eb1f3826618d42e89d92e489c39f1371 papers
(Improved Denoising Diffusion Probablistic Models) : https://arxiv.org/pdf/2102.09672.pdf (Diffusion models beats GANs on Image synthesis) : https://arxiv.org/pdf/2105.05233.pdf youtubes
(improved DDPM) : https://www.youtube.com/watch?v=8dchQOqvrCE (Diffusion models beats GANs) : https://www.youtube.com/watch?v=bSqA2AIaHy8&amp;t=327s Improved DDPM (contribution)
reverse할 때 variance term도 어느정도 학습 하게 해서 NLL(negative log-likelihood)값을 낮추었다."><meta name=author content="Me"><link rel=canonical href=https://dobe0715.github.io/posts/more-diffusion/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content><meta property="og:description" content="참고자료 blogs
(lil log, what are Diffusion Models?) : https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ (Yang song, Generative Modeling by Estimating gradients of the data distribution) : https://yang-song.net/blog/2021/score/ 한국 블로그 : https://deepseow.tistory.com/61 improved DDPM notion : https://sang-yun-lee.notion.site/Improved-Denoising-Diffusion-Probabilistic-Models-efa847335aef4163bfd3ee96c176f659 Diffusion models beats GANs : https://sang-yun-lee.notion.site/Diffusion-Models-Beat-GANs-on-Image-Synthesis-eb1f3826618d42e89d92e489c39f1371 papers
(Improved Denoising Diffusion Probablistic Models) : https://arxiv.org/pdf/2102.09672.pdf (Diffusion models beats GANs on Image synthesis) : https://arxiv.org/pdf/2105.05233.pdf youtubes
(improved DDPM) : https://www.youtube.com/watch?v=8dchQOqvrCE (Diffusion models beats GANs) : https://www.youtube.com/watch?v=bSqA2AIaHy8&amp;t=327s Improved DDPM (contribution)
reverse할 때 variance term도 어느정도 학습 하게 해서 NLL(negative log-likelihood)값을 낮추었다."><meta property="og:type" content="article"><meta property="og:url" content="https://dobe0715.github.io/posts/more-diffusion/"><meta property="og:image" content="https://dobe0715.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dobe0715.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content><meta name=twitter:description content="참고자료 blogs
(lil log, what are Diffusion Models?) : https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ (Yang song, Generative Modeling by Estimating gradients of the data distribution) : https://yang-song.net/blog/2021/score/ 한국 블로그 : https://deepseow.tistory.com/61 improved DDPM notion : https://sang-yun-lee.notion.site/Improved-Denoising-Diffusion-Probabilistic-Models-efa847335aef4163bfd3ee96c176f659 Diffusion models beats GANs : https://sang-yun-lee.notion.site/Diffusion-Models-Beat-GANs-on-Image-Synthesis-eb1f3826618d42e89d92e489c39f1371 papers
(Improved Denoising Diffusion Probablistic Models) : https://arxiv.org/pdf/2102.09672.pdf (Diffusion models beats GANs on Image synthesis) : https://arxiv.org/pdf/2105.05233.pdf youtubes
(improved DDPM) : https://www.youtube.com/watch?v=8dchQOqvrCE (Diffusion models beats GANs) : https://www.youtube.com/watch?v=bSqA2AIaHy8&amp;t=327s Improved DDPM (contribution)
reverse할 때 variance term도 어느정도 학습 하게 해서 NLL(negative log-likelihood)값을 낮추었다."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://dobe0715.github.io/posts/"},{"@type":"ListItem","position":3,"name":"","item":"https://dobe0715.github.io/posts/more-diffusion/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"","name":"","description":"참고자료 blogs\n(lil log, what are Diffusion Models?) : https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ (Yang song, Generative Modeling by Estimating gradients of the data distribution) : https://yang-song.net/blog/2021/score/ 한국 블로그 : https://deepseow.tistory.com/61 improved DDPM notion : https://sang-yun-lee.notion.site/Improved-Denoising-Diffusion-Probabilistic-Models-efa847335aef4163bfd3ee96c176f659 Diffusion models beats GANs : https://sang-yun-lee.notion.site/Diffusion-Models-Beat-GANs-on-Image-Synthesis-eb1f3826618d42e89d92e489c39f1371 papers\n(Improved Denoising Diffusion Probablistic Models) : https://arxiv.org/pdf/2102.09672.pdf (Diffusion models beats GANs on Image synthesis) : https://arxiv.org/pdf/2105.05233.pdf youtubes\n(improved DDPM) : https://www.youtube.com/watch?v=8dchQOqvrCE (Diffusion models beats GANs) : https://www.youtube.com/watch?v=bSqA2AIaHy8\u0026amp;t=327s Improved DDPM (contribution)\nreverse할 때 variance term도 어느정도 학습 하게 해서 NLL(negative log-likelihood)값을 낮추었다.","keywords":[],"articleBody":"참고자료 blogs\n(lil log, what are Diffusion Models?) : https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ (Yang song, Generative Modeling by Estimating gradients of the data distribution) : https://yang-song.net/blog/2021/score/ 한국 블로그 : https://deepseow.tistory.com/61 improved DDPM notion : https://sang-yun-lee.notion.site/Improved-Denoising-Diffusion-Probabilistic-Models-efa847335aef4163bfd3ee96c176f659 Diffusion models beats GANs : https://sang-yun-lee.notion.site/Diffusion-Models-Beat-GANs-on-Image-Synthesis-eb1f3826618d42e89d92e489c39f1371 papers\n(Improved Denoising Diffusion Probablistic Models) : https://arxiv.org/pdf/2102.09672.pdf (Diffusion models beats GANs on Image synthesis) : https://arxiv.org/pdf/2105.05233.pdf youtubes\n(improved DDPM) : https://www.youtube.com/watch?v=8dchQOqvrCE (Diffusion models beats GANs) : https://www.youtube.com/watch?v=bSqA2AIaHy8\u0026t=327s Improved DDPM (contribution)\nreverse할 때 variance term도 어느정도 학습 하게 해서 NLL(negative log-likelihood)값을 낮추었다. variance값 스캐쥴링을 기존의 linear한 것에서 다른 방법으로 바꾸었다. importance sampling기법을 통해 gradient noise를 줄였다. subsequence를 잡아 sampling speed를 향상시켰다. (review) DDPM Data distribution $x_0 \\sim q(x_0)$ 가 주어졌다고 했을 때, q에 대해 forward process를 각 t step에 대해 $\\beta_t$를 통해 스캐쥴링한다. 이 때, 마르코프 연쇄를 통해 다음과 같이 정의할 수 있다.\n$q(x_t|x_{t-1}) = N(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t 𝐈)$\n이때, 충분히 큰 T를 통해 $x_T$를 만들면, 결국 가우시안 분포를 따르도록 가정한다.\n이제, 이상적인 reverse process(=sampling) $q(x_{t-1}|x_t)$를 직접적으로 계산할 수 없어서 NN($p_{\\theta})$을 통해 다음과 같이 근사한다.\n$p_\\theta(x_{t-1}|x_t) := N(x_{t-1};\\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$ 이렇게 잘 정의한 $p_\\theta$를 variational lower bound를 통해 loss를 구해주면, 다음과 같다.\n여기에서 $L_0$와 $L_T$는 DDPM에서 고려하지 않았고, 1 ~ T-1 의 loss를 계산하기 위해 다음과 같이 marginal을 정리하였다.\n$q(x_t|x_0) = N(x_t;\\sqrt{\\bar{\\alpha}_t}x_0, (1-\\bar{\\alpha}_t)I)$ $x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon$ 이제, x_t와 x_0가 주어졌을 때 (reverse에서)예측하고자 하는 posterior $q(x_{t-1}|x_t, x_0)$를 다음과 같이 가우스 커널을 통해 계산한다. 그러면, 원하는 $\\tilde{\\beta}_t$, $\\tilde{\\mu}_t(x_t, x_0)$값이 유도된다.\n위에서 유도한 $\\tilde{\\mu}$와 $\\tilde{\\beta}$에서, $\\tilde{\\beta}$는 forward에서 사용한 $\\beta$를 사용하고 $\\tilde{\\mu}$값만 이용해 대입해서 다음과 같이 구하고자 하는 평균에 대한 값의 파라미터화를 유도한다.\n$\\mu_\\theta(x_t, t) = \\cfrac{1}{\\sqrt{\\alpha_t}}(x_t - \\cfrac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}t}}\\epsilon\\theta(x_t, t))$ 이것을 가지고 각 t step에다가 대입해서 앞의 계수 항들 간단히 하여 최종적으로 다음의 단순화 된 loss term을 얻게 된다.\n$L_{simple} = E_{t, x_0, \\epsilon}\\big[||\\epsilon - \\epsilon_\\theta(x_t, t)||^2\\big]$ 앞으로 이 논문에서 주목할 점은 variance이다.\nImproving the log-likelihood Learning $\\Sigma_\\theta(x_t, t)$ 이전의 DDPM에서는 variance는 학습하지 않도록 하였다. 심지어, 유도된 posterior를 사용하지도 않고 원래의 forward에서의 variance값을 사용했었다.\ni.e. $(posterior) : \\tilde{\\beta}t = \\cfrac{1-\\bar{\\alpha}{t-1}}{1-\\bar{\\alpha}_t}\\beta_t$ but, not use $\\tilde{\\beta}$. $\\sigma_t^2 = \\beta_t$ 이 때, $\\beta_t$ : upper bound, $\\tilde{\\beta_t}$ : lower bound라고 이야기하였다.\n그렇다면, 기존 DDPM에서 저 분산값 사용에 있어서 NLL값 손해가 있지 않을까 해서 값 실험을 해보았다.\ntime step의 완전 초반부를 제외하면, 두 값이 비슷하므로 t값이 커지게 되면 분산부분은 sampling 퀄리티에 크게 영향을 미치지 않는다 초반의 loss term의 step을 보변 전체 loss에 기여하는 정도가 상당히 크다. 따라서, 기존에처럼 고정된 variance를 사용하기보다, 학습시켜서 likelihood를 확 낮출 수 있을듯하다. 이 때, 우리가 학습하고자 하는 $\\Sigma_\\theta$는 작은 값이고, NN을통해 직접적으로 예측하는 것은 어렵기 때문에 $\\beta$와 $\\tilde{\\beta}$를 interpolation하고, 그 정도를 학습하도록 하였다.\n$\\Sigma_\\theta(x_t, t) = \\exp(v\\log{\\beta_t} + (1 - v)\\log{\\tilde{\\beta}_t})$ sigma 값을 저 둘 사이의 값으로 가정하는 근거는??\n(2015, Deep Unsupervised Learning using Nonequilibrium Thermodynamics) : diffusion 시초 논문\nconditional entropy의 관점에서 보았을 때, 이러한 관계가 성립하고, 가우시안 커널을 이용해 구한 값에 대해서 bounded 값을 잘 설명할 수 있다.\n최종적으로, $L_{simple}$은 분산값에 영향을 받지 않으므로 추가 loss term을 추가해준다. 즉, 기존에 학습이 잘 되는 것이 증명이 된 simple텀을 메인으로 두고, vlb텀이 분산값에 따라 guide해주도록 학습한다.\n$L_{hybrid} = L_{simple} + \\lambda L_{vlb}$ $\\lambda$는 0.001, vlb term에서 $\\mu_\\theta$부분은 stop gradient를 적용하여 평균은 simple이 main으로 학습하도록 하였다.\n200K 4K cosine부분을 살펴보면, simple과 vlb는 사용하면 FID, NLL에 대해 trade off를 가짐을 알 수 있다.\nImproving the Noise Schedule 기존의 linear한 scaduling은 noise가 너무 초반부터 들어가서 information을 빨리 붕괴시키는 경향이 있다. 따라서 이를 좀 천천히 들어가게끔 cosine을 통해 다시 정의해주었다.\n$\\bar{\\alpha}_t = \\cfrac{f(t)}{f(0)}$, $f(t) = cos\\bigg(\\cfrac{t/T + s}{1 + s} \\cdot \\cfrac{\\pi}{2} \\bigg)^2$ 이를 통해, 필요하다면 역으로 $\\beta_t$도 쉽게 계산할 수 있다.\nReducing Gradient Noise NLL 관점에서, hybrid loss보단, vlb loss를 그냥 쓰는 것이 당연히 좋지만 실제로 이를 optimize하는 것은 상당히 어려웠다고 한다.\n우선, 가장 큰 문제점으로 본 것은 gradient noise가 vlb의 경우 컸다는 것이다.(가정)\ngradient noise ( != value noise) : optimization(대표적으로 SGD)을 하는 중, 전체 데이터 셋과 미니배치 셋으로부터의 gradient의 오차\nbatch size가 클 수록, 당연히 전체 데이터셋의 gradient대로 잘 가고 빠르게 움직인다. 하지만, 반대의 경우 gradient noise가 커지게 된다. vlb에서의 variance를 낮추는 것에 집중하는데 그러기 위해선 이유를 알아야 한다. 이 논문에선 훈련할 때, vlb의 경우 loss에 영향을 미치는 정도가 time step마다 다른데(figure2), t를 uniformly하게 sampling하기 때문이라고 한다.\n따라서, 중요도에 따른 sampling을 제안한다.\n$L_{vlb} = E_{t \\sim p_t}\\bigg[\\cfrac{L_t}{p_t} \\bigg]$, where $p_t \\propto \\sqrt{E[L_t^2]}, \\sum{p_t} = 1$ 이 때, $E[L_t^2]$ 값은 매번 계산하는 값이기 때문에, 훈련동안 dynamically하게 갱신해준다(앞의 10개 값을 이용).\n각 loss의 값을 일정하게 scaling해주는 대신에, 중요도가 높은 loss는 자주 sampling되어 갱신시켜준다. Improving Sampling Speed 본 논문은 모델을 4000번의 diffusion step을 훈련하도록 하였다. 이 때, sampling을 하면서 걸리는 시간과 GPU가 굉장히 크다. 하지만, 이 논문의 모델은 sub sequence를 통해 sampling 하여도 이미지의 퀄리티를 유지할 수 있다.\n$\\beta_{S_t} = 1 - \\cfrac{\\bar{\\alpha}{S_t}}{\\bar{\\alpha}{S_{t-1}}}$, $\\tilde\\beta_{S_t} = \\cfrac{1 - \\bar{\\alpha}{S_t}}{1 - \\bar{\\alpha}{S_{t-1}}}\\beta_{S_t}$ 이와 같이 잡게되면, 잘 샘플링 되는데 왜 잘 되는지 생각해보면, 기존의 DDPM은 reverse할 때 $\\beta_t$값 만을 사용했 기 때문에, time step을 건너뛰면 그 중간 정보가 소실된다. (DDIM에서의 실험 생각해보면 그 차이가 상당히 큼을 알 수 있다. 즉, $\\beta$ 를 사용한것과 $\\tilde\\beta$를 사용했을 때의 차이가 드러난다.)\n하지만, 본 논문에서는 $\\tilde\\beta_t$를 사용하였고(정확하게는 interpolation한 값인 $\\Sigma_\\theta(x_t, t)$), 그렇기 때문에 NLL, FID에서의 score 선방을 할 수 있었다.\nDiffusion Models Beat GANs on Image Synthesis Contributions 모델 아키텍쳐 tuning attention head 증가 다양한 resolution 층에 attention 사용(기존에는 16x16 -\u003e 8x8, 16x16, 32x32) Big GAN의 residual block을 upsampling, downsampling에 사용 AdaGN 적용 Classifier Guidence 사용하여 FID score 상승 (기존의 GAN 모델을 이김!) Back ground Improved DDPM $\\Sigma_\\theta(x_t, t) = \\exp(v\\log{\\beta_t} + (1 - v)\\log{\\tilde{\\beta}_t})$\nSample Quality Metrics Inception Score ImageNet에서 한가지 class의 distribution에 대해 고정시켜 학습시키고 그것에 대해 __Sharpness, Diversity__를 계산한다.\n모델이 small subset에 대해 적합해져도 점수가 잘나오는 문제점이 존재. $S = \\exp{(E_{x \\sim p}\\big[\\int{c(y|x)\\log{c(y|x)}}dy\\big])}$\n$D = \\exp{(E_{x \\sim p}\\big[\\int{c(y|x)\\log{c(y)}}dy\\big])}$\nS : Sharpness\nclassifier가 확신을 가지고 predict하는가 S가 증가하면, c(y|x)의 엔트로피가 감소한다. -\u003e 데이터들이 잘 분리되어있다 D : Diversity\n얼마나 다양하게 생성하는가 D가 증가하면, marginal distribution인 c(y)의 엔트로피가 증가한다. -\u003e 데이터들이 다양하다 $IS = S \\cdot D$\n이 때, classifier인 $c(\\cdot)$은 ImageNet을 학습한 Inception V3 모델을 사용. 그래서 Inception score라는 이름이 붙어졌다.\nFrechet Inception Score (FID score) 학습시킨 ganerated model로부터 데이터들을 모아두고, test할 데이터를 모아서 각각 같은 모델을 이용해 feature extract를 진행한다. 그리고, 각각의 feature의 분포에 대해 거리를 측정한다. (by wasserstein-2 distance) data space로부터 분포간의 거리를 측정하면 너무 흩어져 있기 때문에 이와 같이 하나의 encoder net을 이용해 모아준다.(manifold hypothesis) Inception score보다 더 Human Judgement에 가깝다는 장점 $FID = ||\\mu_T - \\mu_G||^2_2 + Tr(\\Sigma_T + \\Sigma_G - 2\\sqrt{\\Sigma_T\\Sigma_G})$\n왼쪽부터 각각 Fidelity, Diversity에 해당한다. 즉, 점수가 낮으면 낮을 수록 퀄리티가 좋고 다양성이 풍부한 데이터라고 볼 수 있다. (정확히는 더욱 testing sample space와 유사한 데이터를 만들어낸다.) Architecture Improvement 여러가지 tuning 기법을 이용해 모델의 성능을 끌어올렸다 정도.. Adaptive Group Normalization (AdaGN) $AdaGN(h, y) = y_sGroupNorm(h) + y_b$\nGroup Normalization을 하는 경우 batch size가 너무 작아 의미 없을 때 NLP의 경우 입력 크기의 다름, 미니배치 분포의 다양성 등등 이 외에도 여러 이유가 있는데, batch norm이 아닌, layer norm을 했을 때 conv net의 대해서도 오히려 학습이 잘되고 성능이 좋은 경우도 있다.\n그래서 요새는 웬만하면 batch norm을 하지만, layer norm (group norm)을 사용하는 경우도 더러 있다고 한다.\n궁금증 : 어떤 상황에서 어떤 normalization을 적용하는 것이 좋은가??\nClassifier Guidance 이 논문에서 가장 중요한 부분. 기존의 GAN모델이 FID 점수가 잘 나왔던 이유가, diversity와 fidelity를 trade off로 교환하였기 때문에 퀄리티가 좋았다고 한다.\n즉, Descriminator를 잘 속이기만 하면 되기 때문에 diversity가 낮고 대신에 퀄리티가 좋다.\nDDPM에서도 의도적으로 diversity를 낮춘다면 퀄리티가 올라가고 이를 trade off로서 FID 점수를 높일 수 있지 않을까?\n핵심은 classifier를 어떻게 정의할 것인가이다.(위에 normalizing할 때 class를 정의하긴 했지만 그것 말고 다른 방법을 제시하고 있다.)\n해당 논문에서는 $p_\\phi(y|x_t, t) = p_\\phi(y|x_t), \\epsilon_\\theta(x_t, t) = \\epsilon_\\theta(x_t)$ 라는 notation을 사용하고 있음을 생각하고 이어지는 수식을 따라가보자.\n목표 : $p\\phi(y|x_t)$를 어떻게 잘 정의할 것인가_\n(이 때, 해당 $p_\\phi$는 generating 모델을 y로 유도하게 해주는 역할이다.)\n원하는 방향은 다음과 같다. $p_\\phi$를 학습시키게 되면, noise image x_t를 sampling 할 때, gradient $\\nabla_{x_t}\\log{p_\\phi(y|x_t, t)}$를 이용해서 임의의 label y로 유도한다.\n1. Conditional Reverse Noising Process 기존 diffusion 모델의 unconditional reverse noising process 에서 시작한다.($p_\\theta(x_t|x_{t+1})$)\n임의의 컨디션 라벨 y 에 대해서 다음의 식이 성립한다.(확률의 성질에 의해)\n$p_{\\theta, \\phi}(x_t|x_{t+1}, y) = Zp_{\\theta}(x_t|x_{t+1})p_{\\phi}(y|x_t)$(2)\n(Z는 normalizing 상수)\n그러면, 여기로부터 sampling algorithm을 유도할 수 있다.\n기존의 diffusion model으로부터 sampling을 $p_\\theta(x_t|x_{t+1}) = N(\\mu, \\Sigma)$\n라고 간소화 하여 쓸 수 있고, 여기에 log likelihood를 이용해 표현하면 다음과 같다.\n$\\log{p_\\theta(x_t|x_{t+1})} = -\\cfrac{1}{2}(x_t - \\mu)^T \\Sigma^{-1} (x_t - \\mu) + C$ (4)\n이 때, $\\log{p_\\phi(y|x_t)}$가 $\\Sigma^{-1}$에 비해 낮은 곡률을 갖는다고 가정한다.\n즉, diffusion step이 무한히 발산하지 않을 때를 가정한다.($||\\Sigma|| \\rightarrow 0$일 때)\n그러면, $x_t = \\mu$일 때 taylor expansion을 이용해 $\\log{p_\\phi(y|x_t)}$를 근사할 수 있게된다!!!\n$\\log{p_\\phi(y|x_t)}$\n$\\simeq \\log{p_\\phi(y|x_t)}|{x_t = \\mu} + (x_t - \\mu)\\nabla{x_t}\\log{p_\\phi(y|x_t)|{x_t = \\mu}}$ (5)\n$= (x_t - \\mu)g + C_1$(6)\n(이 때, $g = \\nabla{x_t}\\log_{\\phi}(y|x_t)|_{x_t = \\mu}$)\n이제, (2)의 식에 (4),(6)을 대입해서 전개하고 constant term을 제거해주면 다음의 식 (10)을 얻는다.\n$$\\log{(p_\\theta(x_t|x_{t+1})p_{\\phi}(y|x_t))}\\simeq \\log{p(z)} + C_4, z \\sim N(\\mu + \\Sigma g, \\Sigma)$$\n이제 여기서 log를 양변에서 지워주고나면 남아있는 $C_4$가 처음에 normalization 상수 Z이자, 밑의 gradient scale s를 결정짓는 상수가 된다\n이로부터 다음의 sampling 알고리즘이 유도된다!!\n전체적으로 sampling되는 그림을 보면 다음과 같이 볼 수 있다.\n2. Conditional Sampling for DDIM 위의 방식으로 근사하는 것은 확률적 모델인 DDPM에 적용되지만, 결정적 모델인 DDIM에는 사용할 수 없다. (위에서 보여지는 것처럼 z로부터 random성을 이용하기 때문..)\n따라서, Song의 논문을 빌려, score-based conditioning trick을 이용해 접근한다.\n$\\nabla_{x_t}\\log{q(x_t)} = -\\cfrac{\\epsilon_\\theta(x_t)}{\\sqrt{1-\\bar{\\alpha}_t}}$ (11)\n위의 식은 deterministic sampling에 적용되는 model $\\epsilon_\\theta(x_t)$에 대해 위와 같은 함수가 유도된다.\n$d\\mathbf{x} = \\bigg[\\mathbf{f}(\\mathbf{x}, t) - g(t)^2\\bigtriangledown_x\\log{p_t(\\mathbf{x})}\\bigg]dt + g(t)d\\mathbf{w}$\n를 이용한듯 싶음..\n이제, 이를 앞의 절에서 한것 처럼 $p(x_t)p(y|x_t)$에다가 대입해보자.\n$\\nabla_{x_t}\\log{(p_\\theta(x_t)p_\\phi(y|x_t))} = \\nabla_{x_t}\\log{p_\\theta(x_t)} + \\nabla_{x_t}\\log{p_\\phi(y|x_t)}$\n$= -\\cfrac{1}{\\sqrt{1-\\bar{\\alpha}t}}\\epsilon\\theta(x_t) + \\nabla_{x_t}\\log{p_\\phi(y|x_t)}$ (13)\n위를 앞의 상수로 다시 묶어서 최종적으로 새로운 noise prediction을 만들어 볼 수 있다. $\\hat{\\epsilon}(x_t) := \\epsilon_\\theta(x_t) - \\sqrt{1-\\bar{\\alpha}t}\\nabla{x_t}\\log{p_{\\phi}}(y|x_t)$ (14)\n이로부터 아래의 Classifier guided DDIM sampling 알고리즘이 정의된다.\n3. Scaling Classifier Gradients 이제, large scale의 generation task에 classifier를 적용하기 위해 먼저 classifier를 ImageNet에 학습을 시켰다고 한다. 이 때의 모델은 UNet 이고, 8x8 size의 layer를 downsampling 시켜 분류 문제를 해결하도록 변형하였다.\n그리고, diffusion model에서 사용한 noise와 동일한 분포에 대해서 classifier를 학습시키고, 과적합을 방지하기 위해 randomcrop augmentation을 적용하였다고 한다.\n이후, 학습이 종료되고나서 (10)의 식을 이용하여 diffusion모델과 classifier를 결합시켰다.\n실험에서 classifier gradient가 (s 값)1이면, 50%정도의 sample 정확도를 가짐을 알 수 있다. 하지만, 실제로 sampling을 해보면 그에 미치지 못한다. 그래서 실험을 하다보니 1보다 큰 값을 잡아야 함을 알게 되었다.\n$s \\cdot \\nabla_x \\log{p(y|x)} = \\nabla_x \\log{\\cfrac{1}{Z}p(y|x)^s}$의 관계로부터 이해할 수 있다.\ns값이 커지면, 지수적으로 값이 폭발적으로 오르기 때문데, 더욱 점점 더 sharp 해지게 되고 다시말하면 classifier가 더욱 mode에 집중하게 됨을 의미한다.\n즉, 더 높은 fidelity를 얻고 diversity를 낮추는 것이 된다.\nclassifier gradient값을 높일 수록, FID 점수가 높게 나옴을 알 수 있다.(fidelity 높아짐) 또한, sFID는 전체 데이터에 대한 값인데, 갈수록 낮게나오는 모습을 통해 모델의 diversity가 낮아짐을 알 수 있다. 정말 모든 데이터 셋에 대해서 기존의 모델을 beat했다\n또한 추가적으로, 이미지의 퀄리티도 GAN보다 좋은데 다양성마저 더 좋다.\n","wordCount":"1726","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://dobe0715.github.io/posts/more-diffusion/"},"publisher":{"@type":"Organization","name":"블로그 홈","logo":{"@type":"ImageObject","url":"https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dobe0715.github.io accesskey=h title="Home (Alt + H)"><img src=https://dobe0715.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dobe0715.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://dobe0715.github.io/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dobe0715.github.io>Home</a>&nbsp;»&nbsp;<a href=https://dobe0715.github.io/posts/>Posts</a></div><h1 class=post-title></h1><div class=post-meta>9 min&nbsp;·&nbsp;1726 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/more%20diffusion.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><h2 id=참고자료>참고자료<a hidden class=anchor aria-hidden=true href=#참고자료>#</a></h2><ul><li><p>blogs</p><ul><li>(lil log, what are Diffusion Models?) : <a href=https://lilianweng.github.io/posts/2021-07-11-diffusion-models/>https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</a></li><li>(Yang song, Generative Modeling by Estimating gradients of the data distribution) : <a href=https://yang-song.net/blog/2021/score/>https://yang-song.net/blog/2021/score/</a></li><li>한국 블로그 : <a href=https://deepseow.tistory.com/61>https://deepseow.tistory.com/61</a></li><li>improved DDPM notion : <a href=https://sang-yun-lee.notion.site/Improved-Denoising-Diffusion-Probabilistic-Models-efa847335aef4163bfd3ee96c176f659>https://sang-yun-lee.notion.site/Improved-Denoising-Diffusion-Probabilistic-Models-efa847335aef4163bfd3ee96c176f659</a></li><li>Diffusion models beats GANs : <a href=https://sang-yun-lee.notion.site/Diffusion-Models-Beat-GANs-on-Image-Synthesis-eb1f3826618d42e89d92e489c39f1371>https://sang-yun-lee.notion.site/Diffusion-Models-Beat-GANs-on-Image-Synthesis-eb1f3826618d42e89d92e489c39f1371</a></li></ul></li><li><p>papers</p><ul><li>(Improved Denoising Diffusion Probablistic Models) : <a href=https://arxiv.org/pdf/2102.09672.pdf>https://arxiv.org/pdf/2102.09672.pdf</a></li><li>(Diffusion models beats GANs on Image synthesis) : <a href=https://arxiv.org/pdf/2105.05233.pdf>https://arxiv.org/pdf/2105.05233.pdf</a></li></ul></li><li><p>youtubes</p><ul><li>(improved DDPM) : <a href="https://www.youtube.com/watch?v=8dchQOqvrCE">https://www.youtube.com/watch?v=8dchQOqvrCE</a></li><li>(Diffusion models beats GANs) : <a href="https://www.youtube.com/watch?v=bSqA2AIaHy8&amp;t=327s">https://www.youtube.com/watch?v=bSqA2AIaHy8&amp;t=327s</a></li></ul></li></ul><h1 id=improved-ddpm>Improved DDPM<a hidden class=anchor aria-hidden=true href=#improved-ddpm>#</a></h1><p>(contribution)</p><ol><li>reverse할 때 variance term도 어느정도 학습 하게 해서 NLL(negative log-likelihood)값을 낮추었다.</li><li>variance값 스캐쥴링을 기존의 linear한 것에서 다른 방법으로 바꾸었다.</li><li>importance sampling기법을 통해 gradient noise를 줄였다.</li><li>subsequence를 잡아 sampling speed를 향상시켰다.</li></ol><h3 id=review-ddpm>(review) DDPM<a hidden class=anchor aria-hidden=true href=#review-ddpm>#</a></h3><p>Data distribution $x_0 \sim q(x_0)$ 가 주어졌다고 했을 때, q에 대해 forward process를 각 t step에 대해 $\beta_t$를 통해 스캐쥴링한다. 이 때, 마르코프 연쇄를 통해 다음과 같이 정의할 수 있다.</p><ul><li><p>$q(x_t|x_{t-1}) = N(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t 𝐈)$</p></li><li><p>이때, 충분히 큰 T를 통해 $x_T$를 만들면, 결국 가우시안 분포를 따르도록 가정한다.</p></li></ul><p>이제, 이상적인 reverse process(=sampling) $q(x_{t-1}|x_t)$를 직접적으로 계산할 수 없어서 NN($p_{\theta})$을 통해 다음과 같이 근사한다.</p><ul><li>$p_\theta(x_{t-1}|x_t) := N(x_{t-1};\mu_\theta(x_t, t), \Sigma_\theta(x_t, t))$</li></ul><p>이렇게 잘 정의한 $p_\theta$를 variational lower bound를 통해 loss를 구해주면, 다음과 같다.<br></p><p>여기에서 $L_0$와 $L_T$는 DDPM에서 고려하지 않았고, 1 ~ T-1 의 loss를 계산하기 위해 다음과 같이 marginal을 정리하였다.</p><ul><li>$q(x_t|x_0) = N(x_t;\sqrt{\bar{\alpha}_t}x_0, (1-\bar{\alpha}_t)I)$</li><li>$x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon$</li></ul><p>이제, x_t와 x_0가 주어졌을 때 (reverse에서)예측하고자 하는 posterior $q(x_{t-1}|x_t, x_0)$를 다음과 같이 가우스 커널을 통해 계산한다. 그러면, 원하는 $\tilde{\beta}_t$, $\tilde{\mu}_t(x_t, x_0)$값이 유도된다.</p><p>위에서 유도한 $\tilde{\mu}$와 $\tilde{\beta}$에서, $\tilde{\beta}$는 forward에서 사용한 $\beta$를 사용하고 $\tilde{\mu}$값만 이용해 대입해서 다음과 같이 구하고자 하는 평균에 대한 값의 파라미터화를 유도한다.</p><ul><li>$\mu_\theta(x_t, t) = \cfrac{1}{\sqrt{\alpha_t}}(x_t - \cfrac{\beta_t}{\sqrt{1-\bar{\alpha}<em>t}}\epsilon</em>\theta(x_t, t))$</li></ul><p>이것을 가지고 각 t step에다가 대입해서 앞의 계수 항들 간단히 하여 최종적으로 다음의 단순화 된 loss term을 얻게 된다.</p><ul><li>$L_{simple} = E_{t, x_0, \epsilon}\big[||\epsilon - \epsilon_\theta(x_t, t)||^2\big]$</li></ul><p>앞으로 이 논문에서 주목할 점은 variance이다.</p><h2 id=improving-the-log-likelihood>Improving the log-likelihood<a hidden class=anchor aria-hidden=true href=#improving-the-log-likelihood>#</a></h2><h3 id=learning-sigma_thetax_t-t>Learning $\Sigma_\theta(x_t, t)$<a hidden class=anchor aria-hidden=true href=#learning-sigma_thetax_t-t>#</a></h3><p>이전의 DDPM에서는 variance는 학습하지 않도록 하였다. 심지어, 유도된 posterior를 사용하지도 않고 원래의 forward에서의 variance값을 사용했었다.</p><ul><li>i.e. $(posterior) : \tilde{\beta}<em>t = \cfrac{1-\bar{\alpha}</em>{t-1}}{1-\bar{\alpha}_t}\beta_t$</li><li>but, not use $\tilde{\beta}$.</li><li>$\sigma_t^2 = \beta_t$</li></ul><p>이 때, $\beta_t$ : upper bound, $\tilde{\beta_t}$ : lower bound라고 이야기하였다.</p><p>그렇다면, 기존 DDPM에서 저 분산값 사용에 있어서 NLL값 손해가 있지 않을까 해서 값 실험을 해보았다.</p><p></p><ol><li>time step의 완전 초반부를 제외하면, 두 값이 비슷하므로 t값이 커지게 되면 분산부분은 sampling 퀄리티에 크게 영향을 미치지 않는다</li><li>초반의 loss term의 step을 보변 전체 loss에 기여하는 정도가 상당히 크다. 따라서, 기존에처럼 고정된 variance를 사용하기보다, 학습시켜서 likelihood를 확 낮출 수 있을듯하다.</li></ol><p>이 때, 우리가 학습하고자 하는 $\Sigma_\theta$는 작은 값이고, NN을통해 직접적으로 예측하는 것은 어렵기 때문에 $\beta$와 $\tilde{\beta}$를 interpolation하고, 그 정도를 학습하도록 하였다.</p><ul><li>$\Sigma_\theta(x_t, t) = \exp(v\log{\beta_t} + (1 - v)\log{\tilde{\beta}_t})$</li></ul><p><strong>sigma 값을 저 둘 사이의 값으로 가정하는 근거는??</strong></p><p>(2015, Deep Unsupervised Learning using
Nonequilibrium Thermodynamics) : <strong>diffusion 시초 논문</strong><br><br>conditional entropy의 관점에서 보았을 때, 이러한 관계가 성립하고, 가우시안 커널을 이용해 구한 값에 대해서 bounded 값을 잘 설명할 수 있다.</p><p>최종적으로, $L_{simple}$은 분산값에 영향을 받지 않으므로 추가 loss term을 추가해준다. 즉, 기존에 학습이 잘 되는 것이 증명이 된 simple텀을 메인으로 두고, vlb텀이 분산값에 따라 guide해주도록 학습한다.</p><ul><li>$L_{hybrid} = L_{simple} + \lambda L_{vlb}$</li></ul><p>$\lambda$는 0.001, vlb term에서 $\mu_\theta$부분은 stop gradient를 적용하여 평균은 simple이 main으로 학습하도록 하였다.</p><p>200K 4K cosine부분을 살펴보면, simple과 vlb는 사용하면 FID, NLL에 대해 trade off를 가짐을 알 수 있다.</p><h3 id=improving-the-noise-schedule>Improving the Noise Schedule<a hidden class=anchor aria-hidden=true href=#improving-the-noise-schedule>#</a></h3><p>기존의 linear한 scaduling은 noise가 너무 초반부터 들어가서 information을 빨리 붕괴시키는 경향이 있다. 따라서 이를 좀 천천히 들어가게끔 cosine을 통해 다시 정의해주었다.</p><ul><li>$\bar{\alpha}_t = \cfrac{f(t)}{f(0)}$, $f(t) = cos\bigg(\cfrac{t/T + s}{1 + s} \cdot \cfrac{\pi}{2} \bigg)^2$</li></ul><p>이를 통해, 필요하다면 역으로 $\beta_t$도 쉽게 계산할 수 있다.</p><h3 id=reducing-gradient-noise>Reducing Gradient Noise<a hidden class=anchor aria-hidden=true href=#reducing-gradient-noise>#</a></h3><p>NLL 관점에서, hybrid loss보단, vlb loss를 그냥 쓰는 것이 당연히 좋지만 실제로 이를 optimize하는 것은 상당히 어려웠다고 한다.</p><p></p><p>우선, 가장 큰 문제점으로 본 것은 gradient noise가 vlb의 경우 컸다는 것이다.(가정)</p><ul><li>gradient noise ( != value noise) : optimization(대표적으로 SGD)을 하는 중, 전체 데이터 셋과 미니배치 셋으로부터의 gradient의 오차<br></li><li>batch size가 클 수록, 당연히 전체 데이터셋의 gradient대로 잘 가고 빠르게 움직인다. 하지만, 반대의 경우 gradient noise가 커지게 된다.</li></ul><p>vlb에서의 variance를 낮추는 것에 집중하는데 그러기 위해선 이유를 알아야 한다. 이 논문에선 훈련할 때, vlb의 경우 loss에 영향을 미치는 정도가 time step마다 다른데(figure2), t를 uniformly하게 sampling하기 때문이라고 한다.<br>따라서, 중요도에 따른 sampling을 제안한다.</p><ul><li>$L_{vlb} = E_{t \sim p_t}\bigg[\cfrac{L_t}{p_t} \bigg]$, where $p_t \propto \sqrt{E[L_t^2]}, \sum{p_t} = 1$</li></ul><p>이 때, $E[L_t^2]$ 값은 매번 계산하는 값이기 때문에, 훈련동안 dynamically하게 갱신해준다(앞의 10개 값을 이용).</p><ul><li>각 loss의 값을 일정하게 scaling해주는 대신에, 중요도가 높은 loss는 자주 sampling되어 갱신시켜준다.</li></ul><h3 id=improving-sampling-speed>Improving Sampling Speed<a hidden class=anchor aria-hidden=true href=#improving-sampling-speed>#</a></h3><p>본 논문은 모델을 4000번의 diffusion step을 훈련하도록 하였다. 이 때, sampling을 하면서 걸리는 시간과 GPU가 굉장히 크다. 하지만, 이 논문의 모델은 sub sequence를 통해 sampling 하여도 이미지의 퀄리티를 유지할 수 있다.</p><ul><li>$\beta_{S_t} = 1 - \cfrac{\bar{\alpha}<em>{S_t}}{\bar{\alpha}</em>{S_{t-1}}}$, $\tilde\beta_{S_t} = \cfrac{1 - \bar{\alpha}<em>{S_t}}{1 - \bar{\alpha}</em>{S_{t-1}}}\beta_{S_t}$
이와 같이 잡게되면, 잘 샘플링 되는데 왜 잘 되는지 생각해보면,</li></ul><p>기존의 DDPM은 reverse할 때 $\beta_t$값 만을 사용했 기 때문에, time step을 건너뛰면 그 중간 정보가 소실된다. (DDIM에서의 실험 생각해보면 그 차이가 상당히 큼을 알 수 있다. 즉, $\beta$ 를 사용한것과 $\tilde\beta$를 사용했을 때의 차이가 드러난다.)<br>하지만, 본 논문에서는 $\tilde\beta_t$를 사용하였고(정확하게는 interpolation한 값인 $\Sigma_\theta(x_t, t)$), 그렇기 때문에 NLL, FID에서의 score 선방을 할 수 있었다.</p><h1 id=diffusion-models-beat-gans--on-image-synthesis>Diffusion Models Beat GANs on Image Synthesis<a hidden class=anchor aria-hidden=true href=#diffusion-models-beat-gans--on-image-synthesis>#</a></h1><h2 id=contributions>Contributions<a hidden class=anchor aria-hidden=true href=#contributions>#</a></h2><ol><li>모델 아키텍쳐 tuning<ul><li>attention head 증가</li><li>다양한 resolution 층에 attention 사용(기존에는 16x16 -> 8x8, 16x16, 32x32)</li><li>Big GAN의 residual block을 upsampling, downsampling에 사용</li><li>AdaGN 적용</li></ul></li><li>Classifier Guidence 사용하여 FID score 상승 (기존의 GAN 모델을 이김!)</li></ol><h2 id=back-ground>Back ground<a hidden class=anchor aria-hidden=true href=#back-ground>#</a></h2><h3 id=improved-ddpm-1>Improved DDPM<a hidden class=anchor aria-hidden=true href=#improved-ddpm-1>#</a></h3><p>$\Sigma_\theta(x_t, t) = \exp(v\log{\beta_t} + (1 - v)\log{\tilde{\beta}_t})$</p><h3 id=sample-quality-metrics>Sample Quality Metrics<a hidden class=anchor aria-hidden=true href=#sample-quality-metrics>#</a></h3><h4 id=inception-score>Inception Score<a hidden class=anchor aria-hidden=true href=#inception-score>#</a></h4><ul><li><p>ImageNet에서 한가지 class의 distribution에 대해 고정시켜 학습시키고 그것에 대해 __Sharpness, Diversity__를 계산한다.</p><ul><li><strong>모델이 small subset에 대해 적합해져도 점수가 잘나오는 문제점이 존재.</strong></li></ul></li></ul><p>$S = \exp{(E_{x \sim p}\big[\int{c(y|x)\log{c(y|x)}}dy\big])}$<br>$D = \exp{(E_{x \sim p}\big[\int{c(y|x)\log{c(y)}}dy\big])}$</p><p>S : Sharpness</p><ul><li>classifier가 확신을 가지고 predict하는가</li><li>S가 증가하면, c(y|x)의 엔트로피가 감소한다. -> 데이터들이 잘 분리되어있다</li></ul><p>D : Diversity</p><ul><li>얼마나 다양하게 생성하는가</li><li>D가 증가하면, marginal distribution인 c(y)의 엔트로피가 증가한다. -> 데이터들이 다양하다</li></ul><p>$IS = S \cdot D$</p><p>이 때, classifier인 $c(\cdot)$은 ImageNet을 학습한 Inception V3 모델을 사용. 그래서 Inception score라는 이름이 붙어졌다.</p><h4 id=frechet-inception-score-fid-score>Frechet Inception Score (FID score)<a hidden class=anchor aria-hidden=true href=#frechet-inception-score-fid-score>#</a></h4><ul><li>학습시킨 ganerated model로부터 데이터들을 모아두고, test할 데이터를 모아서 각각 같은 모델을 이용해 feature extract를 진행한다. 그리고, 각각의 feature의 분포에 대해 거리를 측정한다. (by wasserstein-2 distance)<ul><li>data space로부터 분포간의 거리를 측정하면 너무 흩어져 있기 때문에 이와 같이 하나의 encoder net을 이용해 모아준다.(manifold hypothesis)</li><li><strong>Inception score보다 더 Human Judgement에 가깝다는 장점</strong></li></ul></li></ul><p>$FID = ||\mu_T - \mu_G||^2_2 + Tr(\Sigma_T + \Sigma_G - 2\sqrt{\Sigma_T\Sigma_G})$</p><ul><li>왼쪽부터 각각 Fidelity, Diversity에 해당한다.</li><li>즉, 점수가 낮으면 낮을 수록 퀄리티가 좋고 다양성이 풍부한 데이터라고 볼 수 있다. (정확히는 더욱 testing sample space와 유사한 데이터를 만들어낸다.)</li></ul><h2 id=architecture-improvement>Architecture Improvement<a hidden class=anchor aria-hidden=true href=#architecture-improvement>#</a></h2><p></p><ul><li>여러가지 tuning 기법을 이용해 모델의 성능을 끌어올렸다 정도..</li></ul><h3 id=adaptive-group-normalization-adagn>Adaptive Group Normalization (AdaGN)<a hidden class=anchor aria-hidden=true href=#adaptive-group-normalization-adagn>#</a></h3><p>$AdaGN(h, y) = y_sGroupNorm(h) + y_b$</p><ul><li>Group Normalization을 하는 경우<ol><li>batch size가 너무 작아 의미 없을 때</li><li>NLP의 경우 입력 크기의 다름, 미니배치 분포의 다양성 등등</li></ol></li></ul><p>이 외에도 여러 이유가 있는데, batch norm이 아닌, layer norm을 했을 때 conv net의 대해서도 오히려 학습이 잘되고 성능이 좋은 경우도 있다.<br>그래서 요새는 웬만하면 batch norm을 하지만, layer norm (group norm)을 사용하는 경우도 더러 있다고 한다.</p><p><strong>궁금증 : 어떤 상황에서 어떤 normalization을 적용하는 것이 좋은가??</strong></p><h2 id=classifier-guidance>Classifier Guidance<a hidden class=anchor aria-hidden=true href=#classifier-guidance>#</a></h2><ul><li>이 논문에서 가장 중요한 부분.</li></ul><p>기존의 GAN모델이 FID 점수가 잘 나왔던 이유가, diversity와 fidelity를 trade off로 교환하였기 때문에 퀄리티가 좋았다고 한다.<br>즉, Descriminator를 잘 속이기만 하면 되기 때문에 diversity가 낮고 대신에 퀄리티가 좋다.</p><p><strong>DDPM에서도 의도적으로 diversity를 낮춘다면 퀄리티가 올라가고 이를 trade off로서 FID 점수를 높일 수 있지 않을까?</strong></p><p>핵심은 classifier를 어떻게 정의할 것인가이다.(위에 normalizing할 때 class를 정의하긴 했지만 그것 말고 다른 방법을 제시하고 있다.)<br>해당 논문에서는 $p_\phi(y|x_t, t) = p_\phi(y|x_t), \epsilon_\theta(x_t, t) = \epsilon_\theta(x_t)$ 라는 notation을 사용하고 있음을 생각하고 이어지는 수식을 따라가보자.</p><p><em><em>목표 : $p</em>\phi(y|x_t)$를 어떻게 잘 정의할 것인가</em>_<br>(이 때, 해당 $p_\phi$는 generating 모델을 y로 유도하게 해주는 역할이다.)</p><p>원하는 방향은 다음과 같다.
$p_\phi$를 학습시키게 되면, noise image x_t를 sampling 할 때, gradient $\nabla_{x_t}\log{p_\phi(y|x_t, t)}$를 이용해서 임의의 label y로 유도한다.</p><h3 id=1-conditional-reverse-noising-process>1. Conditional Reverse Noising Process<a hidden class=anchor aria-hidden=true href=#1-conditional-reverse-noising-process>#</a></h3><p>기존 diffusion 모델의 unconditional reverse noising process 에서 시작한다.($p_\theta(x_t|x_{t+1})$)</p><p>임의의 컨디션 라벨 y 에 대해서 다음의 식이 성립한다.(확률의 성질에 의해)</p><p>$p_{\theta, \phi}(x_t|x_{t+1}, y) = Zp_{\theta}(x_t|x_{t+1})p_{\phi}(y|x_t)$(2)<br>(Z는 normalizing 상수)</p><p>그러면, 여기로부터 sampling algorithm을 유도할 수 있다.</p><p>기존의 diffusion model으로부터 sampling을<br>$p_\theta(x_t|x_{t+1}) = N(\mu, \Sigma)$<br>라고 간소화 하여 쓸 수 있고,
여기에 log likelihood를 이용해 표현하면 다음과 같다.<br>$\log{p_\theta(x_t|x_{t+1})} = -\cfrac{1}{2}(x_t - \mu)^T \Sigma^{-1} (x_t - \mu) + C$ (4)</p><p>이 때, $\log{p_\phi(y|x_t)}$가 $\Sigma^{-1}$에 비해 낮은 곡률을 갖는다고 가정한다.<br>즉, diffusion step이 무한히 발산하지 않을 때를 가정한다.($||\Sigma|| \rightarrow 0$일 때)<br>그러면, $x_t = \mu$일 때 taylor expansion을 이용해 $\log{p_\phi(y|x_t)}$를 근사할 수 있게된다!!!</p><p>$\log{p_\phi(y|x_t)}$<br>$\simeq \log{p_\phi(y|x_t)}|<em>{x_t = \mu} + (x_t - \mu)\nabla</em>{x_t}\log{p_\phi(y|x_t)|<em>{x_t = \mu}}$ (5)<br>$= (x_t - \mu)g + C_1$(6)<br>(이 때, $g = \nabla</em>{x_t}\log_{\phi}(y|x_t)|_{x_t = \mu}$)</p><p>이제, (2)의 식에 (4),(6)을 대입해서 전개하고 constant term을 제거해주면 다음의 식 (10)을 얻는다.<br>$$\log{(p_\theta(x_t|x_{t+1})p_{\phi}(y|x_t))}\simeq \log{p(z)} + C_4, z \sim N(\mu + \Sigma g, \Sigma)$$</p><p>이제 여기서 log를 양변에서 지워주고나면 남아있는 $C_4$가 처음에 normalization 상수 Z이자, 밑의 gradient scale s를 결정짓는 상수가 된다</p><p>이로부터 다음의 sampling 알고리즘이 유도된다!!<br></p><p>전체적으로 sampling되는 그림을 보면 다음과 같이 볼 수 있다.<br></p><h3 id=2-conditional-sampling-for-ddim>2. Conditional Sampling for DDIM<a hidden class=anchor aria-hidden=true href=#2-conditional-sampling-for-ddim>#</a></h3><p>위의 방식으로 근사하는 것은 확률적 모델인 DDPM에 적용되지만, 결정적 모델인 DDIM에는 사용할 수 없다. (위에서 보여지는 것처럼 z로부터 random성을 이용하기 때문..)</p><p>따라서, Song의 논문을 빌려, score-based conditioning trick을 이용해 접근한다.</p><p>$\nabla_{x_t}\log{q(x_t)} = -\cfrac{\epsilon_\theta(x_t)}{\sqrt{1-\bar{\alpha}_t}}$ (11)</p><p>위의 식은 deterministic sampling에 적용되는 model $\epsilon_\theta(x_t)$에 대해 위와 같은 함수가 유도된다.</p><p>$d\mathbf{x} = \bigg[\mathbf{f}(\mathbf{x}, t) - g(t)^2\bigtriangledown_x\log{p_t(\mathbf{x})}\bigg]dt + g(t)d\mathbf{w}$<br>를 이용한듯 싶음..</p><p>이제, 이를 앞의 절에서 한것 처럼 $p(x_t)p(y|x_t)$에다가 대입해보자.</p><p>$\nabla_{x_t}\log{(p_\theta(x_t)p_\phi(y|x_t))} = \nabla_{x_t}\log{p_\theta(x_t)} + \nabla_{x_t}\log{p_\phi(y|x_t)}$<br>$= -\cfrac{1}{\sqrt{1-\bar{\alpha}<em>t}}\epsilon</em>\theta(x_t) + \nabla_{x_t}\log{p_\phi(y|x_t)}$ (13)</p><p>위를 앞의 상수로 다시 묶어서 최종적으로 새로운 noise prediction을 만들어 볼 수 있다.
$\hat{\epsilon}(x_t) := \epsilon_\theta(x_t) - \sqrt{1-\bar{\alpha}<em>t}\nabla</em>{x_t}\log{p_{\phi}}(y|x_t)$ (14)</p><p>이로부터 아래의 Classifier guided DDIM sampling 알고리즘이 정의된다.<br></p><h3 id=3-scaling-classifier-gradients>3. Scaling Classifier Gradients<a hidden class=anchor aria-hidden=true href=#3-scaling-classifier-gradients>#</a></h3><p>이제, large scale의 generation task에 classifier를 적용하기 위해 먼저 classifier를 ImageNet에 학습을 시켰다고 한다. 이 때의 모델은 UNet 이고, 8x8 size의 layer를 downsampling 시켜 분류 문제를 해결하도록 변형하였다.</p><p>그리고, diffusion model에서 사용한 noise와 동일한 분포에 대해서 classifier를 학습시키고, 과적합을 방지하기 위해 randomcrop augmentation을 적용하였다고 한다.</p><p>이후, 학습이 종료되고나서 (10)의 식을 이용하여 diffusion모델과 classifier를 결합시켰다.</p><p>실험에서 classifier gradient가 (s 값)1이면, 50%정도의 sample 정확도를 가짐을 알 수 있다. 하지만, 실제로 sampling을 해보면 그에 미치지 못한다. 그래서 실험을 하다보니 1보다 큰 값을 잡아야 함을 알게 되었다.</p><p>$s \cdot \nabla_x \log{p(y|x)} = \nabla_x \log{\cfrac{1}{Z}p(y|x)^s}$의 관계로부터 이해할 수 있다.</p><p>s값이 커지면, 지수적으로 값이 폭발적으로 오르기 때문데, 더욱 점점 더 sharp 해지게 되고 다시말하면 classifier가 더욱 mode에 집중하게 됨을 의미한다.</p><p><strong>즉, 더 높은 fidelity를 얻고 diversity를 낮추는 것이 된다.</strong></p><pre><code>classifier gradient값을 높일 수록, FID 점수가 높게 나옴을 알 수 있다.(fidelity 높아짐)
또한, sFID는 전체 데이터에 대한 값인데, 갈수록 낮게나오는 모습을 통해 모델의 diversity가 낮아짐을 알 수 있다.
</code></pre><p>정말 모든 데이터 셋에 대해서 기존의 모델을 beat했다</p><p>또한 추가적으로, 이미지의 퀄리티도 GAN보다 좋은데 다양성마저 더 좋다.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://dobe0715.github.io/posts/ddpm/><span class=title>« Prev</span><br><span></span></a>
<a class=next href=https://dobe0715.github.io/posts/neural-ode/><span class=title>Next »</span><br><span></span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share  on twitter" href="https://twitter.com/intent/tweet/?text=&amp;url=https%3a%2f%2fdobe0715.github.io%2fposts%2fmore-diffusion%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share  on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdobe0715.github.io%2fposts%2fmore-diffusion%2f&amp;title=&amp;summary=&amp;source=https%3a%2f%2fdobe0715.github.io%2fposts%2fmore-diffusion%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share  on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdobe0715.github.io%2fposts%2fmore-diffusion%2f&title="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share  on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdobe0715.github.io%2fposts%2fmore-diffusion%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share  on whatsapp" href="https://api.whatsapp.com/send?text=%20-%20https%3a%2f%2fdobe0715.github.io%2fposts%2fmore-diffusion%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share  on telegram" href="https://telegram.me/share/url?text=&amp;url=https%3a%2f%2fdobe0715.github.io%2fposts%2fmore-diffusion%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share  on ycombinator" href="https://news.ycombinator.com/submitlink?t=&u=https%3a%2f%2fdobe0715.github.io%2fposts%2fmore-diffusion%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://dobe0715.github.io>블로그 홈</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>ë¸”ë¡œê·¸ í™ˆ</title><meta name=keywords content><meta name=description content="ì°¸ê³ ìë£Œ blogs
(lil log, what are Diffusion Models?) : https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ (Yang song, Generative Modeling by Estimating gradients of the data distribution) : https://yang-song.net/blog/2021/score/ í•œêµ­ ë¸”ë¡œê·¸ : https://deepseow.tistory.com/61 improved DDPM notion : https://sang-yun-lee.notion.site/Improved-Denoising-Diffusion-Probabilistic-Models-efa847335aef4163bfd3ee96c176f659 Diffusion models beats GANs : https://sang-yun-lee.notion.site/Diffusion-Models-Beat-GANs-on-Image-Synthesis-eb1f3826618d42e89d92e489c39f1371 papers
(Improved Denoising Diffusion Probablistic Models) : https://arxiv.org/pdf/2102.09672.pdf (Diffusion models beats GANs on Image synthesis) : https://arxiv.org/pdf/2105.05233.pdf youtubes
(improved DDPM) : https://www.youtube.com/watch?v=8dchQOqvrCE (Diffusion models beats GANs) : https://www.youtube.com/watch?v=bSqA2AIaHy8&amp;t=327s Improved DDPM (contribution)
reverseí•  ë•Œ variance termë„ ì–´ëŠì •ë„ í•™ìŠµ í•˜ê²Œ í•´ì„œ NLL(negative log-likelihood)ê°’ì„ ë‚®ì¶”ì—ˆë‹¤."><meta name=author content="Me"><link rel=canonical href=https://dobe0715.github.io/posts/more-diffusion/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content><meta property="og:description" content="ì°¸ê³ ìë£Œ blogs
(lil log, what are Diffusion Models?) : https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ (Yang song, Generative Modeling by Estimating gradients of the data distribution) : https://yang-song.net/blog/2021/score/ í•œêµ­ ë¸”ë¡œê·¸ : https://deepseow.tistory.com/61 improved DDPM notion : https://sang-yun-lee.notion.site/Improved-Denoising-Diffusion-Probabilistic-Models-efa847335aef4163bfd3ee96c176f659 Diffusion models beats GANs : https://sang-yun-lee.notion.site/Diffusion-Models-Beat-GANs-on-Image-Synthesis-eb1f3826618d42e89d92e489c39f1371 papers
(Improved Denoising Diffusion Probablistic Models) : https://arxiv.org/pdf/2102.09672.pdf (Diffusion models beats GANs on Image synthesis) : https://arxiv.org/pdf/2105.05233.pdf youtubes
(improved DDPM) : https://www.youtube.com/watch?v=8dchQOqvrCE (Diffusion models beats GANs) : https://www.youtube.com/watch?v=bSqA2AIaHy8&amp;t=327s Improved DDPM (contribution)
reverseí•  ë•Œ variance termë„ ì–´ëŠì •ë„ í•™ìŠµ í•˜ê²Œ í•´ì„œ NLL(negative log-likelihood)ê°’ì„ ë‚®ì¶”ì—ˆë‹¤."><meta property="og:type" content="article"><meta property="og:url" content="https://dobe0715.github.io/posts/more-diffusion/"><meta property="og:image" content="https://dobe0715.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dobe0715.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content><meta name=twitter:description content="ì°¸ê³ ìë£Œ blogs
(lil log, what are Diffusion Models?) : https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ (Yang song, Generative Modeling by Estimating gradients of the data distribution) : https://yang-song.net/blog/2021/score/ í•œêµ­ ë¸”ë¡œê·¸ : https://deepseow.tistory.com/61 improved DDPM notion : https://sang-yun-lee.notion.site/Improved-Denoising-Diffusion-Probabilistic-Models-efa847335aef4163bfd3ee96c176f659 Diffusion models beats GANs : https://sang-yun-lee.notion.site/Diffusion-Models-Beat-GANs-on-Image-Synthesis-eb1f3826618d42e89d92e489c39f1371 papers
(Improved Denoising Diffusion Probablistic Models) : https://arxiv.org/pdf/2102.09672.pdf (Diffusion models beats GANs on Image synthesis) : https://arxiv.org/pdf/2105.05233.pdf youtubes
(improved DDPM) : https://www.youtube.com/watch?v=8dchQOqvrCE (Diffusion models beats GANs) : https://www.youtube.com/watch?v=bSqA2AIaHy8&amp;t=327s Improved DDPM (contribution)
reverseí•  ë•Œ variance termë„ ì–´ëŠì •ë„ í•™ìŠµ í•˜ê²Œ í•´ì„œ NLL(negative log-likelihood)ê°’ì„ ë‚®ì¶”ì—ˆë‹¤."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://dobe0715.github.io/posts/"},{"@type":"ListItem","position":3,"name":"","item":"https://dobe0715.github.io/posts/more-diffusion/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"","name":"","description":"ì°¸ê³ ìë£Œ blogs\n(lil log, what are Diffusion Models?) : https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ (Yang song, Generative Modeling by Estimating gradients of the data distribution) : https://yang-song.net/blog/2021/score/ í•œêµ­ ë¸”ë¡œê·¸ : https://deepseow.tistory.com/61 improved DDPM notion : https://sang-yun-lee.notion.site/Improved-Denoising-Diffusion-Probabilistic-Models-efa847335aef4163bfd3ee96c176f659 Diffusion models beats GANs : https://sang-yun-lee.notion.site/Diffusion-Models-Beat-GANs-on-Image-Synthesis-eb1f3826618d42e89d92e489c39f1371 papers\n(Improved Denoising Diffusion Probablistic Models) : https://arxiv.org/pdf/2102.09672.pdf (Diffusion models beats GANs on Image synthesis) : https://arxiv.org/pdf/2105.05233.pdf youtubes\n(improved DDPM) : https://www.youtube.com/watch?v=8dchQOqvrCE (Diffusion models beats GANs) : https://www.youtube.com/watch?v=bSqA2AIaHy8\u0026amp;t=327s Improved DDPM (contribution)\nreverseí•  ë•Œ variance termë„ ì–´ëŠì •ë„ í•™ìŠµ í•˜ê²Œ í•´ì„œ NLL(negative log-likelihood)ê°’ì„ ë‚®ì¶”ì—ˆë‹¤.","keywords":[],"articleBody":"ì°¸ê³ ìë£Œ blogs\n(lil log, what are Diffusion Models?) : https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ (Yang song, Generative Modeling by Estimating gradients of the data distribution) : https://yang-song.net/blog/2021/score/ í•œêµ­ ë¸”ë¡œê·¸ : https://deepseow.tistory.com/61 improved DDPM notion : https://sang-yun-lee.notion.site/Improved-Denoising-Diffusion-Probabilistic-Models-efa847335aef4163bfd3ee96c176f659 Diffusion models beats GANs : https://sang-yun-lee.notion.site/Diffusion-Models-Beat-GANs-on-Image-Synthesis-eb1f3826618d42e89d92e489c39f1371 papers\n(Improved Denoising Diffusion Probablistic Models) : https://arxiv.org/pdf/2102.09672.pdf (Diffusion models beats GANs on Image synthesis) : https://arxiv.org/pdf/2105.05233.pdf youtubes\n(improved DDPM) : https://www.youtube.com/watch?v=8dchQOqvrCE (Diffusion models beats GANs) : https://www.youtube.com/watch?v=bSqA2AIaHy8\u0026t=327s Improved DDPM (contribution)\nreverseí•  ë•Œ variance termë„ ì–´ëŠì •ë„ í•™ìŠµ í•˜ê²Œ í•´ì„œ NLL(negative log-likelihood)ê°’ì„ ë‚®ì¶”ì—ˆë‹¤. varianceê°’ ìŠ¤ìºì¥´ë§ì„ ê¸°ì¡´ì˜ linearí•œ ê²ƒì—ì„œ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ë°”ê¾¸ì—ˆë‹¤. importance samplingê¸°ë²•ì„ í†µí•´ gradient noiseë¥¼ ì¤„ì˜€ë‹¤. subsequenceë¥¼ ì¡ì•„ sampling speedë¥¼ í–¥ìƒì‹œì¼°ë‹¤. (review) DDPM Data distribution $x_0 \\sim q(x_0)$ ê°€ ì£¼ì–´ì¡Œë‹¤ê³  í–ˆì„ ë•Œ, qì— ëŒ€í•´ forward processë¥¼ ê° t stepì— ëŒ€í•´ $\\beta_t$ë¥¼ í†µí•´ ìŠ¤ìºì¥´ë§í•œë‹¤. ì´ ë•Œ, ë§ˆë¥´ì½”í”„ ì—°ì‡„ë¥¼ í†µí•´ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.\n$q(x_t|x_{t-1}) = N(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t ğˆ)$\nì´ë•Œ, ì¶©ë¶„íˆ í° Të¥¼ í†µí•´ $x_T$ë¥¼ ë§Œë“¤ë©´, ê²°êµ­ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ë”°ë¥´ë„ë¡ ê°€ì •í•œë‹¤.\nì´ì œ, ì´ìƒì ì¸ reverse process(=sampling) $q(x_{t-1}|x_t)$ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ì—†ì–´ì„œ NN($p_{\\theta})$ì„ í†µí•´ ë‹¤ìŒê³¼ ê°™ì´ ê·¼ì‚¬í•œë‹¤.\n$p_\\theta(x_{t-1}|x_t) := N(x_{t-1};\\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$ ì´ë ‡ê²Œ ì˜ ì •ì˜í•œ $p_\\theta$ë¥¼ variational lower boundë¥¼ í†µí•´ lossë¥¼ êµ¬í•´ì£¼ë©´, ë‹¤ìŒê³¼ ê°™ë‹¤.\nì—¬ê¸°ì—ì„œ $L_0$ì™€ $L_T$ëŠ” DDPMì—ì„œ ê³ ë ¤í•˜ì§€ ì•Šì•˜ê³ , 1 ~ T-1 ì˜ lossë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ marginalì„ ì •ë¦¬í•˜ì˜€ë‹¤.\n$q(x_t|x_0) = N(x_t;\\sqrt{\\bar{\\alpha}_t}x_0, (1-\\bar{\\alpha}_t)I)$ $x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon$ ì´ì œ, x_tì™€ x_0ê°€ ì£¼ì–´ì¡Œì„ ë•Œ (reverseì—ì„œ)ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” posterior $q(x_{t-1}|x_t, x_0)$ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê°€ìš°ìŠ¤ ì»¤ë„ì„ í†µí•´ ê³„ì‚°í•œë‹¤. ê·¸ëŸ¬ë©´, ì›í•˜ëŠ” $\\tilde{\\beta}_t$, $\\tilde{\\mu}_t(x_t, x_0)$ê°’ì´ ìœ ë„ëœë‹¤.\nìœ„ì—ì„œ ìœ ë„í•œ $\\tilde{\\mu}$ì™€ $\\tilde{\\beta}$ì—ì„œ, $\\tilde{\\beta}$ëŠ” forwardì—ì„œ ì‚¬ìš©í•œ $\\beta$ë¥¼ ì‚¬ìš©í•˜ê³  $\\tilde{\\mu}$ê°’ë§Œ ì´ìš©í•´ ëŒ€ì…í•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ êµ¬í•˜ê³ ì í•˜ëŠ” í‰ê· ì— ëŒ€í•œ ê°’ì˜ íŒŒë¼ë¯¸í„°í™”ë¥¼ ìœ ë„í•œë‹¤.\n$\\mu_\\theta(x_t, t) = \\cfrac{1}{\\sqrt{\\alpha_t}}(x_t - \\cfrac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}t}}\\epsilon\\theta(x_t, t))$ ì´ê²ƒì„ ê°€ì§€ê³  ê° t stepì—ë‹¤ê°€ ëŒ€ì…í•´ì„œ ì•ì˜ ê³„ìˆ˜ í•­ë“¤ ê°„ë‹¨íˆ í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ ë‹¤ìŒì˜ ë‹¨ìˆœí™” ëœ loss termì„ ì–»ê²Œ ëœë‹¤.\n$L_{simple} = E_{t, x_0, \\epsilon}\\big[||\\epsilon - \\epsilon_\\theta(x_t, t)||^2\\big]$ ì•ìœ¼ë¡œ ì´ ë…¼ë¬¸ì—ì„œ ì£¼ëª©í•  ì ì€ varianceì´ë‹¤.\nImproving the log-likelihood Learning $\\Sigma_\\theta(x_t, t)$ ì´ì „ì˜ DDPMì—ì„œëŠ” varianceëŠ” í•™ìŠµí•˜ì§€ ì•Šë„ë¡ í•˜ì˜€ë‹¤. ì‹¬ì§€ì–´, ìœ ë„ëœ posteriorë¥¼ ì‚¬ìš©í•˜ì§€ë„ ì•Šê³  ì›ë˜ì˜ forwardì—ì„œì˜ varianceê°’ì„ ì‚¬ìš©í–ˆì—ˆë‹¤.\ni.e. $(posterior) : \\tilde{\\beta}t = \\cfrac{1-\\bar{\\alpha}{t-1}}{1-\\bar{\\alpha}_t}\\beta_t$ but, not use $\\tilde{\\beta}$. $\\sigma_t^2 = \\beta_t$ ì´ ë•Œ, $\\beta_t$ : upper bound, $\\tilde{\\beta_t}$ : lower boundë¼ê³  ì´ì•¼ê¸°í•˜ì˜€ë‹¤.\nê·¸ë ‡ë‹¤ë©´, ê¸°ì¡´ DDPMì—ì„œ ì € ë¶„ì‚°ê°’ ì‚¬ìš©ì— ìˆì–´ì„œ NLLê°’ ì†í•´ê°€ ìˆì§€ ì•Šì„ê¹Œ í•´ì„œ ê°’ ì‹¤í—˜ì„ í•´ë³´ì•˜ë‹¤.\ntime stepì˜ ì™„ì „ ì´ˆë°˜ë¶€ë¥¼ ì œì™¸í•˜ë©´, ë‘ ê°’ì´ ë¹„ìŠ·í•˜ë¯€ë¡œ tê°’ì´ ì»¤ì§€ê²Œ ë˜ë©´ ë¶„ì‚°ë¶€ë¶„ì€ sampling í€„ë¦¬í‹°ì— í¬ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ”ë‹¤ ì´ˆë°˜ì˜ loss termì˜ stepì„ ë³´ë³€ ì „ì²´ lossì— ê¸°ì—¬í•˜ëŠ” ì •ë„ê°€ ìƒë‹¹íˆ í¬ë‹¤. ë”°ë¼ì„œ, ê¸°ì¡´ì—ì²˜ëŸ¼ ê³ ì •ëœ varianceë¥¼ ì‚¬ìš©í•˜ê¸°ë³´ë‹¤, í•™ìŠµì‹œì¼œì„œ likelihoodë¥¼ í™• ë‚®ì¶œ ìˆ˜ ìˆì„ë“¯í•˜ë‹¤. ì´ ë•Œ, ìš°ë¦¬ê°€ í•™ìŠµí•˜ê³ ì í•˜ëŠ” $\\Sigma_\\theta$ëŠ” ì‘ì€ ê°’ì´ê³ , NNì„í†µí•´ ì§ì ‘ì ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ ì–´ë µê¸° ë•Œë¬¸ì— $\\beta$ì™€ $\\tilde{\\beta}$ë¥¼ interpolationí•˜ê³ , ê·¸ ì •ë„ë¥¼ í•™ìŠµí•˜ë„ë¡ í•˜ì˜€ë‹¤.\n$\\Sigma_\\theta(x_t, t) = \\exp(v\\log{\\beta_t} + (1 - v)\\log{\\tilde{\\beta}_t})$ sigma ê°’ì„ ì € ë‘˜ ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ê°€ì •í•˜ëŠ” ê·¼ê±°ëŠ”??\n(2015, Deep Unsupervised Learning using Nonequilibrium Thermodynamics) : diffusion ì‹œì´ˆ ë…¼ë¬¸\nconditional entropyì˜ ê´€ì ì—ì„œ ë³´ì•˜ì„ ë•Œ, ì´ëŸ¬í•œ ê´€ê³„ê°€ ì„±ë¦½í•˜ê³ , ê°€ìš°ì‹œì•ˆ ì»¤ë„ì„ ì´ìš©í•´ êµ¬í•œ ê°’ì— ëŒ€í•´ì„œ bounded ê°’ì„ ì˜ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.\nìµœì¢…ì ìœ¼ë¡œ, $L_{simple}$ì€ ë¶„ì‚°ê°’ì— ì˜í–¥ì„ ë°›ì§€ ì•Šìœ¼ë¯€ë¡œ ì¶”ê°€ loss termì„ ì¶”ê°€í•´ì¤€ë‹¤. ì¦‰, ê¸°ì¡´ì— í•™ìŠµì´ ì˜ ë˜ëŠ” ê²ƒì´ ì¦ëª…ì´ ëœ simpleí…€ì„ ë©”ì¸ìœ¼ë¡œ ë‘ê³ , vlbí…€ì´ ë¶„ì‚°ê°’ì— ë”°ë¼ guideí•´ì£¼ë„ë¡ í•™ìŠµí•œë‹¤.\n$L_{hybrid} = L_{simple} + \\lambda L_{vlb}$ $\\lambda$ëŠ” 0.001, vlb termì—ì„œ $\\mu_\\theta$ë¶€ë¶„ì€ stop gradientë¥¼ ì ìš©í•˜ì—¬ í‰ê· ì€ simpleì´ mainìœ¼ë¡œ í•™ìŠµí•˜ë„ë¡ í•˜ì˜€ë‹¤.\n200K 4K cosineë¶€ë¶„ì„ ì‚´í´ë³´ë©´, simpleê³¼ vlbëŠ” ì‚¬ìš©í•˜ë©´ FID, NLLì— ëŒ€í•´ trade offë¥¼ ê°€ì§ì„ ì•Œ ìˆ˜ ìˆë‹¤.\nImproving the Noise Schedule ê¸°ì¡´ì˜ linearí•œ scadulingì€ noiseê°€ ë„ˆë¬´ ì´ˆë°˜ë¶€í„° ë“¤ì–´ê°€ì„œ informationì„ ë¹¨ë¦¬ ë¶•ê´´ì‹œí‚¤ëŠ” ê²½í–¥ì´ ìˆë‹¤. ë”°ë¼ì„œ ì´ë¥¼ ì¢€ ì²œì²œíˆ ë“¤ì–´ê°€ê²Œë” cosineì„ í†µí•´ ë‹¤ì‹œ ì •ì˜í•´ì£¼ì—ˆë‹¤.\n$\\bar{\\alpha}_t = \\cfrac{f(t)}{f(0)}$, $f(t) = cos\\bigg(\\cfrac{t/T + s}{1 + s} \\cdot \\cfrac{\\pi}{2} \\bigg)^2$ ì´ë¥¼ í†µí•´, í•„ìš”í•˜ë‹¤ë©´ ì—­ìœ¼ë¡œ $\\beta_t$ë„ ì‰½ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.\nReducing Gradient Noise NLL ê´€ì ì—ì„œ, hybrid lossë³´ë‹¨, vlb lossë¥¼ ê·¸ëƒ¥ ì“°ëŠ” ê²ƒì´ ë‹¹ì—°íˆ ì¢‹ì§€ë§Œ ì‹¤ì œë¡œ ì´ë¥¼ optimizeí•˜ëŠ” ê²ƒì€ ìƒë‹¹íˆ ì–´ë ¤ì› ë‹¤ê³  í•œë‹¤.\nìš°ì„ , ê°€ì¥ í° ë¬¸ì œì ìœ¼ë¡œ ë³¸ ê²ƒì€ gradient noiseê°€ vlbì˜ ê²½ìš° ì»¸ë‹¤ëŠ” ê²ƒì´ë‹¤.(ê°€ì •)\ngradient noise ( != value noise) : optimization(ëŒ€í‘œì ìœ¼ë¡œ SGD)ì„ í•˜ëŠ” ì¤‘, ì „ì²´ ë°ì´í„° ì…‹ê³¼ ë¯¸ë‹ˆë°°ì¹˜ ì…‹ìœ¼ë¡œë¶€í„°ì˜ gradientì˜ ì˜¤ì°¨\nbatch sizeê°€ í´ ìˆ˜ë¡, ë‹¹ì—°íˆ ì „ì²´ ë°ì´í„°ì…‹ì˜ gradientëŒ€ë¡œ ì˜ ê°€ê³  ë¹ ë¥´ê²Œ ì›€ì§ì¸ë‹¤. í•˜ì§€ë§Œ, ë°˜ëŒ€ì˜ ê²½ìš° gradient noiseê°€ ì»¤ì§€ê²Œ ëœë‹¤. vlbì—ì„œì˜ varianceë¥¼ ë‚®ì¶”ëŠ” ê²ƒì— ì§‘ì¤‘í•˜ëŠ”ë° ê·¸ëŸ¬ê¸° ìœ„í•´ì„  ì´ìœ ë¥¼ ì•Œì•„ì•¼ í•œë‹¤. ì´ ë…¼ë¬¸ì—ì„  í›ˆë ¨í•  ë•Œ, vlbì˜ ê²½ìš° lossì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì •ë„ê°€ time stepë§ˆë‹¤ ë‹¤ë¥¸ë°(figure2), të¥¼ uniformlyí•˜ê²Œ samplingí•˜ê¸° ë•Œë¬¸ì´ë¼ê³  í•œë‹¤.\në”°ë¼ì„œ, ì¤‘ìš”ë„ì— ë”°ë¥¸ samplingì„ ì œì•ˆí•œë‹¤.\n$L_{vlb} = E_{t \\sim p_t}\\bigg[\\cfrac{L_t}{p_t} \\bigg]$, where $p_t \\propto \\sqrt{E[L_t^2]}, \\sum{p_t} = 1$ ì´ ë•Œ, $E[L_t^2]$ ê°’ì€ ë§¤ë²ˆ ê³„ì‚°í•˜ëŠ” ê°’ì´ê¸° ë•Œë¬¸ì—, í›ˆë ¨ë™ì•ˆ dynamicallyí•˜ê²Œ ê°±ì‹ í•´ì¤€ë‹¤(ì•ì˜ 10ê°œ ê°’ì„ ì´ìš©).\nê° lossì˜ ê°’ì„ ì¼ì •í•˜ê²Œ scalingí•´ì£¼ëŠ” ëŒ€ì‹ ì—, ì¤‘ìš”ë„ê°€ ë†’ì€ lossëŠ” ìì£¼ samplingë˜ì–´ ê°±ì‹ ì‹œì¼œì¤€ë‹¤. Improving Sampling Speed ë³¸ ë…¼ë¬¸ì€ ëª¨ë¸ì„ 4000ë²ˆì˜ diffusion stepì„ í›ˆë ¨í•˜ë„ë¡ í•˜ì˜€ë‹¤. ì´ ë•Œ, samplingì„ í•˜ë©´ì„œ ê±¸ë¦¬ëŠ” ì‹œê°„ê³¼ GPUê°€ êµ‰ì¥íˆ í¬ë‹¤. í•˜ì§€ë§Œ, ì´ ë…¼ë¬¸ì˜ ëª¨ë¸ì€ sub sequenceë¥¼ í†µí•´ sampling í•˜ì—¬ë„ ì´ë¯¸ì§€ì˜ í€„ë¦¬í‹°ë¥¼ ìœ ì§€í•  ìˆ˜ ìˆë‹¤.\n$\\beta_{S_t} = 1 - \\cfrac{\\bar{\\alpha}{S_t}}{\\bar{\\alpha}{S_{t-1}}}$, $\\tilde\\beta_{S_t} = \\cfrac{1 - \\bar{\\alpha}{S_t}}{1 - \\bar{\\alpha}{S_{t-1}}}\\beta_{S_t}$ ì´ì™€ ê°™ì´ ì¡ê²Œë˜ë©´, ì˜ ìƒ˜í”Œë§ ë˜ëŠ”ë° ì™œ ì˜ ë˜ëŠ”ì§€ ìƒê°í•´ë³´ë©´, ê¸°ì¡´ì˜ DDPMì€ reverseí•  ë•Œ $\\beta_t$ê°’ ë§Œì„ ì‚¬ìš©í–ˆ ê¸° ë•Œë¬¸ì—, time stepì„ ê±´ë„ˆë›°ë©´ ê·¸ ì¤‘ê°„ ì •ë³´ê°€ ì†Œì‹¤ëœë‹¤. (DDIMì—ì„œì˜ ì‹¤í—˜ ìƒê°í•´ë³´ë©´ ê·¸ ì°¨ì´ê°€ ìƒë‹¹íˆ í¼ì„ ì•Œ ìˆ˜ ìˆë‹¤. ì¦‰, $\\beta$ ë¥¼ ì‚¬ìš©í•œê²ƒê³¼ $\\tilde\\beta$ë¥¼ ì‚¬ìš©í–ˆì„ ë•Œì˜ ì°¨ì´ê°€ ë“œëŸ¬ë‚œë‹¤.)\ní•˜ì§€ë§Œ, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” $\\tilde\\beta_t$ë¥¼ ì‚¬ìš©í•˜ì˜€ê³ (ì •í™•í•˜ê²ŒëŠ” interpolationí•œ ê°’ì¸ $\\Sigma_\\theta(x_t, t)$), ê·¸ë ‡ê¸° ë•Œë¬¸ì— NLL, FIDì—ì„œì˜ score ì„ ë°©ì„ í•  ìˆ˜ ìˆì—ˆë‹¤.\nDiffusion Models Beat GANs on Image Synthesis Contributions ëª¨ë¸ ì•„í‚¤í…ì³ tuning attention head ì¦ê°€ ë‹¤ì–‘í•œ resolution ì¸µì— attention ì‚¬ìš©(ê¸°ì¡´ì—ëŠ” 16x16 -\u003e 8x8, 16x16, 32x32) Big GANì˜ residual blockì„ upsampling, downsamplingì— ì‚¬ìš© AdaGN ì ìš© Classifier Guidence ì‚¬ìš©í•˜ì—¬ FID score ìƒìŠ¹ (ê¸°ì¡´ì˜ GAN ëª¨ë¸ì„ ì´ê¹€!) Back ground Improved DDPM $\\Sigma_\\theta(x_t, t) = \\exp(v\\log{\\beta_t} + (1 - v)\\log{\\tilde{\\beta}_t})$\nSample Quality Metrics Inception Score ImageNetì—ì„œ í•œê°€ì§€ classì˜ distributionì— ëŒ€í•´ ê³ ì •ì‹œì¼œ í•™ìŠµì‹œí‚¤ê³  ê·¸ê²ƒì— ëŒ€í•´ __Sharpness, Diversity__ë¥¼ ê³„ì‚°í•œë‹¤.\nëª¨ë¸ì´ small subsetì— ëŒ€í•´ ì í•©í•´ì ¸ë„ ì ìˆ˜ê°€ ì˜ë‚˜ì˜¤ëŠ” ë¬¸ì œì ì´ ì¡´ì¬. $S = \\exp{(E_{x \\sim p}\\big[\\int{c(y|x)\\log{c(y|x)}}dy\\big])}$\n$D = \\exp{(E_{x \\sim p}\\big[\\int{c(y|x)\\log{c(y)}}dy\\big])}$\nS : Sharpness\nclassifierê°€ í™•ì‹ ì„ ê°€ì§€ê³  predictí•˜ëŠ”ê°€ Sê°€ ì¦ê°€í•˜ë©´, c(y|x)ì˜ ì—”íŠ¸ë¡œí”¼ê°€ ê°ì†Œí•œë‹¤. -\u003e ë°ì´í„°ë“¤ì´ ì˜ ë¶„ë¦¬ë˜ì–´ìˆë‹¤ D : Diversity\nì–¼ë§ˆë‚˜ ë‹¤ì–‘í•˜ê²Œ ìƒì„±í•˜ëŠ”ê°€ Dê°€ ì¦ê°€í•˜ë©´, marginal distributionì¸ c(y)ì˜ ì—”íŠ¸ë¡œí”¼ê°€ ì¦ê°€í•œë‹¤. -\u003e ë°ì´í„°ë“¤ì´ ë‹¤ì–‘í•˜ë‹¤ $IS = S \\cdot D$\nì´ ë•Œ, classifierì¸ $c(\\cdot)$ì€ ImageNetì„ í•™ìŠµí•œ Inception V3 ëª¨ë¸ì„ ì‚¬ìš©. ê·¸ë˜ì„œ Inception scoreë¼ëŠ” ì´ë¦„ì´ ë¶™ì–´ì¡Œë‹¤.\nFrechet Inception Score (FID score) í•™ìŠµì‹œí‚¨ ganerated modelë¡œë¶€í„° ë°ì´í„°ë“¤ì„ ëª¨ì•„ë‘ê³ , testí•  ë°ì´í„°ë¥¼ ëª¨ì•„ì„œ ê°ê° ê°™ì€ ëª¨ë¸ì„ ì´ìš©í•´ feature extractë¥¼ ì§„í–‰í•œë‹¤. ê·¸ë¦¬ê³ , ê°ê°ì˜ featureì˜ ë¶„í¬ì— ëŒ€í•´ ê±°ë¦¬ë¥¼ ì¸¡ì •í•œë‹¤. (by wasserstein-2 distance) data spaceë¡œë¶€í„° ë¶„í¬ê°„ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ë©´ ë„ˆë¬´ í©ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì— ì´ì™€ ê°™ì´ í•˜ë‚˜ì˜ encoder netì„ ì´ìš©í•´ ëª¨ì•„ì¤€ë‹¤.(manifold hypothesis) Inception scoreë³´ë‹¤ ë” Human Judgementì— ê°€ê¹ë‹¤ëŠ” ì¥ì  $FID = ||\\mu_T - \\mu_G||^2_2 + Tr(\\Sigma_T + \\Sigma_G - 2\\sqrt{\\Sigma_T\\Sigma_G})$\nì™¼ìª½ë¶€í„° ê°ê° Fidelity, Diversityì— í•´ë‹¹í•œë‹¤. ì¦‰, ì ìˆ˜ê°€ ë‚®ìœ¼ë©´ ë‚®ì„ ìˆ˜ë¡ í€„ë¦¬í‹°ê°€ ì¢‹ê³  ë‹¤ì–‘ì„±ì´ í’ë¶€í•œ ë°ì´í„°ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. (ì •í™•íˆëŠ” ë”ìš± testing sample spaceì™€ ìœ ì‚¬í•œ ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ë‚¸ë‹¤.) Architecture Improvement ì—¬ëŸ¬ê°€ì§€ tuning ê¸°ë²•ì„ ì´ìš©í•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ëŒì–´ì˜¬ë ¸ë‹¤ ì •ë„.. Adaptive Group Normalization (AdaGN) $AdaGN(h, y) = y_sGroupNorm(h) + y_b$\nGroup Normalizationì„ í•˜ëŠ” ê²½ìš° batch sizeê°€ ë„ˆë¬´ ì‘ì•„ ì˜ë¯¸ ì—†ì„ ë•Œ NLPì˜ ê²½ìš° ì…ë ¥ í¬ê¸°ì˜ ë‹¤ë¦„, ë¯¸ë‹ˆë°°ì¹˜ ë¶„í¬ì˜ ë‹¤ì–‘ì„± ë“±ë“± ì´ ì™¸ì—ë„ ì—¬ëŸ¬ ì´ìœ ê°€ ìˆëŠ”ë°, batch normì´ ì•„ë‹Œ, layer normì„ í–ˆì„ ë•Œ conv netì˜ ëŒ€í•´ì„œë„ ì˜¤íˆë ¤ í•™ìŠµì´ ì˜ë˜ê³  ì„±ëŠ¥ì´ ì¢‹ì€ ê²½ìš°ë„ ìˆë‹¤.\nê·¸ë˜ì„œ ìš”ìƒˆëŠ” ì›¬ë§Œí•˜ë©´ batch normì„ í•˜ì§€ë§Œ, layer norm (group norm)ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ë„ ë”ëŸ¬ ìˆë‹¤ê³  í•œë‹¤.\nê¶ê¸ˆì¦ : ì–´ë–¤ ìƒí™©ì—ì„œ ì–´ë–¤ normalizationì„ ì ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ì€ê°€??\nClassifier Guidance ì´ ë…¼ë¬¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„. ê¸°ì¡´ì˜ GANëª¨ë¸ì´ FID ì ìˆ˜ê°€ ì˜ ë‚˜ì™”ë˜ ì´ìœ ê°€, diversityì™€ fidelityë¥¼ trade offë¡œ êµí™˜í•˜ì˜€ê¸° ë•Œë¬¸ì— í€„ë¦¬í‹°ê°€ ì¢‹ì•˜ë‹¤ê³  í•œë‹¤.\nì¦‰, Descriminatorë¥¼ ì˜ ì†ì´ê¸°ë§Œ í•˜ë©´ ë˜ê¸° ë•Œë¬¸ì— diversityê°€ ë‚®ê³  ëŒ€ì‹ ì— í€„ë¦¬í‹°ê°€ ì¢‹ë‹¤.\nDDPMì—ì„œë„ ì˜ë„ì ìœ¼ë¡œ diversityë¥¼ ë‚®ì¶˜ë‹¤ë©´ í€„ë¦¬í‹°ê°€ ì˜¬ë¼ê°€ê³  ì´ë¥¼ trade offë¡œì„œ FID ì ìˆ˜ë¥¼ ë†’ì¼ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?\ní•µì‹¬ì€ classifierë¥¼ ì–´ë–»ê²Œ ì •ì˜í•  ê²ƒì¸ê°€ì´ë‹¤.(ìœ„ì— normalizingí•  ë•Œ classë¥¼ ì •ì˜í•˜ê¸´ í–ˆì§€ë§Œ ê·¸ê²ƒ ë§ê³  ë‹¤ë¥¸ ë°©ë²•ì„ ì œì‹œí•˜ê³  ìˆë‹¤.)\ní•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” $p_\\phi(y|x_t, t) = p_\\phi(y|x_t), \\epsilon_\\theta(x_t, t) = \\epsilon_\\theta(x_t)$ ë¼ëŠ” notationì„ ì‚¬ìš©í•˜ê³  ìˆìŒì„ ìƒê°í•˜ê³  ì´ì–´ì§€ëŠ” ìˆ˜ì‹ì„ ë”°ë¼ê°€ë³´ì.\nëª©í‘œ : $p\\phi(y|x_t)$ë¥¼ ì–´ë–»ê²Œ ì˜ ì •ì˜í•  ê²ƒì¸ê°€_\n(ì´ ë•Œ, í•´ë‹¹ $p_\\phi$ëŠ” generating ëª¨ë¸ì„ yë¡œ ìœ ë„í•˜ê²Œ í•´ì£¼ëŠ” ì—­í• ì´ë‹¤.)\nì›í•˜ëŠ” ë°©í–¥ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. $p_\\phi$ë¥¼ í•™ìŠµì‹œí‚¤ê²Œ ë˜ë©´, noise image x_të¥¼ sampling í•  ë•Œ, gradient $\\nabla_{x_t}\\log{p_\\phi(y|x_t, t)}$ë¥¼ ì´ìš©í•´ì„œ ì„ì˜ì˜ label yë¡œ ìœ ë„í•œë‹¤.\n1. Conditional Reverse Noising Process ê¸°ì¡´ diffusion ëª¨ë¸ì˜ unconditional reverse noising process ì—ì„œ ì‹œì‘í•œë‹¤.($p_\\theta(x_t|x_{t+1})$)\nì„ì˜ì˜ ì»¨ë””ì…˜ ë¼ë²¨ y ì— ëŒ€í•´ì„œ ë‹¤ìŒì˜ ì‹ì´ ì„±ë¦½í•œë‹¤.(í™•ë¥ ì˜ ì„±ì§ˆì— ì˜í•´)\n$p_{\\theta, \\phi}(x_t|x_{t+1}, y) = Zp_{\\theta}(x_t|x_{t+1})p_{\\phi}(y|x_t)$(2)\n(ZëŠ” normalizing ìƒìˆ˜)\nê·¸ëŸ¬ë©´, ì—¬ê¸°ë¡œë¶€í„° sampling algorithmì„ ìœ ë„í•  ìˆ˜ ìˆë‹¤.\nê¸°ì¡´ì˜ diffusion modelìœ¼ë¡œë¶€í„° samplingì„ $p_\\theta(x_t|x_{t+1}) = N(\\mu, \\Sigma)$\në¼ê³  ê°„ì†Œí™” í•˜ì—¬ ì“¸ ìˆ˜ ìˆê³ , ì—¬ê¸°ì— log likelihoodë¥¼ ì´ìš©í•´ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n$\\log{p_\\theta(x_t|x_{t+1})} = -\\cfrac{1}{2}(x_t - \\mu)^T \\Sigma^{-1} (x_t - \\mu) + C$ (4)\nì´ ë•Œ, $\\log{p_\\phi(y|x_t)}$ê°€ $\\Sigma^{-1}$ì— ë¹„í•´ ë‚®ì€ ê³¡ë¥ ì„ ê°–ëŠ”ë‹¤ê³  ê°€ì •í•œë‹¤.\nì¦‰, diffusion stepì´ ë¬´í•œíˆ ë°œì‚°í•˜ì§€ ì•Šì„ ë•Œë¥¼ ê°€ì •í•œë‹¤.($||\\Sigma|| \\rightarrow 0$ì¼ ë•Œ)\nê·¸ëŸ¬ë©´, $x_t = \\mu$ì¼ ë•Œ taylor expansionì„ ì´ìš©í•´ $\\log{p_\\phi(y|x_t)}$ë¥¼ ê·¼ì‚¬í•  ìˆ˜ ìˆê²Œëœë‹¤!!!\n$\\log{p_\\phi(y|x_t)}$\n$\\simeq \\log{p_\\phi(y|x_t)}|{x_t = \\mu} + (x_t - \\mu)\\nabla{x_t}\\log{p_\\phi(y|x_t)|{x_t = \\mu}}$ (5)\n$= (x_t - \\mu)g + C_1$(6)\n(ì´ ë•Œ, $g = \\nabla{x_t}\\log_{\\phi}(y|x_t)|_{x_t = \\mu}$)\nì´ì œ, (2)ì˜ ì‹ì— (4),(6)ì„ ëŒ€ì…í•´ì„œ ì „ê°œí•˜ê³  constant termì„ ì œê±°í•´ì£¼ë©´ ë‹¤ìŒì˜ ì‹ (10)ì„ ì–»ëŠ”ë‹¤.\n$$\\log{(p_\\theta(x_t|x_{t+1})p_{\\phi}(y|x_t))}\\simeq \\log{p(z)} + C_4, z \\sim N(\\mu + \\Sigma g, \\Sigma)$$\nì´ì œ ì—¬ê¸°ì„œ logë¥¼ ì–‘ë³€ì—ì„œ ì§€ì›Œì£¼ê³ ë‚˜ë©´ ë‚¨ì•„ìˆëŠ” $C_4$ê°€ ì²˜ìŒì— normalization ìƒìˆ˜ Zì´ì, ë°‘ì˜ gradient scale së¥¼ ê²°ì •ì§“ëŠ” ìƒìˆ˜ê°€ ëœë‹¤\nì´ë¡œë¶€í„° ë‹¤ìŒì˜ sampling ì•Œê³ ë¦¬ì¦˜ì´ ìœ ë„ëœë‹¤!!\nì „ì²´ì ìœ¼ë¡œ samplingë˜ëŠ” ê·¸ë¦¼ì„ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë³¼ ìˆ˜ ìˆë‹¤.\n2. Conditional Sampling for DDIM ìœ„ì˜ ë°©ì‹ìœ¼ë¡œ ê·¼ì‚¬í•˜ëŠ” ê²ƒì€ í™•ë¥ ì  ëª¨ë¸ì¸ DDPMì— ì ìš©ë˜ì§€ë§Œ, ê²°ì •ì  ëª¨ë¸ì¸ DDIMì—ëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤. (ìœ„ì—ì„œ ë³´ì—¬ì§€ëŠ” ê²ƒì²˜ëŸ¼ zë¡œë¶€í„° randomì„±ì„ ì´ìš©í•˜ê¸° ë•Œë¬¸..)\në”°ë¼ì„œ, Songì˜ ë…¼ë¬¸ì„ ë¹Œë ¤, score-based conditioning trickì„ ì´ìš©í•´ ì ‘ê·¼í•œë‹¤.\n$\\nabla_{x_t}\\log{q(x_t)} = -\\cfrac{\\epsilon_\\theta(x_t)}{\\sqrt{1-\\bar{\\alpha}_t}}$ (11)\nìœ„ì˜ ì‹ì€ deterministic samplingì— ì ìš©ë˜ëŠ” model $\\epsilon_\\theta(x_t)$ì— ëŒ€í•´ ìœ„ì™€ ê°™ì€ í•¨ìˆ˜ê°€ ìœ ë„ëœë‹¤.\n$d\\mathbf{x} = \\bigg[\\mathbf{f}(\\mathbf{x}, t) - g(t)^2\\bigtriangledown_x\\log{p_t(\\mathbf{x})}\\bigg]dt + g(t)d\\mathbf{w}$\në¥¼ ì´ìš©í•œë“¯ ì‹¶ìŒ..\nì´ì œ, ì´ë¥¼ ì•ì˜ ì ˆì—ì„œ í•œê²ƒ ì²˜ëŸ¼ $p(x_t)p(y|x_t)$ì—ë‹¤ê°€ ëŒ€ì…í•´ë³´ì.\n$\\nabla_{x_t}\\log{(p_\\theta(x_t)p_\\phi(y|x_t))} = \\nabla_{x_t}\\log{p_\\theta(x_t)} + \\nabla_{x_t}\\log{p_\\phi(y|x_t)}$\n$= -\\cfrac{1}{\\sqrt{1-\\bar{\\alpha}t}}\\epsilon\\theta(x_t) + \\nabla_{x_t}\\log{p_\\phi(y|x_t)}$ (13)\nìœ„ë¥¼ ì•ì˜ ìƒìˆ˜ë¡œ ë‹¤ì‹œ ë¬¶ì–´ì„œ ìµœì¢…ì ìœ¼ë¡œ ìƒˆë¡œìš´ noise predictionì„ ë§Œë“¤ì–´ ë³¼ ìˆ˜ ìˆë‹¤. $\\hat{\\epsilon}(x_t) := \\epsilon_\\theta(x_t) - \\sqrt{1-\\bar{\\alpha}t}\\nabla{x_t}\\log{p_{\\phi}}(y|x_t)$ (14)\nì´ë¡œë¶€í„° ì•„ë˜ì˜ Classifier guided DDIM sampling ì•Œê³ ë¦¬ì¦˜ì´ ì •ì˜ëœë‹¤.\n3. Scaling Classifier Gradients ì´ì œ, large scaleì˜ generation taskì— classifierë¥¼ ì ìš©í•˜ê¸° ìœ„í•´ ë¨¼ì € classifierë¥¼ ImageNetì— í•™ìŠµì„ ì‹œì¼°ë‹¤ê³  í•œë‹¤. ì´ ë•Œì˜ ëª¨ë¸ì€ UNet ì´ê³ , 8x8 sizeì˜ layerë¥¼ downsampling ì‹œì¼œ ë¶„ë¥˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ë„ë¡ ë³€í˜•í•˜ì˜€ë‹¤.\nê·¸ë¦¬ê³ , diffusion modelì—ì„œ ì‚¬ìš©í•œ noiseì™€ ë™ì¼í•œ ë¶„í¬ì— ëŒ€í•´ì„œ classifierë¥¼ í•™ìŠµì‹œí‚¤ê³ , ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ randomcrop augmentationì„ ì ìš©í•˜ì˜€ë‹¤ê³  í•œë‹¤.\nì´í›„, í•™ìŠµì´ ì¢…ë£Œë˜ê³ ë‚˜ì„œ (10)ì˜ ì‹ì„ ì´ìš©í•˜ì—¬ diffusionëª¨ë¸ê³¼ classifierë¥¼ ê²°í•©ì‹œì¼°ë‹¤.\nì‹¤í—˜ì—ì„œ classifier gradientê°€ (s ê°’)1ì´ë©´, 50%ì •ë„ì˜ sample ì •í™•ë„ë¥¼ ê°€ì§ì„ ì•Œ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ, ì‹¤ì œë¡œ samplingì„ í•´ë³´ë©´ ê·¸ì— ë¯¸ì¹˜ì§€ ëª»í•œë‹¤. ê·¸ë˜ì„œ ì‹¤í—˜ì„ í•˜ë‹¤ë³´ë‹ˆ 1ë³´ë‹¤ í° ê°’ì„ ì¡ì•„ì•¼ í•¨ì„ ì•Œê²Œ ë˜ì—ˆë‹¤.\n$s \\cdot \\nabla_x \\log{p(y|x)} = \\nabla_x \\log{\\cfrac{1}{Z}p(y|x)^s}$ì˜ ê´€ê³„ë¡œë¶€í„° ì´í•´í•  ìˆ˜ ìˆë‹¤.\nsê°’ì´ ì»¤ì§€ë©´, ì§€ìˆ˜ì ìœ¼ë¡œ ê°’ì´ í­ë°œì ìœ¼ë¡œ ì˜¤ë¥´ê¸° ë•Œë¬¸ë°, ë”ìš± ì ì  ë” sharp í•´ì§€ê²Œ ë˜ê³  ë‹¤ì‹œë§í•˜ë©´ classifierê°€ ë”ìš± modeì— ì§‘ì¤‘í•˜ê²Œ ë¨ì„ ì˜ë¯¸í•œë‹¤.\nì¦‰, ë” ë†’ì€ fidelityë¥¼ ì–»ê³  diversityë¥¼ ë‚®ì¶”ëŠ” ê²ƒì´ ëœë‹¤.\nclassifier gradientê°’ì„ ë†’ì¼ ìˆ˜ë¡, FID ì ìˆ˜ê°€ ë†’ê²Œ ë‚˜ì˜´ì„ ì•Œ ìˆ˜ ìˆë‹¤.(fidelity ë†’ì•„ì§) ë˜í•œ, sFIDëŠ” ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ê°’ì¸ë°, ê°ˆìˆ˜ë¡ ë‚®ê²Œë‚˜ì˜¤ëŠ” ëª¨ìŠµì„ í†µí•´ ëª¨ë¸ì˜ diversityê°€ ë‚®ì•„ì§ì„ ì•Œ ìˆ˜ ìˆë‹¤. ì •ë§ ëª¨ë“  ë°ì´í„° ì…‹ì— ëŒ€í•´ì„œ ê¸°ì¡´ì˜ ëª¨ë¸ì„ beatí–ˆë‹¤\në˜í•œ ì¶”ê°€ì ìœ¼ë¡œ, ì´ë¯¸ì§€ì˜ í€„ë¦¬í‹°ë„ GANë³´ë‹¤ ì¢‹ì€ë° ë‹¤ì–‘ì„±ë§ˆì € ë” ì¢‹ë‹¤.\n","wordCount":"1726","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://dobe0715.github.io/posts/more-diffusion/"},"publisher":{"@type":"Organization","name":"ë¸”ë¡œê·¸ í™ˆ","logo":{"@type":"ImageObject","url":"https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dobe0715.github.io accesskey=h title="Home (Alt + H)"><img src=https://dobe0715.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dobe0715.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://dobe0715.github.io/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dobe0715.github.io>Home</a>&nbsp;Â»&nbsp;<a href=https://dobe0715.github.io/posts/>Posts</a></div><h1 class=post-title></h1><div class=post-meta>9 min&nbsp;Â·&nbsp;1726 words&nbsp;Â·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/more%20diffusion.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><h2 id=ì°¸ê³ ìë£Œ>ì°¸ê³ ìë£Œ<a hidden class=anchor aria-hidden=true href=#ì°¸ê³ ìë£Œ>#</a></h2><ul><li><p>blogs</p><ul><li>(lil log, what are Diffusion Models?) : <a href=https://lilianweng.github.io/posts/2021-07-11-diffusion-models/>https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</a></li><li>(Yang song, Generative Modeling by Estimating gradients of the data distribution) : <a href=https://yang-song.net/blog/2021/score/>https://yang-song.net/blog/2021/score/</a></li><li>í•œêµ­ ë¸”ë¡œê·¸ : <a href=https://deepseow.tistory.com/61>https://deepseow.tistory.com/61</a></li><li>improved DDPM notion : <a href=https://sang-yun-lee.notion.site/Improved-Denoising-Diffusion-Probabilistic-Models-efa847335aef4163bfd3ee96c176f659>https://sang-yun-lee.notion.site/Improved-Denoising-Diffusion-Probabilistic-Models-efa847335aef4163bfd3ee96c176f659</a></li><li>Diffusion models beats GANs : <a href=https://sang-yun-lee.notion.site/Diffusion-Models-Beat-GANs-on-Image-Synthesis-eb1f3826618d42e89d92e489c39f1371>https://sang-yun-lee.notion.site/Diffusion-Models-Beat-GANs-on-Image-Synthesis-eb1f3826618d42e89d92e489c39f1371</a></li></ul></li><li><p>papers</p><ul><li>(Improved Denoising Diffusion Probablistic Models) : <a href=https://arxiv.org/pdf/2102.09672.pdf>https://arxiv.org/pdf/2102.09672.pdf</a></li><li>(Diffusion models beats GANs on Image synthesis) : <a href=https://arxiv.org/pdf/2105.05233.pdf>https://arxiv.org/pdf/2105.05233.pdf</a></li></ul></li><li><p>youtubes</p><ul><li>(improved DDPM) : <a href="https://www.youtube.com/watch?v=8dchQOqvrCE">https://www.youtube.com/watch?v=8dchQOqvrCE</a></li><li>(Diffusion models beats GANs) : <a href="https://www.youtube.com/watch?v=bSqA2AIaHy8&amp;t=327s">https://www.youtube.com/watch?v=bSqA2AIaHy8&amp;t=327s</a></li></ul></li></ul><h1 id=improved-ddpm>Improved DDPM<a hidden class=anchor aria-hidden=true href=#improved-ddpm>#</a></h1><p>(contribution)</p><ol><li>reverseí•  ë•Œ variance termë„ ì–´ëŠì •ë„ í•™ìŠµ í•˜ê²Œ í•´ì„œ NLL(negative log-likelihood)ê°’ì„ ë‚®ì¶”ì—ˆë‹¤.</li><li>varianceê°’ ìŠ¤ìºì¥´ë§ì„ ê¸°ì¡´ì˜ linearí•œ ê²ƒì—ì„œ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ë°”ê¾¸ì—ˆë‹¤.</li><li>importance samplingê¸°ë²•ì„ í†µí•´ gradient noiseë¥¼ ì¤„ì˜€ë‹¤.</li><li>subsequenceë¥¼ ì¡ì•„ sampling speedë¥¼ í–¥ìƒì‹œì¼°ë‹¤.</li></ol><h3 id=review-ddpm>(review) DDPM<a hidden class=anchor aria-hidden=true href=#review-ddpm>#</a></h3><p>Data distribution $x_0 \sim q(x_0)$ ê°€ ì£¼ì–´ì¡Œë‹¤ê³  í–ˆì„ ë•Œ, qì— ëŒ€í•´ forward processë¥¼ ê° t stepì— ëŒ€í•´ $\beta_t$ë¥¼ í†µí•´ ìŠ¤ìºì¥´ë§í•œë‹¤. ì´ ë•Œ, ë§ˆë¥´ì½”í”„ ì—°ì‡„ë¥¼ í†µí•´ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.</p><ul><li><p>$q(x_t|x_{t-1}) = N(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t ğˆ)$</p></li><li><p>ì´ë•Œ, ì¶©ë¶„íˆ í° Të¥¼ í†µí•´ $x_T$ë¥¼ ë§Œë“¤ë©´, ê²°êµ­ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ë”°ë¥´ë„ë¡ ê°€ì •í•œë‹¤.</p></li></ul><p>ì´ì œ, ì´ìƒì ì¸ reverse process(=sampling) $q(x_{t-1}|x_t)$ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ì—†ì–´ì„œ NN($p_{\theta})$ì„ í†µí•´ ë‹¤ìŒê³¼ ê°™ì´ ê·¼ì‚¬í•œë‹¤.</p><ul><li>$p_\theta(x_{t-1}|x_t) := N(x_{t-1};\mu_\theta(x_t, t), \Sigma_\theta(x_t, t))$</li></ul><p>ì´ë ‡ê²Œ ì˜ ì •ì˜í•œ $p_\theta$ë¥¼ variational lower boundë¥¼ í†µí•´ lossë¥¼ êµ¬í•´ì£¼ë©´, ë‹¤ìŒê³¼ ê°™ë‹¤.<br></p><p>ì—¬ê¸°ì—ì„œ $L_0$ì™€ $L_T$ëŠ” DDPMì—ì„œ ê³ ë ¤í•˜ì§€ ì•Šì•˜ê³ , 1 ~ T-1 ì˜ lossë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ marginalì„ ì •ë¦¬í•˜ì˜€ë‹¤.</p><ul><li>$q(x_t|x_0) = N(x_t;\sqrt{\bar{\alpha}_t}x_0, (1-\bar{\alpha}_t)I)$</li><li>$x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon$</li></ul><p>ì´ì œ, x_tì™€ x_0ê°€ ì£¼ì–´ì¡Œì„ ë•Œ (reverseì—ì„œ)ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” posterior $q(x_{t-1}|x_t, x_0)$ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê°€ìš°ìŠ¤ ì»¤ë„ì„ í†µí•´ ê³„ì‚°í•œë‹¤. ê·¸ëŸ¬ë©´, ì›í•˜ëŠ” $\tilde{\beta}_t$, $\tilde{\mu}_t(x_t, x_0)$ê°’ì´ ìœ ë„ëœë‹¤.</p><p>ìœ„ì—ì„œ ìœ ë„í•œ $\tilde{\mu}$ì™€ $\tilde{\beta}$ì—ì„œ, $\tilde{\beta}$ëŠ” forwardì—ì„œ ì‚¬ìš©í•œ $\beta$ë¥¼ ì‚¬ìš©í•˜ê³  $\tilde{\mu}$ê°’ë§Œ ì´ìš©í•´ ëŒ€ì…í•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ êµ¬í•˜ê³ ì í•˜ëŠ” í‰ê· ì— ëŒ€í•œ ê°’ì˜ íŒŒë¼ë¯¸í„°í™”ë¥¼ ìœ ë„í•œë‹¤.</p><ul><li>$\mu_\theta(x_t, t) = \cfrac{1}{\sqrt{\alpha_t}}(x_t - \cfrac{\beta_t}{\sqrt{1-\bar{\alpha}<em>t}}\epsilon</em>\theta(x_t, t))$</li></ul><p>ì´ê²ƒì„ ê°€ì§€ê³  ê° t stepì—ë‹¤ê°€ ëŒ€ì…í•´ì„œ ì•ì˜ ê³„ìˆ˜ í•­ë“¤ ê°„ë‹¨íˆ í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ ë‹¤ìŒì˜ ë‹¨ìˆœí™” ëœ loss termì„ ì–»ê²Œ ëœë‹¤.</p><ul><li>$L_{simple} = E_{t, x_0, \epsilon}\big[||\epsilon - \epsilon_\theta(x_t, t)||^2\big]$</li></ul><p>ì•ìœ¼ë¡œ ì´ ë…¼ë¬¸ì—ì„œ ì£¼ëª©í•  ì ì€ varianceì´ë‹¤.</p><h2 id=improving-the-log-likelihood>Improving the log-likelihood<a hidden class=anchor aria-hidden=true href=#improving-the-log-likelihood>#</a></h2><h3 id=learning-sigma_thetax_t-t>Learning $\Sigma_\theta(x_t, t)$<a hidden class=anchor aria-hidden=true href=#learning-sigma_thetax_t-t>#</a></h3><p>ì´ì „ì˜ DDPMì—ì„œëŠ” varianceëŠ” í•™ìŠµí•˜ì§€ ì•Šë„ë¡ í•˜ì˜€ë‹¤. ì‹¬ì§€ì–´, ìœ ë„ëœ posteriorë¥¼ ì‚¬ìš©í•˜ì§€ë„ ì•Šê³  ì›ë˜ì˜ forwardì—ì„œì˜ varianceê°’ì„ ì‚¬ìš©í–ˆì—ˆë‹¤.</p><ul><li>i.e. $(posterior) : \tilde{\beta}<em>t = \cfrac{1-\bar{\alpha}</em>{t-1}}{1-\bar{\alpha}_t}\beta_t$</li><li>but, not use $\tilde{\beta}$.</li><li>$\sigma_t^2 = \beta_t$</li></ul><p>ì´ ë•Œ, $\beta_t$ : upper bound, $\tilde{\beta_t}$ : lower boundë¼ê³  ì´ì•¼ê¸°í•˜ì˜€ë‹¤.</p><p>ê·¸ë ‡ë‹¤ë©´, ê¸°ì¡´ DDPMì—ì„œ ì € ë¶„ì‚°ê°’ ì‚¬ìš©ì— ìˆì–´ì„œ NLLê°’ ì†í•´ê°€ ìˆì§€ ì•Šì„ê¹Œ í•´ì„œ ê°’ ì‹¤í—˜ì„ í•´ë³´ì•˜ë‹¤.</p><p></p><ol><li>time stepì˜ ì™„ì „ ì´ˆë°˜ë¶€ë¥¼ ì œì™¸í•˜ë©´, ë‘ ê°’ì´ ë¹„ìŠ·í•˜ë¯€ë¡œ tê°’ì´ ì»¤ì§€ê²Œ ë˜ë©´ ë¶„ì‚°ë¶€ë¶„ì€ sampling í€„ë¦¬í‹°ì— í¬ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ”ë‹¤</li><li>ì´ˆë°˜ì˜ loss termì˜ stepì„ ë³´ë³€ ì „ì²´ lossì— ê¸°ì—¬í•˜ëŠ” ì •ë„ê°€ ìƒë‹¹íˆ í¬ë‹¤. ë”°ë¼ì„œ, ê¸°ì¡´ì—ì²˜ëŸ¼ ê³ ì •ëœ varianceë¥¼ ì‚¬ìš©í•˜ê¸°ë³´ë‹¤, í•™ìŠµì‹œì¼œì„œ likelihoodë¥¼ í™• ë‚®ì¶œ ìˆ˜ ìˆì„ë“¯í•˜ë‹¤.</li></ol><p>ì´ ë•Œ, ìš°ë¦¬ê°€ í•™ìŠµí•˜ê³ ì í•˜ëŠ” $\Sigma_\theta$ëŠ” ì‘ì€ ê°’ì´ê³ , NNì„í†µí•´ ì§ì ‘ì ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ ì–´ë µê¸° ë•Œë¬¸ì— $\beta$ì™€ $\tilde{\beta}$ë¥¼ interpolationí•˜ê³ , ê·¸ ì •ë„ë¥¼ í•™ìŠµí•˜ë„ë¡ í•˜ì˜€ë‹¤.</p><ul><li>$\Sigma_\theta(x_t, t) = \exp(v\log{\beta_t} + (1 - v)\log{\tilde{\beta}_t})$</li></ul><p><strong>sigma ê°’ì„ ì € ë‘˜ ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ê°€ì •í•˜ëŠ” ê·¼ê±°ëŠ”??</strong></p><p>(2015, Deep Unsupervised Learning using
Nonequilibrium Thermodynamics) : <strong>diffusion ì‹œì´ˆ ë…¼ë¬¸</strong><br><br>conditional entropyì˜ ê´€ì ì—ì„œ ë³´ì•˜ì„ ë•Œ, ì´ëŸ¬í•œ ê´€ê³„ê°€ ì„±ë¦½í•˜ê³ , ê°€ìš°ì‹œì•ˆ ì»¤ë„ì„ ì´ìš©í•´ êµ¬í•œ ê°’ì— ëŒ€í•´ì„œ bounded ê°’ì„ ì˜ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.</p><p>ìµœì¢…ì ìœ¼ë¡œ, $L_{simple}$ì€ ë¶„ì‚°ê°’ì— ì˜í–¥ì„ ë°›ì§€ ì•Šìœ¼ë¯€ë¡œ ì¶”ê°€ loss termì„ ì¶”ê°€í•´ì¤€ë‹¤. ì¦‰, ê¸°ì¡´ì— í•™ìŠµì´ ì˜ ë˜ëŠ” ê²ƒì´ ì¦ëª…ì´ ëœ simpleí…€ì„ ë©”ì¸ìœ¼ë¡œ ë‘ê³ , vlbí…€ì´ ë¶„ì‚°ê°’ì— ë”°ë¼ guideí•´ì£¼ë„ë¡ í•™ìŠµí•œë‹¤.</p><ul><li>$L_{hybrid} = L_{simple} + \lambda L_{vlb}$</li></ul><p>$\lambda$ëŠ” 0.001, vlb termì—ì„œ $\mu_\theta$ë¶€ë¶„ì€ stop gradientë¥¼ ì ìš©í•˜ì—¬ í‰ê· ì€ simpleì´ mainìœ¼ë¡œ í•™ìŠµí•˜ë„ë¡ í•˜ì˜€ë‹¤.</p><p>200K 4K cosineë¶€ë¶„ì„ ì‚´í´ë³´ë©´, simpleê³¼ vlbëŠ” ì‚¬ìš©í•˜ë©´ FID, NLLì— ëŒ€í•´ trade offë¥¼ ê°€ì§ì„ ì•Œ ìˆ˜ ìˆë‹¤.</p><h3 id=improving-the-noise-schedule>Improving the Noise Schedule<a hidden class=anchor aria-hidden=true href=#improving-the-noise-schedule>#</a></h3><p>ê¸°ì¡´ì˜ linearí•œ scadulingì€ noiseê°€ ë„ˆë¬´ ì´ˆë°˜ë¶€í„° ë“¤ì–´ê°€ì„œ informationì„ ë¹¨ë¦¬ ë¶•ê´´ì‹œí‚¤ëŠ” ê²½í–¥ì´ ìˆë‹¤. ë”°ë¼ì„œ ì´ë¥¼ ì¢€ ì²œì²œíˆ ë“¤ì–´ê°€ê²Œë” cosineì„ í†µí•´ ë‹¤ì‹œ ì •ì˜í•´ì£¼ì—ˆë‹¤.</p><ul><li>$\bar{\alpha}_t = \cfrac{f(t)}{f(0)}$, $f(t) = cos\bigg(\cfrac{t/T + s}{1 + s} \cdot \cfrac{\pi}{2} \bigg)^2$</li></ul><p>ì´ë¥¼ í†µí•´, í•„ìš”í•˜ë‹¤ë©´ ì—­ìœ¼ë¡œ $\beta_t$ë„ ì‰½ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.</p><h3 id=reducing-gradient-noise>Reducing Gradient Noise<a hidden class=anchor aria-hidden=true href=#reducing-gradient-noise>#</a></h3><p>NLL ê´€ì ì—ì„œ, hybrid lossë³´ë‹¨, vlb lossë¥¼ ê·¸ëƒ¥ ì“°ëŠ” ê²ƒì´ ë‹¹ì—°íˆ ì¢‹ì§€ë§Œ ì‹¤ì œë¡œ ì´ë¥¼ optimizeí•˜ëŠ” ê²ƒì€ ìƒë‹¹íˆ ì–´ë ¤ì› ë‹¤ê³  í•œë‹¤.</p><p></p><p>ìš°ì„ , ê°€ì¥ í° ë¬¸ì œì ìœ¼ë¡œ ë³¸ ê²ƒì€ gradient noiseê°€ vlbì˜ ê²½ìš° ì»¸ë‹¤ëŠ” ê²ƒì´ë‹¤.(ê°€ì •)</p><ul><li>gradient noise ( != value noise) : optimization(ëŒ€í‘œì ìœ¼ë¡œ SGD)ì„ í•˜ëŠ” ì¤‘, ì „ì²´ ë°ì´í„° ì…‹ê³¼ ë¯¸ë‹ˆë°°ì¹˜ ì…‹ìœ¼ë¡œë¶€í„°ì˜ gradientì˜ ì˜¤ì°¨<br></li><li>batch sizeê°€ í´ ìˆ˜ë¡, ë‹¹ì—°íˆ ì „ì²´ ë°ì´í„°ì…‹ì˜ gradientëŒ€ë¡œ ì˜ ê°€ê³  ë¹ ë¥´ê²Œ ì›€ì§ì¸ë‹¤. í•˜ì§€ë§Œ, ë°˜ëŒ€ì˜ ê²½ìš° gradient noiseê°€ ì»¤ì§€ê²Œ ëœë‹¤.</li></ul><p>vlbì—ì„œì˜ varianceë¥¼ ë‚®ì¶”ëŠ” ê²ƒì— ì§‘ì¤‘í•˜ëŠ”ë° ê·¸ëŸ¬ê¸° ìœ„í•´ì„  ì´ìœ ë¥¼ ì•Œì•„ì•¼ í•œë‹¤. ì´ ë…¼ë¬¸ì—ì„  í›ˆë ¨í•  ë•Œ, vlbì˜ ê²½ìš° lossì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì •ë„ê°€ time stepë§ˆë‹¤ ë‹¤ë¥¸ë°(figure2), të¥¼ uniformlyí•˜ê²Œ samplingí•˜ê¸° ë•Œë¬¸ì´ë¼ê³  í•œë‹¤.<br>ë”°ë¼ì„œ, ì¤‘ìš”ë„ì— ë”°ë¥¸ samplingì„ ì œì•ˆí•œë‹¤.</p><ul><li>$L_{vlb} = E_{t \sim p_t}\bigg[\cfrac{L_t}{p_t} \bigg]$, where $p_t \propto \sqrt{E[L_t^2]}, \sum{p_t} = 1$</li></ul><p>ì´ ë•Œ, $E[L_t^2]$ ê°’ì€ ë§¤ë²ˆ ê³„ì‚°í•˜ëŠ” ê°’ì´ê¸° ë•Œë¬¸ì—, í›ˆë ¨ë™ì•ˆ dynamicallyí•˜ê²Œ ê°±ì‹ í•´ì¤€ë‹¤(ì•ì˜ 10ê°œ ê°’ì„ ì´ìš©).</p><ul><li>ê° lossì˜ ê°’ì„ ì¼ì •í•˜ê²Œ scalingí•´ì£¼ëŠ” ëŒ€ì‹ ì—, ì¤‘ìš”ë„ê°€ ë†’ì€ lossëŠ” ìì£¼ samplingë˜ì–´ ê°±ì‹ ì‹œì¼œì¤€ë‹¤.</li></ul><h3 id=improving-sampling-speed>Improving Sampling Speed<a hidden class=anchor aria-hidden=true href=#improving-sampling-speed>#</a></h3><p>ë³¸ ë…¼ë¬¸ì€ ëª¨ë¸ì„ 4000ë²ˆì˜ diffusion stepì„ í›ˆë ¨í•˜ë„ë¡ í•˜ì˜€ë‹¤. ì´ ë•Œ, samplingì„ í•˜ë©´ì„œ ê±¸ë¦¬ëŠ” ì‹œê°„ê³¼ GPUê°€ êµ‰ì¥íˆ í¬ë‹¤. í•˜ì§€ë§Œ, ì´ ë…¼ë¬¸ì˜ ëª¨ë¸ì€ sub sequenceë¥¼ í†µí•´ sampling í•˜ì—¬ë„ ì´ë¯¸ì§€ì˜ í€„ë¦¬í‹°ë¥¼ ìœ ì§€í•  ìˆ˜ ìˆë‹¤.</p><ul><li>$\beta_{S_t} = 1 - \cfrac{\bar{\alpha}<em>{S_t}}{\bar{\alpha}</em>{S_{t-1}}}$, $\tilde\beta_{S_t} = \cfrac{1 - \bar{\alpha}<em>{S_t}}{1 - \bar{\alpha}</em>{S_{t-1}}}\beta_{S_t}$
ì´ì™€ ê°™ì´ ì¡ê²Œë˜ë©´, ì˜ ìƒ˜í”Œë§ ë˜ëŠ”ë° ì™œ ì˜ ë˜ëŠ”ì§€ ìƒê°í•´ë³´ë©´,</li></ul><p>ê¸°ì¡´ì˜ DDPMì€ reverseí•  ë•Œ $\beta_t$ê°’ ë§Œì„ ì‚¬ìš©í–ˆ ê¸° ë•Œë¬¸ì—, time stepì„ ê±´ë„ˆë›°ë©´ ê·¸ ì¤‘ê°„ ì •ë³´ê°€ ì†Œì‹¤ëœë‹¤. (DDIMì—ì„œì˜ ì‹¤í—˜ ìƒê°í•´ë³´ë©´ ê·¸ ì°¨ì´ê°€ ìƒë‹¹íˆ í¼ì„ ì•Œ ìˆ˜ ìˆë‹¤. ì¦‰, $\beta$ ë¥¼ ì‚¬ìš©í•œê²ƒê³¼ $\tilde\beta$ë¥¼ ì‚¬ìš©í–ˆì„ ë•Œì˜ ì°¨ì´ê°€ ë“œëŸ¬ë‚œë‹¤.)<br>í•˜ì§€ë§Œ, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” $\tilde\beta_t$ë¥¼ ì‚¬ìš©í•˜ì˜€ê³ (ì •í™•í•˜ê²ŒëŠ” interpolationí•œ ê°’ì¸ $\Sigma_\theta(x_t, t)$), ê·¸ë ‡ê¸° ë•Œë¬¸ì— NLL, FIDì—ì„œì˜ score ì„ ë°©ì„ í•  ìˆ˜ ìˆì—ˆë‹¤.</p><h1 id=diffusion-models-beat-gans--on-image-synthesis>Diffusion Models Beat GANs on Image Synthesis<a hidden class=anchor aria-hidden=true href=#diffusion-models-beat-gans--on-image-synthesis>#</a></h1><h2 id=contributions>Contributions<a hidden class=anchor aria-hidden=true href=#contributions>#</a></h2><ol><li>ëª¨ë¸ ì•„í‚¤í…ì³ tuning<ul><li>attention head ì¦ê°€</li><li>ë‹¤ì–‘í•œ resolution ì¸µì— attention ì‚¬ìš©(ê¸°ì¡´ì—ëŠ” 16x16 -> 8x8, 16x16, 32x32)</li><li>Big GANì˜ residual blockì„ upsampling, downsamplingì— ì‚¬ìš©</li><li>AdaGN ì ìš©</li></ul></li><li>Classifier Guidence ì‚¬ìš©í•˜ì—¬ FID score ìƒìŠ¹ (ê¸°ì¡´ì˜ GAN ëª¨ë¸ì„ ì´ê¹€!)</li></ol><h2 id=back-ground>Back ground<a hidden class=anchor aria-hidden=true href=#back-ground>#</a></h2><h3 id=improved-ddpm-1>Improved DDPM<a hidden class=anchor aria-hidden=true href=#improved-ddpm-1>#</a></h3><p>$\Sigma_\theta(x_t, t) = \exp(v\log{\beta_t} + (1 - v)\log{\tilde{\beta}_t})$</p><h3 id=sample-quality-metrics>Sample Quality Metrics<a hidden class=anchor aria-hidden=true href=#sample-quality-metrics>#</a></h3><h4 id=inception-score>Inception Score<a hidden class=anchor aria-hidden=true href=#inception-score>#</a></h4><ul><li><p>ImageNetì—ì„œ í•œê°€ì§€ classì˜ distributionì— ëŒ€í•´ ê³ ì •ì‹œì¼œ í•™ìŠµì‹œí‚¤ê³  ê·¸ê²ƒì— ëŒ€í•´ __Sharpness, Diversity__ë¥¼ ê³„ì‚°í•œë‹¤.</p><ul><li><strong>ëª¨ë¸ì´ small subsetì— ëŒ€í•´ ì í•©í•´ì ¸ë„ ì ìˆ˜ê°€ ì˜ë‚˜ì˜¤ëŠ” ë¬¸ì œì ì´ ì¡´ì¬.</strong></li></ul></li></ul><p>$S = \exp{(E_{x \sim p}\big[\int{c(y|x)\log{c(y|x)}}dy\big])}$<br>$D = \exp{(E_{x \sim p}\big[\int{c(y|x)\log{c(y)}}dy\big])}$</p><p>S : Sharpness</p><ul><li>classifierê°€ í™•ì‹ ì„ ê°€ì§€ê³  predictí•˜ëŠ”ê°€</li><li>Sê°€ ì¦ê°€í•˜ë©´, c(y|x)ì˜ ì—”íŠ¸ë¡œí”¼ê°€ ê°ì†Œí•œë‹¤. -> ë°ì´í„°ë“¤ì´ ì˜ ë¶„ë¦¬ë˜ì–´ìˆë‹¤</li></ul><p>D : Diversity</p><ul><li>ì–¼ë§ˆë‚˜ ë‹¤ì–‘í•˜ê²Œ ìƒì„±í•˜ëŠ”ê°€</li><li>Dê°€ ì¦ê°€í•˜ë©´, marginal distributionì¸ c(y)ì˜ ì—”íŠ¸ë¡œí”¼ê°€ ì¦ê°€í•œë‹¤. -> ë°ì´í„°ë“¤ì´ ë‹¤ì–‘í•˜ë‹¤</li></ul><p>$IS = S \cdot D$</p><p>ì´ ë•Œ, classifierì¸ $c(\cdot)$ì€ ImageNetì„ í•™ìŠµí•œ Inception V3 ëª¨ë¸ì„ ì‚¬ìš©. ê·¸ë˜ì„œ Inception scoreë¼ëŠ” ì´ë¦„ì´ ë¶™ì–´ì¡Œë‹¤.</p><h4 id=frechet-inception-score-fid-score>Frechet Inception Score (FID score)<a hidden class=anchor aria-hidden=true href=#frechet-inception-score-fid-score>#</a></h4><ul><li>í•™ìŠµì‹œí‚¨ ganerated modelë¡œë¶€í„° ë°ì´í„°ë“¤ì„ ëª¨ì•„ë‘ê³ , testí•  ë°ì´í„°ë¥¼ ëª¨ì•„ì„œ ê°ê° ê°™ì€ ëª¨ë¸ì„ ì´ìš©í•´ feature extractë¥¼ ì§„í–‰í•œë‹¤. ê·¸ë¦¬ê³ , ê°ê°ì˜ featureì˜ ë¶„í¬ì— ëŒ€í•´ ê±°ë¦¬ë¥¼ ì¸¡ì •í•œë‹¤. (by wasserstein-2 distance)<ul><li>data spaceë¡œë¶€í„° ë¶„í¬ê°„ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ë©´ ë„ˆë¬´ í©ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì— ì´ì™€ ê°™ì´ í•˜ë‚˜ì˜ encoder netì„ ì´ìš©í•´ ëª¨ì•„ì¤€ë‹¤.(manifold hypothesis)</li><li><strong>Inception scoreë³´ë‹¤ ë” Human Judgementì— ê°€ê¹ë‹¤ëŠ” ì¥ì </strong></li></ul></li></ul><p>$FID = ||\mu_T - \mu_G||^2_2 + Tr(\Sigma_T + \Sigma_G - 2\sqrt{\Sigma_T\Sigma_G})$</p><ul><li>ì™¼ìª½ë¶€í„° ê°ê° Fidelity, Diversityì— í•´ë‹¹í•œë‹¤.</li><li>ì¦‰, ì ìˆ˜ê°€ ë‚®ìœ¼ë©´ ë‚®ì„ ìˆ˜ë¡ í€„ë¦¬í‹°ê°€ ì¢‹ê³  ë‹¤ì–‘ì„±ì´ í’ë¶€í•œ ë°ì´í„°ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. (ì •í™•íˆëŠ” ë”ìš± testing sample spaceì™€ ìœ ì‚¬í•œ ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ë‚¸ë‹¤.)</li></ul><h2 id=architecture-improvement>Architecture Improvement<a hidden class=anchor aria-hidden=true href=#architecture-improvement>#</a></h2><p></p><ul><li>ì—¬ëŸ¬ê°€ì§€ tuning ê¸°ë²•ì„ ì´ìš©í•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ëŒì–´ì˜¬ë ¸ë‹¤ ì •ë„..</li></ul><h3 id=adaptive-group-normalization-adagn>Adaptive Group Normalization (AdaGN)<a hidden class=anchor aria-hidden=true href=#adaptive-group-normalization-adagn>#</a></h3><p>$AdaGN(h, y) = y_sGroupNorm(h) + y_b$</p><ul><li>Group Normalizationì„ í•˜ëŠ” ê²½ìš°<ol><li>batch sizeê°€ ë„ˆë¬´ ì‘ì•„ ì˜ë¯¸ ì—†ì„ ë•Œ</li><li>NLPì˜ ê²½ìš° ì…ë ¥ í¬ê¸°ì˜ ë‹¤ë¦„, ë¯¸ë‹ˆë°°ì¹˜ ë¶„í¬ì˜ ë‹¤ì–‘ì„± ë“±ë“±</li></ol></li></ul><p>ì´ ì™¸ì—ë„ ì—¬ëŸ¬ ì´ìœ ê°€ ìˆëŠ”ë°, batch normì´ ì•„ë‹Œ, layer normì„ í–ˆì„ ë•Œ conv netì˜ ëŒ€í•´ì„œë„ ì˜¤íˆë ¤ í•™ìŠµì´ ì˜ë˜ê³  ì„±ëŠ¥ì´ ì¢‹ì€ ê²½ìš°ë„ ìˆë‹¤.<br>ê·¸ë˜ì„œ ìš”ìƒˆëŠ” ì›¬ë§Œí•˜ë©´ batch normì„ í•˜ì§€ë§Œ, layer norm (group norm)ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ë„ ë”ëŸ¬ ìˆë‹¤ê³  í•œë‹¤.</p><p><strong>ê¶ê¸ˆì¦ : ì–´ë–¤ ìƒí™©ì—ì„œ ì–´ë–¤ normalizationì„ ì ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ì€ê°€??</strong></p><h2 id=classifier-guidance>Classifier Guidance<a hidden class=anchor aria-hidden=true href=#classifier-guidance>#</a></h2><ul><li>ì´ ë…¼ë¬¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„.</li></ul><p>ê¸°ì¡´ì˜ GANëª¨ë¸ì´ FID ì ìˆ˜ê°€ ì˜ ë‚˜ì™”ë˜ ì´ìœ ê°€, diversityì™€ fidelityë¥¼ trade offë¡œ êµí™˜í•˜ì˜€ê¸° ë•Œë¬¸ì— í€„ë¦¬í‹°ê°€ ì¢‹ì•˜ë‹¤ê³  í•œë‹¤.<br>ì¦‰, Descriminatorë¥¼ ì˜ ì†ì´ê¸°ë§Œ í•˜ë©´ ë˜ê¸° ë•Œë¬¸ì— diversityê°€ ë‚®ê³  ëŒ€ì‹ ì— í€„ë¦¬í‹°ê°€ ì¢‹ë‹¤.</p><p><strong>DDPMì—ì„œë„ ì˜ë„ì ìœ¼ë¡œ diversityë¥¼ ë‚®ì¶˜ë‹¤ë©´ í€„ë¦¬í‹°ê°€ ì˜¬ë¼ê°€ê³  ì´ë¥¼ trade offë¡œì„œ FID ì ìˆ˜ë¥¼ ë†’ì¼ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?</strong></p><p>í•µì‹¬ì€ classifierë¥¼ ì–´ë–»ê²Œ ì •ì˜í•  ê²ƒì¸ê°€ì´ë‹¤.(ìœ„ì— normalizingí•  ë•Œ classë¥¼ ì •ì˜í•˜ê¸´ í–ˆì§€ë§Œ ê·¸ê²ƒ ë§ê³  ë‹¤ë¥¸ ë°©ë²•ì„ ì œì‹œí•˜ê³  ìˆë‹¤.)<br>í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” $p_\phi(y|x_t, t) = p_\phi(y|x_t), \epsilon_\theta(x_t, t) = \epsilon_\theta(x_t)$ ë¼ëŠ” notationì„ ì‚¬ìš©í•˜ê³  ìˆìŒì„ ìƒê°í•˜ê³  ì´ì–´ì§€ëŠ” ìˆ˜ì‹ì„ ë”°ë¼ê°€ë³´ì.</p><p><em><em>ëª©í‘œ : $p</em>\phi(y|x_t)$ë¥¼ ì–´ë–»ê²Œ ì˜ ì •ì˜í•  ê²ƒì¸ê°€</em>_<br>(ì´ ë•Œ, í•´ë‹¹ $p_\phi$ëŠ” generating ëª¨ë¸ì„ yë¡œ ìœ ë„í•˜ê²Œ í•´ì£¼ëŠ” ì—­í• ì´ë‹¤.)</p><p>ì›í•˜ëŠ” ë°©í–¥ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
$p_\phi$ë¥¼ í•™ìŠµì‹œí‚¤ê²Œ ë˜ë©´, noise image x_të¥¼ sampling í•  ë•Œ, gradient $\nabla_{x_t}\log{p_\phi(y|x_t, t)}$ë¥¼ ì´ìš©í•´ì„œ ì„ì˜ì˜ label yë¡œ ìœ ë„í•œë‹¤.</p><h3 id=1-conditional-reverse-noising-process>1. Conditional Reverse Noising Process<a hidden class=anchor aria-hidden=true href=#1-conditional-reverse-noising-process>#</a></h3><p>ê¸°ì¡´ diffusion ëª¨ë¸ì˜ unconditional reverse noising process ì—ì„œ ì‹œì‘í•œë‹¤.($p_\theta(x_t|x_{t+1})$)</p><p>ì„ì˜ì˜ ì»¨ë””ì…˜ ë¼ë²¨ y ì— ëŒ€í•´ì„œ ë‹¤ìŒì˜ ì‹ì´ ì„±ë¦½í•œë‹¤.(í™•ë¥ ì˜ ì„±ì§ˆì— ì˜í•´)</p><p>$p_{\theta, \phi}(x_t|x_{t+1}, y) = Zp_{\theta}(x_t|x_{t+1})p_{\phi}(y|x_t)$(2)<br>(ZëŠ” normalizing ìƒìˆ˜)</p><p>ê·¸ëŸ¬ë©´, ì—¬ê¸°ë¡œë¶€í„° sampling algorithmì„ ìœ ë„í•  ìˆ˜ ìˆë‹¤.</p><p>ê¸°ì¡´ì˜ diffusion modelìœ¼ë¡œë¶€í„° samplingì„<br>$p_\theta(x_t|x_{t+1}) = N(\mu, \Sigma)$<br>ë¼ê³  ê°„ì†Œí™” í•˜ì—¬ ì“¸ ìˆ˜ ìˆê³ ,
ì—¬ê¸°ì— log likelihoodë¥¼ ì´ìš©í•´ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.<br>$\log{p_\theta(x_t|x_{t+1})} = -\cfrac{1}{2}(x_t - \mu)^T \Sigma^{-1} (x_t - \mu) + C$ (4)</p><p>ì´ ë•Œ, $\log{p_\phi(y|x_t)}$ê°€ $\Sigma^{-1}$ì— ë¹„í•´ ë‚®ì€ ê³¡ë¥ ì„ ê°–ëŠ”ë‹¤ê³  ê°€ì •í•œë‹¤.<br>ì¦‰, diffusion stepì´ ë¬´í•œíˆ ë°œì‚°í•˜ì§€ ì•Šì„ ë•Œë¥¼ ê°€ì •í•œë‹¤.($||\Sigma|| \rightarrow 0$ì¼ ë•Œ)<br>ê·¸ëŸ¬ë©´, $x_t = \mu$ì¼ ë•Œ taylor expansionì„ ì´ìš©í•´ $\log{p_\phi(y|x_t)}$ë¥¼ ê·¼ì‚¬í•  ìˆ˜ ìˆê²Œëœë‹¤!!!</p><p>$\log{p_\phi(y|x_t)}$<br>$\simeq \log{p_\phi(y|x_t)}|<em>{x_t = \mu} + (x_t - \mu)\nabla</em>{x_t}\log{p_\phi(y|x_t)|<em>{x_t = \mu}}$ (5)<br>$= (x_t - \mu)g + C_1$(6)<br>(ì´ ë•Œ, $g = \nabla</em>{x_t}\log_{\phi}(y|x_t)|_{x_t = \mu}$)</p><p>ì´ì œ, (2)ì˜ ì‹ì— (4),(6)ì„ ëŒ€ì…í•´ì„œ ì „ê°œí•˜ê³  constant termì„ ì œê±°í•´ì£¼ë©´ ë‹¤ìŒì˜ ì‹ (10)ì„ ì–»ëŠ”ë‹¤.<br>$$\log{(p_\theta(x_t|x_{t+1})p_{\phi}(y|x_t))}\simeq \log{p(z)} + C_4, z \sim N(\mu + \Sigma g, \Sigma)$$</p><p>ì´ì œ ì—¬ê¸°ì„œ logë¥¼ ì–‘ë³€ì—ì„œ ì§€ì›Œì£¼ê³ ë‚˜ë©´ ë‚¨ì•„ìˆëŠ” $C_4$ê°€ ì²˜ìŒì— normalization ìƒìˆ˜ Zì´ì, ë°‘ì˜ gradient scale së¥¼ ê²°ì •ì§“ëŠ” ìƒìˆ˜ê°€ ëœë‹¤</p><p>ì´ë¡œë¶€í„° ë‹¤ìŒì˜ sampling ì•Œê³ ë¦¬ì¦˜ì´ ìœ ë„ëœë‹¤!!<br></p><p>ì „ì²´ì ìœ¼ë¡œ samplingë˜ëŠ” ê·¸ë¦¼ì„ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë³¼ ìˆ˜ ìˆë‹¤.<br></p><h3 id=2-conditional-sampling-for-ddim>2. Conditional Sampling for DDIM<a hidden class=anchor aria-hidden=true href=#2-conditional-sampling-for-ddim>#</a></h3><p>ìœ„ì˜ ë°©ì‹ìœ¼ë¡œ ê·¼ì‚¬í•˜ëŠ” ê²ƒì€ í™•ë¥ ì  ëª¨ë¸ì¸ DDPMì— ì ìš©ë˜ì§€ë§Œ, ê²°ì •ì  ëª¨ë¸ì¸ DDIMì—ëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤. (ìœ„ì—ì„œ ë³´ì—¬ì§€ëŠ” ê²ƒì²˜ëŸ¼ zë¡œë¶€í„° randomì„±ì„ ì´ìš©í•˜ê¸° ë•Œë¬¸..)</p><p>ë”°ë¼ì„œ, Songì˜ ë…¼ë¬¸ì„ ë¹Œë ¤, score-based conditioning trickì„ ì´ìš©í•´ ì ‘ê·¼í•œë‹¤.</p><p>$\nabla_{x_t}\log{q(x_t)} = -\cfrac{\epsilon_\theta(x_t)}{\sqrt{1-\bar{\alpha}_t}}$ (11)</p><p>ìœ„ì˜ ì‹ì€ deterministic samplingì— ì ìš©ë˜ëŠ” model $\epsilon_\theta(x_t)$ì— ëŒ€í•´ ìœ„ì™€ ê°™ì€ í•¨ìˆ˜ê°€ ìœ ë„ëœë‹¤.</p><p>$d\mathbf{x} = \bigg[\mathbf{f}(\mathbf{x}, t) - g(t)^2\bigtriangledown_x\log{p_t(\mathbf{x})}\bigg]dt + g(t)d\mathbf{w}$<br>ë¥¼ ì´ìš©í•œë“¯ ì‹¶ìŒ..</p><p>ì´ì œ, ì´ë¥¼ ì•ì˜ ì ˆì—ì„œ í•œê²ƒ ì²˜ëŸ¼ $p(x_t)p(y|x_t)$ì—ë‹¤ê°€ ëŒ€ì…í•´ë³´ì.</p><p>$\nabla_{x_t}\log{(p_\theta(x_t)p_\phi(y|x_t))} = \nabla_{x_t}\log{p_\theta(x_t)} + \nabla_{x_t}\log{p_\phi(y|x_t)}$<br>$= -\cfrac{1}{\sqrt{1-\bar{\alpha}<em>t}}\epsilon</em>\theta(x_t) + \nabla_{x_t}\log{p_\phi(y|x_t)}$ (13)</p><p>ìœ„ë¥¼ ì•ì˜ ìƒìˆ˜ë¡œ ë‹¤ì‹œ ë¬¶ì–´ì„œ ìµœì¢…ì ìœ¼ë¡œ ìƒˆë¡œìš´ noise predictionì„ ë§Œë“¤ì–´ ë³¼ ìˆ˜ ìˆë‹¤.
$\hat{\epsilon}(x_t) := \epsilon_\theta(x_t) - \sqrt{1-\bar{\alpha}<em>t}\nabla</em>{x_t}\log{p_{\phi}}(y|x_t)$ (14)</p><p>ì´ë¡œë¶€í„° ì•„ë˜ì˜ Classifier guided DDIM sampling ì•Œê³ ë¦¬ì¦˜ì´ ì •ì˜ëœë‹¤.<br></p><h3 id=3-scaling-classifier-gradients>3. Scaling Classifier Gradients<a hidden class=anchor aria-hidden=true href=#3-scaling-classifier-gradients>#</a></h3><p>ì´ì œ, large scaleì˜ generation taskì— classifierë¥¼ ì ìš©í•˜ê¸° ìœ„í•´ ë¨¼ì € classifierë¥¼ ImageNetì— í•™ìŠµì„ ì‹œì¼°ë‹¤ê³  í•œë‹¤. ì´ ë•Œì˜ ëª¨ë¸ì€ UNet ì´ê³ , 8x8 sizeì˜ layerë¥¼ downsampling ì‹œì¼œ ë¶„ë¥˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ë„ë¡ ë³€í˜•í•˜ì˜€ë‹¤.</p><p>ê·¸ë¦¬ê³ , diffusion modelì—ì„œ ì‚¬ìš©í•œ noiseì™€ ë™ì¼í•œ ë¶„í¬ì— ëŒ€í•´ì„œ classifierë¥¼ í•™ìŠµì‹œí‚¤ê³ , ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ randomcrop augmentationì„ ì ìš©í•˜ì˜€ë‹¤ê³  í•œë‹¤.</p><p>ì´í›„, í•™ìŠµì´ ì¢…ë£Œë˜ê³ ë‚˜ì„œ (10)ì˜ ì‹ì„ ì´ìš©í•˜ì—¬ diffusionëª¨ë¸ê³¼ classifierë¥¼ ê²°í•©ì‹œì¼°ë‹¤.</p><p>ì‹¤í—˜ì—ì„œ classifier gradientê°€ (s ê°’)1ì´ë©´, 50%ì •ë„ì˜ sample ì •í™•ë„ë¥¼ ê°€ì§ì„ ì•Œ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ, ì‹¤ì œë¡œ samplingì„ í•´ë³´ë©´ ê·¸ì— ë¯¸ì¹˜ì§€ ëª»í•œë‹¤. ê·¸ë˜ì„œ ì‹¤í—˜ì„ í•˜ë‹¤ë³´ë‹ˆ 1ë³´ë‹¤ í° ê°’ì„ ì¡ì•„ì•¼ í•¨ì„ ì•Œê²Œ ë˜ì—ˆë‹¤.</p><p>$s \cdot \nabla_x \log{p(y|x)} = \nabla_x \log{\cfrac{1}{Z}p(y|x)^s}$ì˜ ê´€ê³„ë¡œë¶€í„° ì´í•´í•  ìˆ˜ ìˆë‹¤.</p><p>sê°’ì´ ì»¤ì§€ë©´, ì§€ìˆ˜ì ìœ¼ë¡œ ê°’ì´ í­ë°œì ìœ¼ë¡œ ì˜¤ë¥´ê¸° ë•Œë¬¸ë°, ë”ìš± ì ì  ë” sharp í•´ì§€ê²Œ ë˜ê³  ë‹¤ì‹œë§í•˜ë©´ classifierê°€ ë”ìš± modeì— ì§‘ì¤‘í•˜ê²Œ ë¨ì„ ì˜ë¯¸í•œë‹¤.</p><p><strong>ì¦‰, ë” ë†’ì€ fidelityë¥¼ ì–»ê³  diversityë¥¼ ë‚®ì¶”ëŠ” ê²ƒì´ ëœë‹¤.</strong></p><pre><code>classifier gradientê°’ì„ ë†’ì¼ ìˆ˜ë¡, FID ì ìˆ˜ê°€ ë†’ê²Œ ë‚˜ì˜´ì„ ì•Œ ìˆ˜ ìˆë‹¤.(fidelity ë†’ì•„ì§)
ë˜í•œ, sFIDëŠ” ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ê°’ì¸ë°, ê°ˆìˆ˜ë¡ ë‚®ê²Œë‚˜ì˜¤ëŠ” ëª¨ìŠµì„ í†µí•´ ëª¨ë¸ì˜ diversityê°€ ë‚®ì•„ì§ì„ ì•Œ ìˆ˜ ìˆë‹¤.
</code></pre><p>ì •ë§ ëª¨ë“  ë°ì´í„° ì…‹ì— ëŒ€í•´ì„œ ê¸°ì¡´ì˜ ëª¨ë¸ì„ beatí–ˆë‹¤</p><p>ë˜í•œ ì¶”ê°€ì ìœ¼ë¡œ, ì´ë¯¸ì§€ì˜ í€„ë¦¬í‹°ë„ GANë³´ë‹¤ ì¢‹ì€ë° ë‹¤ì–‘ì„±ë§ˆì € ë” ì¢‹ë‹¤.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://dobe0715.github.io/posts/ddpm/><span class=title>Â« Prev</span><br><span></span></a>
<a class=next href=https://dobe0715.github.io/posts/neural-ode/><span class=title>Next Â»</span><br><span></span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share  on twitter" href="https://twitter.com/intent/tweet/?text=&amp;url=https%3a%2f%2fdobe0715.github.io%2fposts%2fmore-diffusion%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share  on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdobe0715.github.io%2fposts%2fmore-diffusion%2f&amp;title=&amp;summary=&amp;source=https%3a%2f%2fdobe0715.github.io%2fposts%2fmore-diffusion%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share  on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdobe0715.github.io%2fposts%2fmore-diffusion%2f&title="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share  on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdobe0715.github.io%2fposts%2fmore-diffusion%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share  on whatsapp" href="https://api.whatsapp.com/send?text=%20-%20https%3a%2f%2fdobe0715.github.io%2fposts%2fmore-diffusion%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share  on telegram" href="https://telegram.me/share/url?text=&amp;url=https%3a%2f%2fdobe0715.github.io%2fposts%2fmore-diffusion%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share  on ycombinator" href="https://news.ycombinator.com/submitlink?t=&u=https%3a%2f%2fdobe0715.github.io%2fposts%2fmore-diffusion%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://dobe0715.github.io>ë¸”ë¡œê·¸ í™ˆ</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
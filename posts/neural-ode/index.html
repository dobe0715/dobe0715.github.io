<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Neural Ordinary Differential Equations review | 블로그 홈</title><meta name=keywords content><meta name=description content="Neural Ordinary Differential Equations 참고자료
paper : https://arxiv.org/abs/1806.07366 blog : https://seewoo5.tistory.com/12 youtube : https://www.youtube.com/watch?v=UegW1cIRee4&amp;t=1594s Contributions 특별한 backpropagation을 사용해 Memory의 절약을 이뤘다. 기존의 반복적인 network들을 일반화하여 해석할 수 있다. forward method ResNet의 경우를 살펴보면, residual connection이 반복되는 구조라고 생각 할 수 있다.
이 때, hidden state가 다음과 같이 residual block을 거치는 과정을 재귀적으로 표현할 수 있다.
$h_{t+1} = h_t + f(h_t, \theta_t)$
이를 풀어서 쓰면, T번째 state는 다음과 같다. $h_T = h_0 + f(h_0, \theta_0) + f(h_1, \theta_1) + &mldr; + f(h_{T-1}, \theta_{T-1})$"><meta name=author content="Me"><link rel=canonical href=https://dobe0715.github.io/posts/neural-ode/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Neural Ordinary Differential Equations review"><meta property="og:description" content="Neural Ordinary Differential Equations 참고자료
paper : https://arxiv.org/abs/1806.07366 blog : https://seewoo5.tistory.com/12 youtube : https://www.youtube.com/watch?v=UegW1cIRee4&amp;t=1594s Contributions 특별한 backpropagation을 사용해 Memory의 절약을 이뤘다. 기존의 반복적인 network들을 일반화하여 해석할 수 있다. forward method ResNet의 경우를 살펴보면, residual connection이 반복되는 구조라고 생각 할 수 있다.
이 때, hidden state가 다음과 같이 residual block을 거치는 과정을 재귀적으로 표현할 수 있다.
$h_{t+1} = h_t + f(h_t, \theta_t)$
이를 풀어서 쓰면, T번째 state는 다음과 같다. $h_T = h_0 + f(h_0, \theta_0) + f(h_1, \theta_1) + &mldr; + f(h_{T-1}, \theta_{T-1})$"><meta property="og:type" content="article"><meta property="og:url" content="https://dobe0715.github.io/posts/neural-ode/"><meta property="og:image" content="https://dobe0715.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dobe0715.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Neural Ordinary Differential Equations review"><meta name=twitter:description content="Neural Ordinary Differential Equations 참고자료
paper : https://arxiv.org/abs/1806.07366 blog : https://seewoo5.tistory.com/12 youtube : https://www.youtube.com/watch?v=UegW1cIRee4&amp;t=1594s Contributions 특별한 backpropagation을 사용해 Memory의 절약을 이뤘다. 기존의 반복적인 network들을 일반화하여 해석할 수 있다. forward method ResNet의 경우를 살펴보면, residual connection이 반복되는 구조라고 생각 할 수 있다.
이 때, hidden state가 다음과 같이 residual block을 거치는 과정을 재귀적으로 표현할 수 있다.
$h_{t+1} = h_t + f(h_t, \theta_t)$
이를 풀어서 쓰면, T번째 state는 다음과 같다. $h_T = h_0 + f(h_0, \theta_0) + f(h_1, \theta_1) + &mldr; + f(h_{T-1}, \theta_{T-1})$"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://dobe0715.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Neural Ordinary Differential Equations review","item":"https://dobe0715.github.io/posts/neural-ode/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Neural Ordinary Differential Equations review","name":"Neural Ordinary Differential Equations review","description":"Neural Ordinary Differential Equations 참고자료\npaper : https://arxiv.org/abs/1806.07366 blog : https://seewoo5.tistory.com/12 youtube : https://www.youtube.com/watch?v=UegW1cIRee4\u0026amp;t=1594s Contributions 특별한 backpropagation을 사용해 Memory의 절약을 이뤘다. 기존의 반복적인 network들을 일반화하여 해석할 수 있다. forward method ResNet의 경우를 살펴보면, residual connection이 반복되는 구조라고 생각 할 수 있다.\n이 때, hidden state가 다음과 같이 residual block을 거치는 과정을 재귀적으로 표현할 수 있다.\n$h_{t+1} = h_t + f(h_t, \\theta_t)$\n이를 풀어서 쓰면, T번째 state는 다음과 같다. $h_T = h_0 + f(h_0, \\theta_0) + f(h_1, \\theta_1) + \u0026hellip; + f(h_{T-1}, \\theta_{T-1})$","keywords":[],"articleBody":"Neural Ordinary Differential Equations 참고자료\npaper : https://arxiv.org/abs/1806.07366 blog : https://seewoo5.tistory.com/12 youtube : https://www.youtube.com/watch?v=UegW1cIRee4\u0026t=1594s Contributions 특별한 backpropagation을 사용해 Memory의 절약을 이뤘다. 기존의 반복적인 network들을 일반화하여 해석할 수 있다. forward method ResNet의 경우를 살펴보면, residual connection이 반복되는 구조라고 생각 할 수 있다.\n이 때, hidden state가 다음과 같이 residual block을 거치는 과정을 재귀적으로 표현할 수 있다.\n$h_{t+1} = h_t + f(h_t, \\theta_t)$\n이를 풀어서 쓰면, T번째 state는 다음과 같다. $h_T = h_0 + f(h_0, \\theta_0) + f(h_1, \\theta_1) + … + f(h_{T-1}, \\theta_{T-1})$\n해당 논문에선, 이러한 iterative한 update를 continuous한 변환과정의 Euler discretization으로 볼 수 있다고 설명한다.\n그래서, 주어진 식을 continuous하게 바꿔주면, 다음과 같다.\n$\\cfrac{d\\mathbf{h}(t)}{dt} = f(\\mathbf{h}(t), t, \\theta)$\n이 때의 f는, __해당 time step 다음(t+1)에 해당하는 layer를 생성해주는 함수__라고 볼 수 있다.\nh(t) : t 시점에서의 hidden state t와 $\\theta$를 분리한 이유 : layer를 무한하게 만들 수 없기 때문. 기존의 network는 이산적인 정해진 시간에 대해서만 state를 보낼 수 있었지만, ODE network를 사용하면 그 외의 경우에도 가능하다. 즉, 1.5층, 2.7층 layer도 실현가능하다는 의미.\nbackward method loss function : $L(\\cdot)$ (scalar function) hidden state : $z(\\cdot)$ (vector) $L(\\mathbf{z}(t_1)) = L\\bigg(\\mathbf{z}(t_0) + \\int_{t_0}^{t_1}{f(\\mathbf{z}(t), t, \\theta)dt} \\bigg) = L(ODESolve(\\mathbf{z}(t_0), f, t_0, t_1, \\theta)$\n다음과 같이 정의한다. 이 때, ODESolve가 존재한다고 가정.\n현재 시점과 state vector와 다음 시점을 입력받아, 함수에 집어넣어 그 다음 step까지의 편차를 계산하고 더한다는 의미.\nadjoint sensitivity method 모델을 optimize하기 위해서는 근본적으로, loss인 L에대한 파라미터 $\\theta$의 미분을 구해야 하고 이는 각 $z, t, \\theta$로 loss에 대한 미분 계산으로 이어진다.\n이에 대한 방법론을 제시한다.\n$\\mathbf{a}(t) := \\cfrac{\\partial{L}}{\\partial{\\mathbf{z}(t)}}$\n이 a를 adjoint라고 부르며, time step에서의 hidden state output으로 미분한 것을 의미한다.\n실제로 컴퓨터로 계산할때는 discrete하게 되므로, $t_i, t_{i+1}$step마다 dynamic하게 계산해주면 된다.\nchain rule과 미분의 정의, taylor 전개를 이용해서 다음을 유도할 수 있다.\n$\\cfrac{d\\mathbf{a}(t)}{dt} = -\\mathbf{a}(t)^T \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\mathbf{z}}$\nasm proof 위의 식으로부터, $\\theta$를 통한 미분값도 계산할 수 있다.\n$\\cfrac{dL}{d\\theta} = -\\int_{t_1}^{t_0} \\mathbf{a}(t)^T \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\mathbf{z}} dt$\n이렇게 미분값을 계산하게 되면장점이 무엇이냐, 바로 memory complexity 감소이다.\n원래의 Network들은 각 layer의 마다 미분값을 저장하고 갱신해줘야 한다.\n하지만, 여기에서는 초기값 a(T)만 저장해놓으면, ODESolve 모듈에 집어넣어서 한번만 계산해주면 된다.\nResNet : 6개의 residual blocks을 사용. RK-Net : Runge-Kutta integrator를 이용해 직접 backpropagation 계산 ODE-Net : ours 코드 github : https://github.com/msurtsukov/neural-ode Neural ODE base 함수 import math import numpy as np from IPython.display import clear_output from tqdm import tqdm_notebook as tqdm import matplotlib as mpl import matplotlib.pyplot as plt %matplotlib inline import seaborn as sns sns.color_palette(\"bright\") import matplotlib as mpl import matplotlib.cm as cm import torch from torch import Tensor from torch import nn from torch.nn import functional as F from torch.autograd import Variable use_cuda = torch.cuda.is_available() def ode_solve(z0, t0, t1, f): \"\"\" Simplest Euler ODE initial value solver \"\"\" h_max = 0.05 n_steps = math.ceil((abs(t1 - t0)/h_max).max().item()) # hidden state를 0.05초와 가깝도록 시간을 잘라서 계산함(0.046~0.05초). 그보다 작은 단위는 작은 단위로 한 번만 계산 # ex) time list가 [0, 1, 1.5, 1.51, 2] 이러면, 0.05*20 / 0.05*10 / 0.01*1 / 0.049*10 h = (t1 - t0)/n_steps t = t0 z = z0 # solver function을 이용해 계산. for i_step in range(n_steps): z = z + h * f(z, t) t = t + h return z class ODEF(nn.Module): def forward_with_grad(self, z, t, grad_outputs): \"\"\"Compute f and a df/dz, a df/dp, a df/dt\"\"\" batch_size = z.shape[0] out = self.forward(z, t) # adjoint method. a * df/d(*) # a 를 grad_outputs로 두고, autograd.grad를 해주면, 곱해주는 효과가 난다. a = grad_outputs adfdz, adfdt, *adfdp = torch.autograd.grad( (out,), (z, t) + tuple(self.parameters()), grad_outputs=(a), allow_unused=True, retain_graph=True ) # grad method automatically sums gradients for batch items, we have to expand them back if adfdp is not None: adfdp = torch.cat([p_grad.flatten() for p_grad in adfdp]).unsqueeze(0) adfdp = adfdp.expand(batch_size, -1) / batch_size if adfdt is not None: adfdt = adfdt.expand(batch_size, 1) / batch_size return out, adfdz, adfdt, adfdp def flatten_parameters(self): p_shapes = [] flat_parameters = [] for p in self.parameters(): p_shapes.append(p.size()) flat_parameters.append(p.flatten()) return torch.cat(flat_parameters) static method class 를 사용할 때, 보통은 객체를 생성하여 성질을 넘겨받아서 사용하는데, 객체 없이 class에 바로 접근할 수 있다.\nclass 안에 함수에 데코레이터를 활용하여 적용하고\n이를 사용하면 class에 독립적인 함수를 가시성 있게 사용할 수 있으며, 메모리의 장점도 가지고 있다.\n또한, 독립적이기 때문에, 상속받으면 해당 메소드는 그대로 유지된다.\n# static method class test_class: def __init__(self): return print(\"instance made\") @staticmethod def test_output(num1, num2): return num1 + num2 # staticmethod print(test_class.test_output(1, 2)) # 3 # original way cls_instance = test_class() # instance made print(cls_instance.test_output(1, 2)) # 3 3 instance made 3 class ODEAdjoint(torch.autograd.Function): @staticmethod def forward(ctx, z0, t, flat_parameters, func): # func : 학습하고자 하는 함수 f # staticmethod : 객체를 정의하지 않고 class에 직접 접근하는 방법 assert isinstance(func, ODEF) bs, *z_shape = z0.size() time_len = t.size(0) with torch.no_grad(): z = torch.zeros(time_len, bs, *z_shape).to(z0) z[0] = z0 for i_t in range(time_len - 1): z0 = ode_solve(z0, t[i_t], t[i_t+1], func) z[i_t+1] = z0 ctx.func = func ctx.save_for_backward(t, z.clone(), flat_parameters) return z @staticmethod def backward(ctx, dLdz): \"\"\" dLdz shape: time_len, batch_size, *z_shape \"\"\" func = ctx.func t, z, flat_parameters = ctx.saved_tensors time_len, bs, *z_shape = z.size() n_dim = np.prod(z_shape) n_params = flat_parameters.size(0) # Dynamics of augmented system to be calculated backwards in time # adjoint sensitivity method로 사용하기 위해 새롭게 정의한 solver 함수. # paper에서 말했듯이, 기울기를 저장하지 않고, time_step마다 삭제해준다. def augmented_dynamics(aug_z_i, t_i): \"\"\" tensors here are temporal slices t_i - is tensor with size: bs, 1 aug_z_i - is tensor with size: bs, n_dim*2 + n_params + 1 \"\"\" z_i, a = aug_z_i[:, :n_dim], aug_z_i[:, n_dim:2*n_dim] # ignore parameters and time # Unflatten z and a z_i = z_i.view(bs, *z_shape) a = a.view(bs, *z_shape) with torch.set_grad_enabled(True): t_i = t_i.detach().requires_grad_(True) z_i = z_i.detach().requires_grad_(True) func_eval, adfdz, adfdt, adfdp = func.forward_with_grad(z_i, t_i, grad_outputs=a) # bs, *z_shape adfdz = adfdz.to(z_i) if adfdz is not None else torch.zeros(bs, *z_shape).to(z_i) adfdp = adfdp.to(z_i) if adfdp is not None else torch.zeros(bs, n_params).to(z_i) adfdt = adfdt.to(z_i) if adfdt is not None else torch.zeros(bs, 1).to(z_i) # Flatten f and adfdz func_eval = func_eval.view(bs, n_dim) adfdz = adfdz.view(bs, n_dim) return torch.cat((func_eval, -adfdz, -adfdp, -adfdt), dim=1) dLdz = dLdz.view(time_len, bs, n_dim) # flatten dLdz for convenience with torch.no_grad(): ## Create placeholders for output gradients # Prev computed backwards adjoints to be adjusted by direct gradients adj_z = torch.zeros(bs, n_dim).to(dLdz) adj_p = torch.zeros(bs, n_params).to(dLdz) # In contrast to z and p we need to return gradients for all times adj_t = torch.zeros(time_len, bs, 1).to(dLdz) for i_t in range(time_len-1, 0, -1): z_i = z[i_t] t_i = t[i_t] f_i = func(z_i, t_i).view(bs, n_dim) # Compute direct gradients dLdz_i = dLdz[i_t] dLdt_i = torch.bmm(torch.transpose(dLdz_i.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0] # Adjusting adjoints with direct gradients adj_z += dLdz_i adj_t[i_t] = adj_t[i_t] - dLdt_i # Pack augmented variable aug_z = torch.cat((z_i.view(bs, n_dim), adj_z, torch.zeros(bs, n_params).to(z), adj_t[i_t]), dim=-1) # Solve augmented system backwards aug_ans = ode_solve(aug_z, t_i, t[i_t-1], augmented_dynamics) # Unpack solved backwards augmented system adj_z[:] = aug_ans[:, n_dim:2*n_dim] adj_p[:] += aug_ans[:, 2*n_dim:2*n_dim + n_params] adj_t[i_t-1] = aug_ans[:, 2*n_dim + n_params:] del aug_z, aug_ans ## Adjust 0 time adjoint with direct gradients # Compute direct gradients dLdz_0 = dLdz[0] dLdt_0 = torch.bmm(torch.transpose(dLdz_0.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0] # Adjust adjoints adj_z += dLdz_0 adj_t[0] = adj_t[0] - dLdt_0 return adj_z.view(bs, *z_shape), adj_t, adj_p, None # apply method @torch.no_grad() def init_weights(m): print(m) if type(m) == nn.Linear: m.weight.fill_(1.0) print(m.weight) net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[1., 1.], [1., 1.]], requires_grad=True) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[1., 1.], [1., 1.]], requires_grad=True) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) class NeuralODE(nn.Module): def __init__(self, func): super(NeuralODE, self).__init__() assert isinstance(func, ODEF) self.func = func def forward(self, z0, t=Tensor([0., 1.]), return_whole_sequence=False): t = t.to(z0) z = ODEAdjoint.apply(z0, t, self.func.flatten_parameters(), self.func) if return_whole_sequence: return z else: return z[-1] train utils class LinearODEF(ODEF): def __init__(self, W): super(LinearODEF, self).__init__() self.lin = nn.Linear(2, 2, bias=False) self.lin.weight = nn.Parameter(W) def forward(self, x, t): return self.lin(x) # 맞추고자하는 target function class SpiralFunctionExample(LinearODEF): def __init__(self): super(SpiralFunctionExample, self).__init__(Tensor([[-0.1, -1.], [1., -0.1]])) # initialize할 training function class RandomLinearODEF(LinearODEF): def __init__(self): super(RandomLinearODEF, self).__init__(torch.randn(2, 2)/2.) # 2*2 tensor로 되어있는 레이어를 통해, 새로운 manifold로 mapping해주는 layer. # 구체적인 값은 밑의 forward를 통해 알 수 있다. # 둘이 값을 각각 매기고, 빼준다. (trajectory생성하기 위함.(t번째랑 t-1번째 값을 빼면, 그 사이의 간격 출력되는 것을 이용)) class TestODEF(ODEF): def __init__(self, A, B, x0): super(TestODEF, self).__init__() self.A = nn.Linear(2, 2, bias=False) self.A.weight = nn.Parameter(A) self.B = nn.Linear(2, 2, bias=False) self.B.weight = nn.Parameter(B) self.x0 = nn.Parameter(x0) def forward(self, x, t): xTx0 = torch.sum(x*self.x0, dim=1) dxdt = torch.sigmoid(xTx0) * self.A(x - self.x0) + torch.sigmoid(-xTx0) * self.B(x + self.x0) return dxdt # hidden dimension을 설정해놓고, 2 -\u003e 16 -\u003e 2 로 계산시켜서 출력시키도록 한다. class NNODEF(ODEF): def __init__(self, in_dim, hid_dim, time_invariant=False): super(NNODEF, self).__init__() self.time_invariant = time_invariant if time_invariant: self.lin1 = nn.Linear(in_dim, hid_dim) else: self.lin1 = nn.Linear(in_dim+1, hid_dim) self.lin2 = nn.Linear(hid_dim, hid_dim) self.lin3 = nn.Linear(hid_dim, in_dim) self.elu = nn.ELU(inplace=True) def forward(self, x, t): if not self.time_invariant: x = torch.cat((x, t), dim=-1) h = self.elu(self.lin1(x)) h = self.elu(self.lin2(h)) out = self.lin3(h) return out def to_np(x): return x.detach().cpu().numpy() def plot_trajectories(obs=None, times=None, trajs=None, save=None, figsize=(16, 8)): plt.figure(figsize=figsize) if obs is not None: if times is None: times = [None] * len(obs) for o, t in zip(obs, times): o, t = to_np(o), to_np(t) for b_i in range(o.shape[1]): plt.scatter(o[:, b_i, 0], o[:, b_i, 1], c=t[:, b_i, 0], cmap=cm.plasma) if trajs is not None: for z in trajs: z = to_np(z) plt.plot(z[:, 0, 0], z[:, 0, 1], lw=1.5) if save is not None: plt.savefig(save) plt.show() def conduct_experiment(ode_true, ode_trained, n_steps, name, plot_freq=10): # Create data # hidden state의 initial 값 z0 = Variable(torch.Tensor([[0.6, 0.3]])) t_max = 6.29*5 n_points = 200 index_np = np.arange(0, n_points, 1, dtype=np.int) index_np = np.hstack([index_np[:, None]]) times_np = np.linspace(0, t_max, num=n_points) times_np = np.hstack([times_np[:, None]]) times = torch.from_numpy(times_np[:, :, None]).to(z0) obs = ode_true(z0, times, return_whole_sequence=True).detach() obs = obs + torch.randn_like(obs) * 0.01 # Get trajectory of random timespan min_delta_time = 1.0 max_delta_time = 5.0 max_points_num = 32 def create_batch(): t0 = np.random.uniform(0, t_max - max_delta_time) t1 = t0 + np.random.uniform(min_delta_time, max_delta_time) idx = sorted(np.random.permutation(index_np[(times_np \u003e t0) \u0026 (times_np \u003c t1)])[:max_points_num]) obs_ = obs[idx] ts_ = times[idx] return obs_, ts_ # Train Neural ODE optimizer = torch.optim.Adam(ode_trained.parameters(), lr=0.01) for i in range(n_steps): obs_, ts_ = create_batch() z_ = ode_trained(obs_[0], ts_, return_whole_sequence=True) loss = F.mse_loss(z_, obs_.detach()) optimizer.zero_grad() loss.backward(retain_graph=True) optimizer.step() if i % plot_freq == 0: z_p = ode_trained(z0, times, return_whole_sequence=True) plot_trajectories(obs=[obs], times=[times], trajs=[z_p], save=f\"./assets/imgs/{name}/{i}.png\") clear_output(wait=True) application : concepts ode_true = NeuralODE(SpiralFunctionExample()) ode_trained = NeuralODE(RandomLinearODEF()) conduct_experiment(ode_true, ode_trained, 500, \"linear\") func = TestODEF(Tensor([[-0.1, -0.5], [0.5, -0.1]]), Tensor([[0.2, 1.], [-1, 0.2]]), Tensor([[-1., 0.]])) ode_true = NeuralODE(func) func = NNODEF(2, 16, time_invariant=True) ode_trained = NeuralODE(func) conduct_experiment(ode_true, ode_trained, 3000, \"comp\", plot_freq=30) application : ResNet def norm(dim): return nn.BatchNorm2d(dim) # 크기 안변하는 conv연산. # receptive field만 상승한다. def conv3x3(in_feats, out_feats, stride=1): return nn.Conv2d(in_feats, out_feats, kernel_size=3, stride=stride, padding=1, bias=False) def add_time(in_tensor, t): bs, c, w, h = in_tensor.shape return torch.cat((in_tensor, t.expand(bs, 1, w, h)), dim=1) class ConvODEF(ODEF): def __init__(self, dim): super(ConvODEF, self).__init__() # 중간 feature dimension 1 줄여놓고 연산한다.(왜 늘리지 않고 줄이지) self.conv1 = conv3x3(dim + 1, dim) self.norm1 = norm(dim) self.conv2 = conv3x3(dim + 1, dim) self.norm2 = norm(dim) def forward(self, x, t): xt = add_time(x, t) h = self.norm1(torch.relu(self.conv1(xt))) ht = add_time(h, t) dxdt = self.norm2(torch.relu(self.conv2(ht))) return dxdt class ContinuousNeuralMNISTClassifier(nn.Module): def __init__(self, ode): super(ContinuousNeuralMNISTClassifier, self).__init__() self.downsampling = nn.Sequential( nn.Conv2d(1, 64, 3, 1), norm(64), nn.ReLU(inplace=True), nn.Conv2d(64, 64, 4, 2, 1), norm(64), nn.ReLU(inplace=True), nn.Conv2d(64, 64, 4, 2, 1), ) self.feature = ode self.norm = norm(64) self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) self.fc = nn.Linear(64, 10) def forward(self, x): x = self.downsampling(x) x = self.feature(x) x = self.norm(x) x = self.avg_pool(x) shape = torch.prod(torch.tensor(x.shape[1:])).item() x = x.view(-1, shape) out = self.fc(x) return out func = ConvODEF(64) ode = NeuralODE(func) model = ContinuousNeuralMNISTClassifier(ode) if use_cuda: model = model.cuda() func ConvODEF( (conv1): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ode NeuralODE( (func): ConvODEF( (conv1): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) model ContinuousNeuralMNISTClassifier( (downsampling): Sequential( (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)) (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)) ) (feature): NeuralODE( (func): ConvODEF( (conv1): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1)) (fc): Linear(in_features=64, out_features=10, bias=True) ) import torchvision img_std = 0.3081 img_mean = 0.1307 batch_size = 32 train_loader = torch.utils.data.DataLoader( torchvision.datasets.MNIST(\".data/mnist\", train=True, download=True, transform=torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((img_mean,), (img_std,)) ]) ), batch_size=batch_size, shuffle=True ) test_loader = torch.utils.data.DataLoader( torchvision.datasets.MNIST(\".data/mnist\", train=False, download=True, transform=torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((img_mean,), (img_std,)) ]) ), batch_size=128, shuffle=True ) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to .data/mnist/MNIST/raw/train-images-idx3-ubyte.gz 100%|██████████| 9912422/9912422 [00:00\u003c00:00, 104652221.37it/s] Extracting .data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to .data/mnist/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to .data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz 100%|██████████| 28881/28881 [00:00\u003c00:00, 33352338.61it/s] Extracting .data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to .data/mnist/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to .data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz 100%|██████████| 1648877/1648877 [00:00\u003c00:00, 28561776.33it/s] Extracting .data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to .data/mnist/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to .data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz 100%|██████████| 4542/4542 [00:00\u003c00:00, 3722987.84it/s] Extracting .data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to .data/mnist/MNIST/raw optimizer = torch.optim.Adam(model.parameters()) def train(epoch): num_items = 0 train_losses = [] model.train() criterion = nn.CrossEntropyLoss() print(f\"Training Epoch {epoch}...\") for batch_idx, (data, target) in tqdm(enumerate(train_loader), total=len(train_loader)): if use_cuda: data = data.cuda() target = target.cuda() optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() train_losses += [loss.item()] num_items += data.shape[0] print('Train loss: {:.5f}'.format(np.mean(train_losses))) return train_losses def test(): accuracy = 0.0 num_items = 0 model.eval() criterion = nn.CrossEntropyLoss() print(f\"Testing...\") with torch.no_grad(): for batch_idx, (data, target) in tqdm(enumerate(test_loader), total=len(test_loader)): if use_cuda: data = data.cuda() target = target.cuda() output = model(data) accuracy += torch.sum(torch.argmax(output, dim=1) == target).item() num_items += data.shape[0] accuracy = accuracy * 100 / num_items print(\"Test Accuracy: {:.3f}%\".format(accuracy)) n_epochs = 5 test() train_losses = [] for epoch in range(1, n_epochs + 1): train_losses += train(epoch) test() Testing... :9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0 Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook` for batch_idx, (data, target) in tqdm(enumerate(test_loader), total=len(test_loader)): 0%| | 0/79 [00:00\u003c?, ?it/s] Test Accuracy: 13.320% Training Epoch 1... :8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0 Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook` for batch_idx, (data, target) in tqdm(enumerate(train_loader), total=len(train_loader)): 0%| | 0/1875 [00:00\u003c?, ?it/s] Train loss: 0.15641 Testing... 0%| | 0/79 [00:00\u003c?, ?it/s] Test Accuracy: 98.370% Training Epoch 2... 0%| | 0/1875 [00:00\u003c?, ?it/s] Train loss: 0.04976 Testing... 0%| | 0/79 [00:00\u003c?, ?it/s] Test Accuracy: 98.890% Training Epoch 3... 0%| | 0/1875 [00:00\u003c?, ?it/s] Train loss: 0.03789 Testing... 0%| | 0/79 [00:00\u003c?, ?it/s] Test Accuracy: 98.450% Training Epoch 4... 0%| | 0/1875 [00:00\u003c?, ?it/s] Train loss: 0.02935 Testing... 0%| | 0/79 [00:00\u003c?, ?it/s] Test Accuracy: 98.860% Training Epoch 5... 0%| | 0/1875 [00:00\u003c?, ?it/s] Train loss: 0.02512 Testing... 0%| | 0/79 [00:00\u003c?, ?it/s] Test Accuracy: 98.970% import pandas as pd plt.figure(figsize=(9, 5)) history = pd.DataFrame({\"loss\": train_losses}) history[\"cum_data\"] = history.index * batch_size history[\"smooth_loss\"] = history.loss.ewm(halflife=10).mean() history.plot(x=\"cum_data\", y=\"smooth_loss\", figsize=(12, 5), title=\"train error\") ","wordCount":"2505","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://dobe0715.github.io/posts/neural-ode/"},"publisher":{"@type":"Organization","name":"블로그 홈","logo":{"@type":"ImageObject","url":"https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dobe0715.github.io accesskey=h title="Home (Alt + H)"><img src=https://dobe0715.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dobe0715.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://dobe0715.github.io/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dobe0715.github.io>Home</a>&nbsp;»&nbsp;<a href=https://dobe0715.github.io/posts/>Posts</a></div><h1 class=post-title>Neural Ordinary Differential Equations review</h1><div class=post-meta>12 min&nbsp;·&nbsp;2505 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/Neural%20ODE.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><h1 id=neural-ordinary-differential-equations>Neural Ordinary Differential Equations<a hidden class=anchor aria-hidden=true href=#neural-ordinary-differential-equations>#</a></h1><p>참고자료</p><ul><li>paper : <a href=https://arxiv.org/abs/1806.07366>https://arxiv.org/abs/1806.07366</a></li><li>blog : <a href=https://seewoo5.tistory.com/12>https://seewoo5.tistory.com/12</a></li><li>youtube : <a href="https://www.youtube.com/watch?v=UegW1cIRee4&amp;t=1594s">https://www.youtube.com/watch?v=UegW1cIRee4&amp;t=1594s</a></li></ul><h2 id=contributions>Contributions<a hidden class=anchor aria-hidden=true href=#contributions>#</a></h2><ul><li>특별한 backpropagation을 사용해 Memory의 절약을 이뤘다.</li><li>기존의 반복적인 network들을 일반화하여 해석할 수 있다.</li></ul><h2 id=forward-method>forward method<a hidden class=anchor aria-hidden=true href=#forward-method>#</a></h2><p>ResNet의 경우를 살펴보면, residual connection이 반복되는 구조라고 생각 할 수 있다.</p><p>이 때, hidden state가 다음과 같이 residual block을 거치는 과정을 재귀적으로 표현할 수 있다.<br>$h_{t+1} = h_t + f(h_t, \theta_t)$</p><p>이를 풀어서 쓰면, T번째 state는 다음과 같다.
$h_T = h_0 + f(h_0, \theta_0) + f(h_1, \theta_1) + &mldr; + f(h_{T-1}, \theta_{T-1})$</p><p>해당 논문에선, 이러한 iterative한 update를 continuous한 변환과정의 Euler discretization으로 볼 수 있다고 설명한다.</p><p></p><p>그래서, 주어진 식을 continuous하게 바꿔주면, 다음과 같다.<br>$\cfrac{d\mathbf{h}(t)}{dt} = f(\mathbf{h}(t), t, \theta)$</p><p>이 때의 f는, __해당 time step 다음(t+1)에 해당하는 layer를 생성해주는 함수__라고 볼 수 있다.</p><ul><li>h(t) : t 시점에서의 hidden state</li><li>t와 $\theta$를 분리한 이유 : layer를 무한하게 만들 수 없기 때문.</li></ul><p>기존의 network는 이산적인 정해진 시간에 대해서만 state를 보낼 수 있었지만, ODE network를 사용하면 그 외의 경우에도 가능하다. 즉, 1.5층, 2.7층 layer도 실현가능하다는 의미.</p><h2 id=backward-method>backward method<a hidden class=anchor aria-hidden=true href=#backward-method>#</a></h2><ul><li>loss function : $L(\cdot)$ (scalar function)</li><li>hidden state : $z(\cdot)$ (vector)</li></ul><p>$L(\mathbf{z}(t_1)) = L\bigg(\mathbf{z}(t_0) + \int_{t_0}^{t_1}{f(\mathbf{z}(t), t, \theta)dt} \bigg) = L(ODESolve(\mathbf{z}(t_0), f, t_0, t_1, \theta)$</p><p>다음과 같이 정의한다. 이 때, ODESolve가 존재한다고 가정.<br>현재 시점과 state vector와 다음 시점을 입력받아, 함수에 집어넣어 그 다음 step까지의 편차를 계산하고 더한다는 의미.</p><h3 id=adjoint-sensitivity-method>adjoint sensitivity method<a hidden class=anchor aria-hidden=true href=#adjoint-sensitivity-method>#</a></h3><p>모델을 optimize하기 위해서는 근본적으로, loss인 L에대한 파라미터 $\theta$의 미분을 구해야 하고 이는 각 $z, t, \theta$로 loss에 대한 미분 계산으로 이어진다.<br>이에 대한 방법론을 제시한다.</p><p>$\mathbf{a}(t) := \cfrac{\partial{L}}{\partial{\mathbf{z}(t)}}$<br>이 a를 adjoint라고 부르며, time step에서의 hidden state output으로 미분한 것을 의미한다.<br>실제로 컴퓨터로 계산할때는 discrete하게 되므로, $t_i, t_{i+1}$step마다 dynamic하게 계산해주면 된다.</p><p>chain rule과 미분의 정의, taylor 전개를 이용해서 다음을 유도할 수 있다.<br>$\cfrac{d\mathbf{a}(t)}{dt} = -\mathbf{a}(t)^T \frac{\partial f(\mathbf{z}(t), t, \theta)}{\partial \mathbf{z}}$</p><h4 id=asm-proof>asm proof<a hidden class=anchor aria-hidden=true href=#asm-proof>#</a></h4><h4 id=heading><a hidden class=anchor aria-hidden=true href=#heading>#</a></h4><p>위의 식으로부터, $\theta$를 통한 미분값도 계산할 수 있다.</p><p>$\cfrac{dL}{d\theta} = -\int_{t_1}^{t_0} \mathbf{a}(t)^T \frac{\partial f(\mathbf{z}(t), t, \theta)}{\partial \mathbf{z}} dt$</p><p>이렇게 미분값을 계산하게 되면장점이 무엇이냐, 바로 memory complexity 감소이다.<br>원래의 Network들은 각 layer의 마다 미분값을 저장하고 갱신해줘야 한다.</p><p>하지만, 여기에서는 초기값 a(T)만 저장해놓으면, ODESolve 모듈에 집어넣어서 한번만 계산해주면 된다.</p><ul><li>ResNet : 6개의 residual blocks을 사용.</li><li>RK-Net : Runge-Kutta integrator를 이용해 직접 backpropagation 계산</li><li>ODE-Net : ours</li></ul><h2 id=코드>코드<a hidden class=anchor aria-hidden=true href=#코드>#</a></h2><ul><li>github : <a href=https://github.com/msurtsukov/neural-ode>https://github.com/msurtsukov/neural-ode</a></li></ul><h3 id=neural-ode-base-함수>Neural ODE base 함수<a hidden class=anchor aria-hidden=true href=#neural-ode-base-함수>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>IPython.display</span> <span class=kn>import</span> <span class=n>clear_output</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tqdm</span> <span class=kn>import</span> <span class=n>tqdm_notebook</span> <span class=k>as</span> <span class=n>tqdm</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib</span> <span class=k>as</span> <span class=nn>mpl</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=o>%</span><span class=n>matplotlib</span> <span class=n>inline</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>seaborn</span> <span class=k>as</span> <span class=nn>sns</span>
</span></span><span class=line><span class=cl><span class=n>sns</span><span class=o>.</span><span class=n>color_palette</span><span class=p>(</span><span class=s2>&#34;bright&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib</span> <span class=k>as</span> <span class=nn>mpl</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.cm</span> <span class=k>as</span> <span class=nn>cm</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>Tensor</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.nn</span>  <span class=kn>import</span> <span class=n>functional</span> <span class=k>as</span> <span class=n>F</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.autograd</span> <span class=kn>import</span> <span class=n>Variable</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>use_cuda</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>ode_solve</span><span class=p>(</span><span class=n>z0</span><span class=p>,</span> <span class=n>t0</span><span class=p>,</span> <span class=n>t1</span><span class=p>,</span> <span class=n>f</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Simplest Euler ODE initial value solver
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>h_max</span> <span class=o>=</span> <span class=mf>0.05</span>
</span></span><span class=line><span class=cl>    <span class=n>n_steps</span> <span class=o>=</span> <span class=n>math</span><span class=o>.</span><span class=n>ceil</span><span class=p>((</span><span class=nb>abs</span><span class=p>(</span><span class=n>t1</span> <span class=o>-</span> <span class=n>t0</span><span class=p>)</span><span class=o>/</span><span class=n>h_max</span><span class=p>)</span><span class=o>.</span><span class=n>max</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># hidden state를 0.05초와 가깝도록 시간을 잘라서 계산함(0.046~0.05초). 그보다 작은 단위는 작은 단위로 한 번만 계산</span>
</span></span><span class=line><span class=cl>    <span class=c1># ex) time list가 [0, 1, 1.5, 1.51, 2] 이러면, 0.05*20 / 0.05*10 / 0.01*1 / 0.049*10</span>
</span></span><span class=line><span class=cl>    <span class=n>h</span> <span class=o>=</span> <span class=p>(</span><span class=n>t1</span> <span class=o>-</span> <span class=n>t0</span><span class=p>)</span><span class=o>/</span><span class=n>n_steps</span>
</span></span><span class=line><span class=cl>    <span class=n>t</span> <span class=o>=</span> <span class=n>t0</span>
</span></span><span class=line><span class=cl>    <span class=n>z</span> <span class=o>=</span> <span class=n>z0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># solver function을 이용해 계산.</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i_step</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_steps</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>z</span> <span class=o>=</span> <span class=n>z</span> <span class=o>+</span> <span class=n>h</span> <span class=o>*</span> <span class=n>f</span><span class=p>(</span><span class=n>z</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>t</span> <span class=o>=</span> <span class=n>t</span> <span class=o>+</span> <span class=n>h</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>z</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>ODEF</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward_with_grad</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>z</span><span class=p>,</span> <span class=n>t</span><span class=p>,</span> <span class=n>grad_outputs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Compute f and a df/dz, a df/dp, a df/dt&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_size</span> <span class=o>=</span> <span class=n>z</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>z</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># adjoint method. a * df/d(*)</span>
</span></span><span class=line><span class=cl>        <span class=c1># a 를 grad_outputs로 두고, autograd.grad를 해주면, 곱해주는 효과가 난다.</span>
</span></span><span class=line><span class=cl>        <span class=n>a</span> <span class=o>=</span> <span class=n>grad_outputs</span>
</span></span><span class=line><span class=cl>        <span class=n>adfdz</span><span class=p>,</span> <span class=n>adfdt</span><span class=p>,</span> <span class=o>*</span><span class=n>adfdp</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>autograd</span><span class=o>.</span><span class=n>grad</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=p>(</span><span class=n>out</span><span class=p>,),</span> <span class=p>(</span><span class=n>z</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span> <span class=o>+</span> <span class=nb>tuple</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>parameters</span><span class=p>()),</span> <span class=n>grad_outputs</span><span class=o>=</span><span class=p>(</span><span class=n>a</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>allow_unused</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>retain_graph</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># grad method automatically sums gradients for batch items, we have to expand them back</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>adfdp</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>adfdp</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>p_grad</span><span class=o>.</span><span class=n>flatten</span><span class=p>()</span> <span class=k>for</span> <span class=n>p_grad</span> <span class=ow>in</span> <span class=n>adfdp</span><span class=p>])</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>adfdp</span> <span class=o>=</span> <span class=n>adfdp</span><span class=o>.</span><span class=n>expand</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span> <span class=o>/</span> <span class=n>batch_size</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>adfdt</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>adfdt</span> <span class=o>=</span> <span class=n>adfdt</span><span class=o>.</span><span class=n>expand</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span> <span class=o>/</span> <span class=n>batch_size</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span><span class=p>,</span> <span class=n>adfdz</span><span class=p>,</span> <span class=n>adfdt</span><span class=p>,</span> <span class=n>adfdp</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>flatten_parameters</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>p_shapes</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=n>flat_parameters</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>parameters</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>p_shapes</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
</span></span><span class=line><span class=cl>            <span class=n>flat_parameters</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>flatten</span><span class=p>())</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span><span class=n>flat_parameters</span><span class=p>)</span>
</span></span></code></pre></div><h4 id=static-method>static method<a hidden class=anchor aria-hidden=true href=#static-method>#</a></h4><p>class 를 사용할 때, 보통은 객체를 생성하여 성질을 넘겨받아서 사용하는데,
객체 없이 class에 바로 접근할 수 있다.</p><p>class 안에 함수에 데코레이터를 활용하여 적용하고<br>이를 사용하면 class에 독립적인 함수를 가시성 있게 사용할 수 있으며, 메모리의 장점도 가지고 있다.<br>또한, 독립적이기 때문에, 상속받으면 해당 메소드는 그대로 유지된다.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># static method</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>test_class</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;instance made&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@staticmethod</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>test_output</span><span class=p>(</span><span class=n>num1</span><span class=p>,</span> <span class=n>num2</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>num1</span> <span class=o>+</span> <span class=n>num2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># staticmethod</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>test_class</span><span class=o>.</span><span class=n>test_output</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span> <span class=c1># 3</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># original way</span>
</span></span><span class=line><span class=cl><span class=n>cls_instance</span> <span class=o>=</span> <span class=n>test_class</span><span class=p>()</span> <span class=c1># instance made</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>cls_instance</span><span class=o>.</span><span class=n>test_output</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span> <span class=c1># 3</span>
</span></span></code></pre></div><pre><code>3
instance made
3
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>ODEAdjoint</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>autograd</span><span class=o>.</span><span class=n>Function</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nd>@staticmethod</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=n>ctx</span><span class=p>,</span> <span class=n>z0</span><span class=p>,</span> <span class=n>t</span><span class=p>,</span> <span class=n>flat_parameters</span><span class=p>,</span> <span class=n>func</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># func : 학습하고자 하는 함수 f</span>
</span></span><span class=line><span class=cl>        <span class=c1># staticmethod : 객체를 정의하지 않고 class에 직접 접근하는 방법</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>func</span><span class=p>,</span> <span class=n>ODEF</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>bs</span><span class=p>,</span> <span class=o>*</span><span class=n>z_shape</span> <span class=o>=</span> <span class=n>z0</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>time_len</span> <span class=o>=</span> <span class=n>t</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>z</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>time_len</span><span class=p>,</span> <span class=n>bs</span><span class=p>,</span> <span class=o>*</span><span class=n>z_shape</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>z0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>z</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=n>z0</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>i_t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>time_len</span> <span class=o>-</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=n>z0</span> <span class=o>=</span> <span class=n>ode_solve</span><span class=p>(</span><span class=n>z0</span><span class=p>,</span> <span class=n>t</span><span class=p>[</span><span class=n>i_t</span><span class=p>],</span> <span class=n>t</span><span class=p>[</span><span class=n>i_t</span><span class=o>+</span><span class=mi>1</span><span class=p>],</span> <span class=n>func</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>z</span><span class=p>[</span><span class=n>i_t</span><span class=o>+</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=n>z0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>ctx</span><span class=o>.</span><span class=n>func</span> <span class=o>=</span> <span class=n>func</span>
</span></span><span class=line><span class=cl>        <span class=n>ctx</span><span class=o>.</span><span class=n>save_for_backward</span><span class=p>(</span><span class=n>t</span><span class=p>,</span> <span class=n>z</span><span class=o>.</span><span class=n>clone</span><span class=p>(),</span> <span class=n>flat_parameters</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>z</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@staticmethod</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>backward</span><span class=p>(</span><span class=n>ctx</span><span class=p>,</span> <span class=n>dLdz</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        dLdz shape: time_len, batch_size, *z_shape
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>func</span> <span class=o>=</span> <span class=n>ctx</span><span class=o>.</span><span class=n>func</span>
</span></span><span class=line><span class=cl>        <span class=n>t</span><span class=p>,</span> <span class=n>z</span><span class=p>,</span> <span class=n>flat_parameters</span> <span class=o>=</span> <span class=n>ctx</span><span class=o>.</span><span class=n>saved_tensors</span>
</span></span><span class=line><span class=cl>        <span class=n>time_len</span><span class=p>,</span> <span class=n>bs</span><span class=p>,</span> <span class=o>*</span><span class=n>z_shape</span> <span class=o>=</span> <span class=n>z</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>n_dim</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>prod</span><span class=p>(</span><span class=n>z_shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>n_params</span> <span class=o>=</span> <span class=n>flat_parameters</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Dynamics of augmented system to be calculated backwards in time</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># adjoint sensitivity method로 사용하기 위해 새롭게 정의한 solver 함수.</span>
</span></span><span class=line><span class=cl>        <span class=c1># paper에서 말했듯이, 기울기를 저장하지 않고, time_step마다 삭제해준다.</span>
</span></span><span class=line><span class=cl>        <span class=k>def</span> <span class=nf>augmented_dynamics</span><span class=p>(</span><span class=n>aug_z_i</span><span class=p>,</span> <span class=n>t_i</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>            tensors here are temporal slices
</span></span></span><span class=line><span class=cl><span class=s2>            t_i - is tensor with size: bs, 1
</span></span></span><span class=line><span class=cl><span class=s2>            aug_z_i - is tensor with size: bs, n_dim*2 + n_params + 1
</span></span></span><span class=line><span class=cl><span class=s2>            &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>            <span class=n>z_i</span><span class=p>,</span> <span class=n>a</span> <span class=o>=</span> <span class=n>aug_z_i</span><span class=p>[:,</span> <span class=p>:</span><span class=n>n_dim</span><span class=p>],</span> <span class=n>aug_z_i</span><span class=p>[:,</span> <span class=n>n_dim</span><span class=p>:</span><span class=mi>2</span><span class=o>*</span><span class=n>n_dim</span><span class=p>]</span>  <span class=c1># ignore parameters and time</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># Unflatten z and a</span>
</span></span><span class=line><span class=cl>            <span class=n>z_i</span> <span class=o>=</span> <span class=n>z_i</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bs</span><span class=p>,</span> <span class=o>*</span><span class=n>z_shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>a</span> <span class=o>=</span> <span class=n>a</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bs</span><span class=p>,</span> <span class=o>*</span><span class=n>z_shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>set_grad_enabled</span><span class=p>(</span><span class=kc>True</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=n>t_i</span> <span class=o>=</span> <span class=n>t_i</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>(</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>z_i</span> <span class=o>=</span> <span class=n>z_i</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>(</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>func_eval</span><span class=p>,</span> <span class=n>adfdz</span><span class=p>,</span> <span class=n>adfdt</span><span class=p>,</span> <span class=n>adfdp</span> <span class=o>=</span> <span class=n>func</span><span class=o>.</span><span class=n>forward_with_grad</span><span class=p>(</span><span class=n>z_i</span><span class=p>,</span> <span class=n>t_i</span><span class=p>,</span> <span class=n>grad_outputs</span><span class=o>=</span><span class=n>a</span><span class=p>)</span>  <span class=c1># bs, *z_shape</span>
</span></span><span class=line><span class=cl>                <span class=n>adfdz</span> <span class=o>=</span> <span class=n>adfdz</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>z_i</span><span class=p>)</span> <span class=k>if</span> <span class=n>adfdz</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>bs</span><span class=p>,</span> <span class=o>*</span><span class=n>z_shape</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>z_i</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>adfdp</span> <span class=o>=</span> <span class=n>adfdp</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>z_i</span><span class=p>)</span> <span class=k>if</span> <span class=n>adfdp</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>bs</span><span class=p>,</span> <span class=n>n_params</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>z_i</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>adfdt</span> <span class=o>=</span> <span class=n>adfdt</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>z_i</span><span class=p>)</span> <span class=k>if</span> <span class=n>adfdt</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>bs</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>z_i</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># Flatten f and adfdz</span>
</span></span><span class=line><span class=cl>            <span class=n>func_eval</span> <span class=o>=</span> <span class=n>func_eval</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bs</span><span class=p>,</span> <span class=n>n_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>adfdz</span> <span class=o>=</span> <span class=n>adfdz</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bs</span><span class=p>,</span> <span class=n>n_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>func_eval</span><span class=p>,</span> <span class=o>-</span><span class=n>adfdz</span><span class=p>,</span> <span class=o>-</span><span class=n>adfdp</span><span class=p>,</span> <span class=o>-</span><span class=n>adfdt</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>dLdz</span> <span class=o>=</span> <span class=n>dLdz</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>time_len</span><span class=p>,</span> <span class=n>bs</span><span class=p>,</span> <span class=n>n_dim</span><span class=p>)</span>  <span class=c1># flatten dLdz for convenience</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=c1>## Create placeholders for output gradients</span>
</span></span><span class=line><span class=cl>            <span class=c1># Prev computed backwards adjoints to be adjusted by direct gradients</span>
</span></span><span class=line><span class=cl>            <span class=n>adj_z</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>bs</span><span class=p>,</span> <span class=n>n_dim</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>dLdz</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>adj_p</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>bs</span><span class=p>,</span> <span class=n>n_params</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>dLdz</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=c1># In contrast to z and p we need to return gradients for all times</span>
</span></span><span class=line><span class=cl>            <span class=n>adj_t</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>time_len</span><span class=p>,</span> <span class=n>bs</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>dLdz</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>i_t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>time_len</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=n>z_i</span> <span class=o>=</span> <span class=n>z</span><span class=p>[</span><span class=n>i_t</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=n>t_i</span> <span class=o>=</span> <span class=n>t</span><span class=p>[</span><span class=n>i_t</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=n>f_i</span> <span class=o>=</span> <span class=n>func</span><span class=p>(</span><span class=n>z_i</span><span class=p>,</span> <span class=n>t_i</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bs</span><span class=p>,</span> <span class=n>n_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># Compute direct gradients</span>
</span></span><span class=line><span class=cl>                <span class=n>dLdz_i</span> <span class=o>=</span> <span class=n>dLdz</span><span class=p>[</span><span class=n>i_t</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=n>dLdt_i</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>bmm</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>dLdz_i</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>),</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span> <span class=n>f_i</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>))[:,</span> <span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># Adjusting adjoints with direct gradients</span>
</span></span><span class=line><span class=cl>                <span class=n>adj_z</span> <span class=o>+=</span> <span class=n>dLdz_i</span>
</span></span><span class=line><span class=cl>                <span class=n>adj_t</span><span class=p>[</span><span class=n>i_t</span><span class=p>]</span> <span class=o>=</span> <span class=n>adj_t</span><span class=p>[</span><span class=n>i_t</span><span class=p>]</span> <span class=o>-</span> <span class=n>dLdt_i</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># Pack augmented variable</span>
</span></span><span class=line><span class=cl>                <span class=n>aug_z</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>z_i</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bs</span><span class=p>,</span> <span class=n>n_dim</span><span class=p>),</span> <span class=n>adj_z</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>bs</span><span class=p>,</span> <span class=n>n_params</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>z</span><span class=p>),</span> <span class=n>adj_t</span><span class=p>[</span><span class=n>i_t</span><span class=p>]),</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># Solve augmented system backwards</span>
</span></span><span class=line><span class=cl>                <span class=n>aug_ans</span> <span class=o>=</span> <span class=n>ode_solve</span><span class=p>(</span><span class=n>aug_z</span><span class=p>,</span> <span class=n>t_i</span><span class=p>,</span> <span class=n>t</span><span class=p>[</span><span class=n>i_t</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=n>augmented_dynamics</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># Unpack solved backwards augmented system</span>
</span></span><span class=line><span class=cl>                <span class=n>adj_z</span><span class=p>[:]</span> <span class=o>=</span> <span class=n>aug_ans</span><span class=p>[:,</span> <span class=n>n_dim</span><span class=p>:</span><span class=mi>2</span><span class=o>*</span><span class=n>n_dim</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=n>adj_p</span><span class=p>[:]</span> <span class=o>+=</span> <span class=n>aug_ans</span><span class=p>[:,</span> <span class=mi>2</span><span class=o>*</span><span class=n>n_dim</span><span class=p>:</span><span class=mi>2</span><span class=o>*</span><span class=n>n_dim</span> <span class=o>+</span> <span class=n>n_params</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=n>adj_t</span><span class=p>[</span><span class=n>i_t</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=n>aug_ans</span><span class=p>[:,</span> <span class=mi>2</span><span class=o>*</span><span class=n>n_dim</span> <span class=o>+</span> <span class=n>n_params</span><span class=p>:]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=k>del</span> <span class=n>aug_z</span><span class=p>,</span> <span class=n>aug_ans</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1>## Adjust 0 time adjoint with direct gradients</span>
</span></span><span class=line><span class=cl>            <span class=c1># Compute direct gradients</span>
</span></span><span class=line><span class=cl>            <span class=n>dLdz_0</span> <span class=o>=</span> <span class=n>dLdz</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>dLdt_0</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>bmm</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>dLdz_0</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>),</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span> <span class=n>f_i</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>))[:,</span> <span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># Adjust adjoints</span>
</span></span><span class=line><span class=cl>            <span class=n>adj_z</span> <span class=o>+=</span> <span class=n>dLdz_0</span>
</span></span><span class=line><span class=cl>            <span class=n>adj_t</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=n>adj_t</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>-</span> <span class=n>dLdt_0</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>adj_z</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bs</span><span class=p>,</span> <span class=o>*</span><span class=n>z_shape</span><span class=p>),</span> <span class=n>adj_t</span><span class=p>,</span> <span class=n>adj_p</span><span class=p>,</span> <span class=kc>None</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># apply method</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@torch.no_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>init_weights</span><span class=p>(</span><span class=n>m</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>m</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=nb>type</span><span class=p>(</span><span class=n>m</span><span class=p>)</span> <span class=o>==</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>m</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>fill_</span><span class=p>(</span><span class=mf>1.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=n>m</span><span class=o>.</span><span class=n>weight</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>net</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>net</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=n>init_weights</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)





Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>NeuralODE</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>func</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>NeuralODE</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>func</span><span class=p>,</span> <span class=n>ODEF</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>func</span> <span class=o>=</span> <span class=n>func</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>z0</span><span class=p>,</span> <span class=n>t</span><span class=o>=</span><span class=n>Tensor</span><span class=p>([</span><span class=mf>0.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>]),</span> <span class=n>return_whole_sequence</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>t</span> <span class=o>=</span> <span class=n>t</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>z0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>z</span> <span class=o>=</span> <span class=n>ODEAdjoint</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=n>z0</span><span class=p>,</span> <span class=n>t</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>func</span><span class=o>.</span><span class=n>flatten_parameters</span><span class=p>(),</span> <span class=bp>self</span><span class=o>.</span><span class=n>func</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>return_whole_sequence</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>z</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>z</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span></code></pre></div><h3 id=train-utils>train utils<a hidden class=anchor aria-hidden=true href=#train-utils>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>LinearODEF</span><span class=p>(</span><span class=n>ODEF</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>W</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>LinearODEF</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>lin</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>lin</span><span class=o>.</span><span class=n>weight</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>W</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>t</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>lin</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 맞추고자하는 target function</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SpiralFunctionExample</span><span class=p>(</span><span class=n>LinearODEF</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>SpiralFunctionExample</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>Tensor</span><span class=p>([[</span><span class=o>-</span><span class=mf>0.1</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.</span><span class=p>],</span> <span class=p>[</span><span class=mf>1.</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1</span><span class=p>]]))</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># initialize할 training function</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>RandomLinearODEF</span><span class=p>(</span><span class=n>LinearODEF</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>RandomLinearODEF</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>/</span><span class=mf>2.</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 2*2 tensor로 되어있는 레이어를 통해, 새로운 manifold로 mapping해주는 layer.</span>
</span></span><span class=line><span class=cl><span class=c1># 구체적인 값은 밑의 forward를 통해 알 수 있다.</span>
</span></span><span class=line><span class=cl><span class=c1># 둘이 값을 각각 매기고, 빼준다. (trajectory생성하기 위함.(t번째랑 t-1번째 값을 빼면, 그 사이의 간격 출력되는 것을 이용))</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>TestODEF</span><span class=p>(</span><span class=n>ODEF</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>A</span><span class=p>,</span> <span class=n>B</span><span class=p>,</span> <span class=n>x0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>TestODEF</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>A</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>A</span><span class=o>.</span><span class=n>weight</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>A</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>B</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>B</span><span class=o>.</span><span class=n>weight</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>B</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>x0</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>x0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>t</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>xTx0</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>x</span><span class=o>*</span><span class=bp>self</span><span class=o>.</span><span class=n>x0</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>dxdt</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=n>xTx0</span><span class=p>)</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>A</span><span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>x0</span><span class=p>)</span> <span class=o>+</span> <span class=n>torch</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=o>-</span><span class=n>xTx0</span><span class=p>)</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>B</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>x0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>dxdt</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># hidden dimension을 설정해놓고, 2 -&gt; 16 -&gt; 2 로 계산시켜서 출력시키도록 한다.</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>NNODEF</span><span class=p>(</span><span class=n>ODEF</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>in_dim</span><span class=p>,</span> <span class=n>hid_dim</span><span class=p>,</span> <span class=n>time_invariant</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>NNODEF</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>time_invariant</span> <span class=o>=</span> <span class=n>time_invariant</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>time_invariant</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>lin1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>in_dim</span><span class=p>,</span> <span class=n>hid_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>lin1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>in_dim</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span> <span class=n>hid_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>lin2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hid_dim</span><span class=p>,</span> <span class=n>hid_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>lin3</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hid_dim</span><span class=p>,</span> <span class=n>in_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>elu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ELU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>t</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>time_invariant</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>x</span><span class=p>,</span> <span class=n>t</span><span class=p>),</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>h</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>elu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>lin1</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>h</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>elu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>lin2</span><span class=p>(</span><span class=n>h</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>lin3</span><span class=p>(</span><span class=n>h</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>to_np</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>x</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span><span class=o>.</span><span class=n>cpu</span><span class=p>()</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>plot_trajectories</span><span class=p>(</span><span class=n>obs</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>times</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>trajs</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>save</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span> <span class=mi>8</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=n>figsize</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>obs</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>times</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>times</span> <span class=o>=</span> <span class=p>[</span><span class=kc>None</span><span class=p>]</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>obs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>o</span><span class=p>,</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>obs</span><span class=p>,</span> <span class=n>times</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>o</span><span class=p>,</span> <span class=n>t</span> <span class=o>=</span> <span class=n>to_np</span><span class=p>(</span><span class=n>o</span><span class=p>),</span> <span class=n>to_np</span><span class=p>(</span><span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>b_i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>o</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>                <span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>o</span><span class=p>[:,</span> <span class=n>b_i</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>o</span><span class=p>[:,</span> <span class=n>b_i</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>t</span><span class=p>[:,</span> <span class=n>b_i</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>cmap</span><span class=o>=</span><span class=n>cm</span><span class=o>.</span><span class=n>plasma</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>trajs</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>z</span> <span class=ow>in</span> <span class=n>trajs</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>z</span> <span class=o>=</span> <span class=n>to_np</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>z</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>z</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>lw</span><span class=o>=</span><span class=mf>1.5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>save</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=n>save</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>conduct_experiment</span><span class=p>(</span><span class=n>ode_true</span><span class=p>,</span> <span class=n>ode_trained</span><span class=p>,</span> <span class=n>n_steps</span><span class=p>,</span> <span class=n>name</span><span class=p>,</span> <span class=n>plot_freq</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># Create data</span>
</span></span><span class=line><span class=cl>    <span class=c1># hidden state의 initial 값</span>
</span></span><span class=line><span class=cl>    <span class=n>z0</span> <span class=o>=</span> <span class=n>Variable</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>([[</span><span class=mf>0.6</span><span class=p>,</span> <span class=mf>0.3</span><span class=p>]]))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>t_max</span> <span class=o>=</span> <span class=mf>6.29</span><span class=o>*</span><span class=mi>5</span>
</span></span><span class=line><span class=cl>    <span class=n>n_points</span> <span class=o>=</span> <span class=mi>200</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>index_np</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>n_points</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>int</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>index_np</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>hstack</span><span class=p>([</span><span class=n>index_np</span><span class=p>[:,</span> <span class=kc>None</span><span class=p>]])</span>
</span></span><span class=line><span class=cl>    <span class=n>times_np</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>t_max</span><span class=p>,</span> <span class=n>num</span><span class=o>=</span><span class=n>n_points</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>times_np</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>hstack</span><span class=p>([</span><span class=n>times_np</span><span class=p>[:,</span> <span class=kc>None</span><span class=p>]])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>times</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>from_numpy</span><span class=p>(</span><span class=n>times_np</span><span class=p>[:,</span> <span class=p>:,</span> <span class=kc>None</span><span class=p>])</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>z0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>obs</span> <span class=o>=</span> <span class=n>ode_true</span><span class=p>(</span><span class=n>z0</span><span class=p>,</span> <span class=n>times</span><span class=p>,</span> <span class=n>return_whole_sequence</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>obs</span> <span class=o>=</span> <span class=n>obs</span> <span class=o>+</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn_like</span><span class=p>(</span><span class=n>obs</span><span class=p>)</span> <span class=o>*</span> <span class=mf>0.01</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Get trajectory of random timespan</span>
</span></span><span class=line><span class=cl>    <span class=n>min_delta_time</span> <span class=o>=</span> <span class=mf>1.0</span>
</span></span><span class=line><span class=cl>    <span class=n>max_delta_time</span> <span class=o>=</span> <span class=mf>5.0</span>
</span></span><span class=line><span class=cl>    <span class=n>max_points_num</span> <span class=o>=</span> <span class=mi>32</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>create_batch</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=n>t0</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>t_max</span> <span class=o>-</span> <span class=n>max_delta_time</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>t1</span> <span class=o>=</span> <span class=n>t0</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=n>min_delta_time</span><span class=p>,</span> <span class=n>max_delta_time</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>idx</span> <span class=o>=</span> <span class=nb>sorted</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>permutation</span><span class=p>(</span><span class=n>index_np</span><span class=p>[(</span><span class=n>times_np</span> <span class=o>&gt;</span> <span class=n>t0</span><span class=p>)</span> <span class=o>&amp;</span> <span class=p>(</span><span class=n>times_np</span> <span class=o>&lt;</span> <span class=n>t1</span><span class=p>)])[:</span><span class=n>max_points_num</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>obs_</span> <span class=o>=</span> <span class=n>obs</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>ts_</span> <span class=o>=</span> <span class=n>times</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>obs_</span><span class=p>,</span> <span class=n>ts_</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Train Neural ODE</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>ode_trained</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_steps</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>obs_</span><span class=p>,</span> <span class=n>ts_</span> <span class=o>=</span> <span class=n>create_batch</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>z_</span> <span class=o>=</span> <span class=n>ode_trained</span><span class=p>(</span><span class=n>obs_</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>ts_</span><span class=p>,</span> <span class=n>return_whole_sequence</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>mse_loss</span><span class=p>(</span><span class=n>z_</span><span class=p>,</span> <span class=n>obs_</span><span class=o>.</span><span class=n>detach</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>retain_graph</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>i</span> <span class=o>%</span> <span class=n>plot_freq</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>z_p</span> <span class=o>=</span> <span class=n>ode_trained</span><span class=p>(</span><span class=n>z0</span><span class=p>,</span> <span class=n>times</span><span class=p>,</span> <span class=n>return_whole_sequence</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>plot_trajectories</span><span class=p>(</span><span class=n>obs</span><span class=o>=</span><span class=p>[</span><span class=n>obs</span><span class=p>],</span> <span class=n>times</span><span class=o>=</span><span class=p>[</span><span class=n>times</span><span class=p>],</span> <span class=n>trajs</span><span class=o>=</span><span class=p>[</span><span class=n>z_p</span><span class=p>],</span> <span class=n>save</span><span class=o>=</span><span class=sa>f</span><span class=s2>&#34;./assets/imgs/</span><span class=si>{</span><span class=n>name</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s2>.png&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>clear_output</span><span class=p>(</span><span class=n>wait</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=application--concepts>application : concepts<a hidden class=anchor aria-hidden=true href=#application--concepts>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>ode_true</span> <span class=o>=</span> <span class=n>NeuralODE</span><span class=p>(</span><span class=n>SpiralFunctionExample</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>ode_trained</span> <span class=o>=</span> <span class=n>NeuralODE</span><span class=p>(</span><span class=n>RandomLinearODEF</span><span class=p>())</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>conduct_experiment</span><span class=p>(</span><span class=n>ode_true</span><span class=p>,</span> <span class=n>ode_trained</span><span class=p>,</span> <span class=mi>500</span><span class=p>,</span> <span class=s2>&#34;linear&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><img loading=lazy src=output_52_0.png alt=png></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>func</span> <span class=o>=</span> <span class=n>TestODEF</span><span class=p>(</span><span class=n>Tensor</span><span class=p>([[</span><span class=o>-</span><span class=mf>0.1</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5</span><span class=p>],</span> <span class=p>[</span><span class=mf>0.5</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1</span><span class=p>]]),</span> <span class=n>Tensor</span><span class=p>([[</span><span class=mf>0.2</span><span class=p>,</span> <span class=mf>1.</span><span class=p>],</span> <span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>]]),</span> <span class=n>Tensor</span><span class=p>([[</span><span class=o>-</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>]]))</span>
</span></span><span class=line><span class=cl><span class=n>ode_true</span> <span class=o>=</span> <span class=n>NeuralODE</span><span class=p>(</span><span class=n>func</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>func</span> <span class=o>=</span> <span class=n>NNODEF</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=n>time_invariant</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ode_trained</span> <span class=o>=</span> <span class=n>NeuralODE</span><span class=p>(</span><span class=n>func</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>conduct_experiment</span><span class=p>(</span><span class=n>ode_true</span><span class=p>,</span> <span class=n>ode_trained</span><span class=p>,</span> <span class=mi>3000</span><span class=p>,</span> <span class=s2>&#34;comp&#34;</span><span class=p>,</span> <span class=n>plot_freq</span><span class=o>=</span><span class=mi>30</span><span class=p>)</span>
</span></span></code></pre></div><p><img loading=lazy src=output_54_0.png alt=png></p><h3 id=application--resnet>application : ResNet<a hidden class=anchor aria-hidden=true href=#application--resnet>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>norm</span><span class=p>(</span><span class=n>dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=n>dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 크기 안변하는 conv연산.</span>
</span></span><span class=line><span class=cl><span class=c1># receptive field만 상승한다.</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>conv3x3</span><span class=p>(</span><span class=n>in_feats</span><span class=p>,</span> <span class=n>out_feats</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>in_feats</span><span class=p>,</span> <span class=n>out_feats</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=n>stride</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>add_time</span><span class=p>(</span><span class=n>in_tensor</span><span class=p>,</span> <span class=n>t</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>bs</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>w</span><span class=p>,</span> <span class=n>h</span> <span class=o>=</span> <span class=n>in_tensor</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>in_tensor</span><span class=p>,</span> <span class=n>t</span><span class=o>.</span><span class=n>expand</span><span class=p>(</span><span class=n>bs</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>w</span><span class=p>,</span> <span class=n>h</span><span class=p>)),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>ConvODEF</span><span class=p>(</span><span class=n>ODEF</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>ConvODEF</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 중간 feature dimension 1 줄여놓고 연산한다.(왜 늘리지 않고 줄이지)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span> <span class=o>=</span> <span class=n>conv3x3</span><span class=p>(</span><span class=n>dim</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=n>dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span> <span class=o>=</span> <span class=n>norm</span><span class=p>(</span><span class=n>dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>conv2</span> <span class=o>=</span> <span class=n>conv3x3</span><span class=p>(</span><span class=n>dim</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=n>dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span> <span class=o>=</span> <span class=n>norm</span><span class=p>(</span><span class=n>dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>t</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>xt</span> <span class=o>=</span> <span class=n>add_time</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>h</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>conv1</span><span class=p>(</span><span class=n>xt</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        <span class=n>ht</span> <span class=o>=</span> <span class=n>add_time</span><span class=p>(</span><span class=n>h</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>dxdt</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>conv2</span><span class=p>(</span><span class=n>ht</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>dxdt</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>ContinuousNeuralMNISTClassifier</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>ode</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>ContinuousNeuralMNISTClassifier</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>downsampling</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>norm</span><span class=p>(</span><span class=mi>64</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>norm</span><span class=p>(</span><span class=mi>64</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>feature</span> <span class=o>=</span> <span class=n>ode</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm</span> <span class=o>=</span> <span class=n>norm</span><span class=p>(</span><span class=mi>64</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>avg_pool</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>AdaptiveAvgPool2d</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>downsampling</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>feature</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>avg_pool</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>shape</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>prod</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>:]))</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>func</span> <span class=o>=</span> <span class=n>ConvODEF</span><span class=p>(</span><span class=mi>64</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ode</span> <span class=o>=</span> <span class=n>NeuralODE</span><span class=p>(</span><span class=n>func</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>ContinuousNeuralMNISTClassifier</span><span class=p>(</span><span class=n>ode</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>use_cuda</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>func</span>
</span></span></code></pre></div><pre><code>ConvODEF(
  (conv1): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>ode</span>
</span></span></code></pre></div><pre><code>NeuralODE(
  (func): ConvODEF(
    (conv1): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>model</span>
</span></span></code></pre></div><pre><code>ContinuousNeuralMNISTClassifier(
  (downsampling): Sequential(
    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  )
  (feature): NeuralODE(
    (func): ConvODEF(
      (conv1): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torchvision</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>img_std</span> <span class=o>=</span> <span class=mf>0.3081</span>
</span></span><span class=line><span class=cl><span class=n>img_mean</span> <span class=o>=</span> <span class=mf>0.1307</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>32</span>
</span></span><span class=line><span class=cl><span class=n>train_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>torchvision</span><span class=o>.</span><span class=n>datasets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=s2>&#34;.data/mnist&#34;</span><span class=p>,</span> <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                             <span class=n>transform</span><span class=o>=</span><span class=n>torchvision</span><span class=o>.</span><span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
</span></span><span class=line><span class=cl>                                 <span class=n>torchvision</span><span class=o>.</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>                                 <span class=n>torchvision</span><span class=o>.</span><span class=n>transforms</span><span class=o>.</span><span class=n>Normalize</span><span class=p>((</span><span class=n>img_mean</span><span class=p>,),</span> <span class=p>(</span><span class=n>img_std</span><span class=p>,))</span>
</span></span><span class=line><span class=cl>                             <span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>test_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>torchvision</span><span class=o>.</span><span class=n>datasets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=s2>&#34;.data/mnist&#34;</span><span class=p>,</span> <span class=n>train</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                             <span class=n>transform</span><span class=o>=</span><span class=n>torchvision</span><span class=o>.</span><span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
</span></span><span class=line><span class=cl>                                 <span class=n>torchvision</span><span class=o>.</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>                                 <span class=n>torchvision</span><span class=o>.</span><span class=n>transforms</span><span class=o>.</span><span class=n>Normalize</span><span class=p>((</span><span class=n>img_mean</span><span class=p>,),</span> <span class=p>(</span><span class=n>img_std</span><span class=p>,))</span>
</span></span><span class=line><span class=cl>                             <span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><pre><code>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to .data/mnist/MNIST/raw/train-images-idx3-ubyte.gz


100%|██████████| 9912422/9912422 [00:00&lt;00:00, 104652221.37it/s]


Extracting .data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to .data/mnist/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to .data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz


100%|██████████| 28881/28881 [00:00&lt;00:00, 33352338.61it/s]


Extracting .data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to .data/mnist/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to .data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz


100%|██████████| 1648877/1648877 [00:00&lt;00:00, 28561776.33it/s]


Extracting .data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to .data/mnist/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to .data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz


100%|██████████| 4542/4542 [00:00&lt;00:00, 3722987.84it/s]


Extracting .data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to .data/mnist/MNIST/raw
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>train</span><span class=p>(</span><span class=n>epoch</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>num_items</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=n>train_losses</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Training Epoch </span><span class=si>{</span><span class=n>epoch</span><span class=si>}</span><span class=s2>...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>batch_idx</span><span class=p>,</span> <span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span> <span class=ow>in</span> <span class=n>tqdm</span><span class=p>(</span><span class=nb>enumerate</span><span class=p>(</span><span class=n>train_loader</span><span class=p>),</span> <span class=n>total</span><span class=o>=</span><span class=nb>len</span><span class=p>(</span><span class=n>train_loader</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>use_cuda</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>data</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>target</span> <span class=o>=</span> <span class=n>target</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>train_losses</span> <span class=o>+=</span> <span class=p>[</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()]</span>
</span></span><span class=line><span class=cl>        <span class=n>num_items</span> <span class=o>+=</span> <span class=n>data</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Train loss: </span><span class=si>{:.5f}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>train_losses</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>train_losses</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>test</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>accuracy</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>    <span class=n>num_items</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Testing...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>batch_idx</span><span class=p>,</span> <span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span> <span class=ow>in</span> <span class=n>tqdm</span><span class=p>(</span><span class=nb>enumerate</span><span class=p>(</span><span class=n>test_loader</span><span class=p>),</span>  <span class=n>total</span><span class=o>=</span><span class=nb>len</span><span class=p>(</span><span class=n>test_loader</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>use_cuda</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>data</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                <span class=n>target</span> <span class=o>=</span> <span class=n>target</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>accuracy</span> <span class=o>+=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span> <span class=o>==</span> <span class=n>target</span><span class=p>)</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>num_items</span> <span class=o>+=</span> <span class=n>data</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>accuracy</span> <span class=o>=</span> <span class=n>accuracy</span> <span class=o>*</span> <span class=mi>100</span> <span class=o>/</span> <span class=n>num_items</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Test Accuracy: </span><span class=si>{:.3f}</span><span class=s2>%&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>accuracy</span><span class=p>))</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>n_epochs</span> <span class=o>=</span> <span class=mi>5</span>
</span></span><span class=line><span class=cl><span class=n>test</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>train_losses</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>n_epochs</span> <span class=o>+</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>train_losses</span> <span class=o>+=</span> <span class=n>train</span><span class=p>(</span><span class=n>epoch</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>test</span><span class=p>()</span>
</span></span></code></pre></div><pre><code>Testing...


&lt;ipython-input-24-21ca1da03176&gt;:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0
Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`
  for batch_idx, (data, target) in tqdm(enumerate(test_loader),  total=len(test_loader)):



  0%|          | 0/79 [00:00&lt;?, ?it/s]


Test Accuracy: 13.320%
Training Epoch 1...


&lt;ipython-input-23-128259ae99dc&gt;:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0
Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`
  for batch_idx, (data, target) in tqdm(enumerate(train_loader), total=len(train_loader)):



  0%|          | 0/1875 [00:00&lt;?, ?it/s]


Train loss: 0.15641
Testing...



  0%|          | 0/79 [00:00&lt;?, ?it/s]


Test Accuracy: 98.370%
Training Epoch 2...



  0%|          | 0/1875 [00:00&lt;?, ?it/s]


Train loss: 0.04976
Testing...



  0%|          | 0/79 [00:00&lt;?, ?it/s]


Test Accuracy: 98.890%
Training Epoch 3...



  0%|          | 0/1875 [00:00&lt;?, ?it/s]


Train loss: 0.03789
Testing...



  0%|          | 0/79 [00:00&lt;?, ?it/s]


Test Accuracy: 98.450%
Training Epoch 4...



  0%|          | 0/1875 [00:00&lt;?, ?it/s]


Train loss: 0.02935
Testing...



  0%|          | 0/79 [00:00&lt;?, ?it/s]


Test Accuracy: 98.860%
Training Epoch 5...



  0%|          | 0/1875 [00:00&lt;?, ?it/s]


Train loss: 0.02512
Testing...



  0%|          | 0/79 [00:00&lt;?, ?it/s]


Test Accuracy: 98.970%
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>9</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>history</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>({</span><span class=s2>&#34;loss&#34;</span><span class=p>:</span> <span class=n>train_losses</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=n>history</span><span class=p>[</span><span class=s2>&#34;cum_data&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>history</span><span class=o>.</span><span class=n>index</span> <span class=o>*</span> <span class=n>batch_size</span>
</span></span><span class=line><span class=cl><span class=n>history</span><span class=p>[</span><span class=s2>&#34;smooth_loss&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>history</span><span class=o>.</span><span class=n>loss</span><span class=o>.</span><span class=n>ewm</span><span class=p>(</span><span class=n>halflife</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>history</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>x</span><span class=o>=</span><span class=s2>&#34;cum_data&#34;</span><span class=p>,</span> <span class=n>y</span><span class=o>=</span><span class=s2>&#34;smooth_loss&#34;</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>5</span><span class=p>),</span> <span class=n>title</span><span class=o>=</span><span class=s2>&#34;train error&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>&lt;Axes: title={'center': 'train error'}, xlabel='cum_data'&gt;




&lt;Figure size 900x500 with 0 Axes&gt;
</code></pre><p><img loading=lazy src=output_68_2.png alt=png></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>func_r</span> <span class=o>=</span> <span class=n>ConvODEF</span><span class=p>(</span><span class=mi>64</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ode_r</span> <span class=o>=</span> <span class=n>NeuralODE</span><span class=p>(</span><span class=n>func</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model_r</span> <span class=o>=</span> <span class=n>ContinuousNeuralMNISTClassifier</span><span class=p>(</span><span class=n>ode</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>use_cuda</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>model_r</span> <span class=o>=</span> <span class=n>model_r</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://dobe0715.github.io/posts/more-diffusion/><span class=title>« Prev</span><br><span>Improved DDPM, Diffusion models beat GANs</span></a>
<a class=next href=https://dobe0715.github.io/posts/score-based-model/><span class=title>Next »</span><br><span>Score Based Models 정리본</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Neural Ordinary Differential Equations review on twitter" href="https://twitter.com/intent/tweet/?text=Neural%20Ordinary%20Differential%20Equations%20review&amp;url=https%3a%2f%2fdobe0715.github.io%2fposts%2fneural-ode%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Neural Ordinary Differential Equations review on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdobe0715.github.io%2fposts%2fneural-ode%2f&amp;title=Neural%20Ordinary%20Differential%20Equations%20review&amp;summary=Neural%20Ordinary%20Differential%20Equations%20review&amp;source=https%3a%2f%2fdobe0715.github.io%2fposts%2fneural-ode%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Neural Ordinary Differential Equations review on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdobe0715.github.io%2fposts%2fneural-ode%2f&title=Neural%20Ordinary%20Differential%20Equations%20review"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Neural Ordinary Differential Equations review on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdobe0715.github.io%2fposts%2fneural-ode%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Neural Ordinary Differential Equations review on whatsapp" href="https://api.whatsapp.com/send?text=Neural%20Ordinary%20Differential%20Equations%20review%20-%20https%3a%2f%2fdobe0715.github.io%2fposts%2fneural-ode%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Neural Ordinary Differential Equations review on telegram" href="https://telegram.me/share/url?text=Neural%20Ordinary%20Differential%20Equations%20review&amp;url=https%3a%2f%2fdobe0715.github.io%2fposts%2fneural-ode%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Neural Ordinary Differential Equations review on ycombinator" href="https://news.ycombinator.com/submitlink?t=Neural%20Ordinary%20Differential%20Equations%20review&u=https%3a%2f%2fdobe0715.github.io%2fposts%2fneural-ode%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://dobe0715.github.io>블로그 홈</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>CNN 모델 정리 | 블로그 홈</title><meta name=keywords content><meta name=description content="Inception Net(GoogLeNet) https://arxiv.org/pdf/1409.4842.pdf 참고 블로그 https://phil-baek.tistory.com/entry/3-GoogLeNet-Going-deeper-with-convolutions-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0 모델 나오게 된 배경 DNN의 성능을 향상시키는 방법 가장 직접적인 방법은 size를 늘리는 것에서 시작
구체적으로, depth/width를 증가시키면 되는데 이 때 두가지 문제점이 있다.
파라미터 수의 증가
파라미터가 늘어날 경우, 이에 비하여 학습데이터가 부족하면 오버피팅에 일어나기 쉽다.
컴퓨팅 자원 사용량의 증가
만약, 두개의 conv layer가 연속으로 있는 상황에서, 거의 대부분의 가중치가 0이면, 계산에 있어서 버려지는 자원이 많아진다.
but, 컴퓨팅자원은 한정적이므로 효율적으로 분배하는 것이 중요"><meta name=author content="Me"><link rel=canonical href=https://dobe0715.github.io/posts/classification-cnn-models/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="CNN 모델 정리"><meta property="og:description" content="Inception Net(GoogLeNet) https://arxiv.org/pdf/1409.4842.pdf 참고 블로그 https://phil-baek.tistory.com/entry/3-GoogLeNet-Going-deeper-with-convolutions-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0 모델 나오게 된 배경 DNN의 성능을 향상시키는 방법 가장 직접적인 방법은 size를 늘리는 것에서 시작
구체적으로, depth/width를 증가시키면 되는데 이 때 두가지 문제점이 있다.
파라미터 수의 증가
파라미터가 늘어날 경우, 이에 비하여 학습데이터가 부족하면 오버피팅에 일어나기 쉽다.
컴퓨팅 자원 사용량의 증가
만약, 두개의 conv layer가 연속으로 있는 상황에서, 거의 대부분의 가중치가 0이면, 계산에 있어서 버려지는 자원이 많아진다.
but, 컴퓨팅자원은 한정적이므로 효율적으로 분배하는 것이 중요"><meta property="og:type" content="article"><meta property="og:url" content="https://dobe0715.github.io/posts/classification-cnn-models/"><meta property="og:image" content="https://dobe0715.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dobe0715.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="CNN 모델 정리"><meta name=twitter:description content="Inception Net(GoogLeNet) https://arxiv.org/pdf/1409.4842.pdf 참고 블로그 https://phil-baek.tistory.com/entry/3-GoogLeNet-Going-deeper-with-convolutions-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0 모델 나오게 된 배경 DNN의 성능을 향상시키는 방법 가장 직접적인 방법은 size를 늘리는 것에서 시작
구체적으로, depth/width를 증가시키면 되는데 이 때 두가지 문제점이 있다.
파라미터 수의 증가
파라미터가 늘어날 경우, 이에 비하여 학습데이터가 부족하면 오버피팅에 일어나기 쉽다.
컴퓨팅 자원 사용량의 증가
만약, 두개의 conv layer가 연속으로 있는 상황에서, 거의 대부분의 가중치가 0이면, 계산에 있어서 버려지는 자원이 많아진다.
but, 컴퓨팅자원은 한정적이므로 효율적으로 분배하는 것이 중요"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://dobe0715.github.io/posts/"},{"@type":"ListItem","position":3,"name":"CNN 모델 정리","item":"https://dobe0715.github.io/posts/classification-cnn-models/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"CNN 모델 정리","name":"CNN 모델 정리","description":"Inception Net(GoogLeNet) https://arxiv.org/pdf/1409.4842.pdf 참고 블로그 https://phil-baek.tistory.com/entry/3-GoogLeNet-Going-deeper-with-convolutions-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0 모델 나오게 된 배경 DNN의 성능을 향상시키는 방법 가장 직접적인 방법은 size를 늘리는 것에서 시작\n구체적으로, depth/width를 증가시키면 되는데 이 때 두가지 문제점이 있다.\n파라미터 수의 증가\n파라미터가 늘어날 경우, 이에 비하여 학습데이터가 부족하면 오버피팅에 일어나기 쉽다.\n컴퓨팅 자원 사용량의 증가\n만약, 두개의 conv layer가 연속으로 있는 상황에서, 거의 대부분의 가중치가 0이면, 계산에 있어서 버려지는 자원이 많아진다.\nbut, 컴퓨팅자원은 한정적이므로 효율적으로 분배하는 것이 중요","keywords":[],"articleBody":"Inception Net(GoogLeNet) https://arxiv.org/pdf/1409.4842.pdf 참고 블로그 https://phil-baek.tistory.com/entry/3-GoogLeNet-Going-deeper-with-convolutions-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0 모델 나오게 된 배경 DNN의 성능을 향상시키는 방법 가장 직접적인 방법은 size를 늘리는 것에서 시작\n구체적으로, depth/width를 증가시키면 되는데 이 때 두가지 문제점이 있다.\n파라미터 수의 증가\n파라미터가 늘어날 경우, 이에 비하여 학습데이터가 부족하면 오버피팅에 일어나기 쉽다.\n컴퓨팅 자원 사용량의 증가\n만약, 두개의 conv layer가 연속으로 있는 상황에서, 거의 대부분의 가중치가 0이면, 계산에 있어서 버려지는 자원이 많아진다.\nbut, 컴퓨팅자원은 한정적이므로 효율적으로 분배하는 것이 중요\n해결방법으론, dense한 Fully Coneected에서, Sparsely Connected(연관있는 애들 끼리) 구조로 바꾸는 것.\n(대부분의 경량화는 결국 이것이 목표인 듯 하다.)\n왼쪽이 sparsely, 오른쪽이 fully\nbut, sparsely한 구조를 사용하려면 더 섬세한 작업이 필요하고, 이러한 것은 컴퓨팅의 병렬 작업에 그리 적합하지 않다.\n그래서, Sparse한 효과를 내는 비슷한 구조를 만들어내는 것이 목표이다\n모델 구조 filter의 size에 따라 잡아낼 수 있는 local한 의미가 다르기 때문에, 각각이 반응하도록 하고, 이를 통해 sparse한 효과를 낼 수 있다.\n그리고, 병목현상 및 연산량을 줄이기 위해서 각각의 3x3, 5x5 전에 1x1 layer를 추가해주었다.\nmax pooling 이후에 1x1한 것은 previous layer의 특징을 보존시키기 위해서라고 생각이 든다.\n어쨋든, GoogLeNet은 이러한 inception module을 여러개 쌓아서 만든 구조이다. 이후의 detail한 부분은 읽어보면 될정도\nResNet 논문\nresnet : https://arxiv.org/pdf/1512.03385.pdf resnet ensemble : https://arxiv.org/pdf/1605.06431.pdf 참고 블로그\n투빅스 : https://tobigs.gitbook.io/tobigs/deep-learning/computer-vision/resnet-deep-residual-learning-for-image-recognition https://blog.kubwa.co.kr/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-deep-residual-learning-for-image-recognition-2015-%EA%B0%84%EB%8B%A8%ED%95%9C-%EC%8B%A4%EC%8A%B5%EC%BD%94%EB%93%9C-a55cb68981b1 참고 유튜브\n혁펜하임 : https://www.youtube.com/watch?v=Fypk0ec32BU\u0026list=PL_iJu012NOxd_lWvBM8RfXeB7nPYDwafn\u0026index=14\u0026t=0s 모델 나오게 된 배경 depth가 단순히 깊어지기만 해서 모델이 좋아지지는 않는다.(순전히 오버피팅이 아닌게, 훈련 셋의 정확도 마저도 낮다.)\n모델 구조 아래의 block을 마구마구 쌓은 구조를 갖는다.\n논문에서 말하고자하는 바는 다음과 같다.\n우리가 얻길 원하는 이상적인 함수 $H(x)$가 있다고하면, $F(x)$를 다음과 같이 정의한다.\n$F(x) := H(x) - x$\n이 때, 이상적인 함수가 $H(x) \\simeq x$를 만족하면 좋겠다고 이야기함.\nwhy? 아마도, layer가 깊어지면 깊어질 수록 값들 변화하는 횟수가 많아지는데, 그 변동 폭이 너무 크다면 오버피팅의 위험도 있고, 학습하는 것에 어려움이 있을 것이다.\n즉, identity 함수와 비슷해지도록 학습하는 것이 좋다..!!\n가정 : non linear layer들이 점진적으로 복잡한 함수로 접근해나갈 것이다.($H(x) \\simeq x$)\n그렇다면, 이러한 상황에서 residual connection을 하면 왜 학습이 잘되는가?\nnon linear layer를 포함한 함수 $F(x)$를 가지고, identity mapping을 하는 것보다, zero mapping을 하도록 optimizing하는 것이 더 쉽기 때문이다.\n선형 레이어들의 경우에는 초깃값을 단위행렬 이런거로 맞추면 가능하겠지만, 비선형 레이어의 경우에는 초깃값을 어떻게 설정할지도 의문이고, 그럴 바에야 어차피 초깃값 0 근처로 잡아버리고 그 근처로 optiminze하는 것이 훨씬 쉽다.\nResidual Learning!\n기존의 plain network의 경우에는 층이 깊어졌을 때, error가 증가하는 모습을 볼 수 있다.\n하지만, resnet의 경우에는 층이 깊어지면 error가 내려간다.\nResNet의 다른 해석(앙상블) (2016)Residual Networks Behave Like Ensembles of Relatively Shallow Networks 이처럼 앙상블로 볼 수 있다는 이야기와, layer들을 몇개 제거해봤을 때,\nresnet은 성능에 큰 차이가 없으나 vggnet은 거의 제대로된 기능을 하지 못한다.\nMobile Net 논문 https://arxiv.org/pdf/1704.04861.pdf 참고 블로그 https://deep-learning-study.tistory.com/532 https://velog.io/@twinjuy/MobileNet-%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0 모델 나오게 된 배경 mobile, embedded vision network에 적용하기 위해 모델을 경량화 시켜야 한다.\n즉, 저용량 메모리 환경에서 accuracy와 latency(깊이)의 균형을 잘 찾기 위함\n모델 구조 Depthwise Separable Convolution\n해당 모델은 Depthwise Convolution을 거치고, Pointwise Convolution을 결합한 것이다.\n(a)를 보면, 일반적인 conv filter이다.\nkernel size를 $D_K \\cdot D_K$이라하고, input channel을 M, output channel을 N이라하고 결과 feature map size를 $D_F \\cdot D_F$라 하면 다음의 컴퓨팅 cost가 나온다.\n$$D_K \\cdot D_K \\cdot M \\cdot N \\cdot D_F \\cdot D_F$$\n(b)는 depthwise conv filter이다.\n$D_K \\cdot D_K \\cdot 1$size의 filter가 이전 layer의 각각의 channel에 conv 연산을 수행해준다. 따라서 컴퓨팅 cost는 다음과 같다. $$D_K \\cdot D_K \\cdot M \\cdot D_F \\cdot D_F$$\n(c)는 pointwise conv filter이다.\n이전 layer에 대해서 M의 input channel, N의 output channel을 얻도록 해주려 한다. 또한 feature map의 크기가 $D_F \\cdot D_F$로 나오고 1x1으로 곱해주므로 다음의 컴퓨팅 cost를 갖는다. $$M \\cdot N \\cdot D_F \\cdot D_F$$\n최종적으로 classic한 conv layer와의 computation cost를 비교해보면 다음과 같다.\n$$\\cfrac{D_K \\cdot D_K \\cdot M \\cdot D_F \\cdot D_F + M \\cdot N \\cdot D_F \\cdot D_F}{D_K \\cdot D_K \\cdot M \\cdot N \\cdot D_F \\cdot D_F} = \\cfrac{1}{N} + \\cfrac{1}{D_K^2}$$\n3x3 kernel을 쓰게되면, 기존에 비해 8~9배정도 computation을 덜 하게 된다고 한다.\n여기에 추가적으로 width/resolution Multiplier를 적용해 더욱 경량화를 시킬 수 있었다. 최종적인 식은 다음과 같다. $$D_K \\cdot D_K \\cdot \\alpha M \\cdot \\rho D_F \\cdot \\rho D_F + \\alpha M \\cdot \\alpha N \\cdot \\rho D_F \\cdot \\rho D_F$$\n$\\alpha$는 0.25, 0.5, 0.75, 1 으로 width multiplier\n$\\rho$는 resolution multiplier\n각각 $\\alpha^2, \\rho^2$만큼의 computational cost를 절약시켜줬다.\n정확도는 기존의 GoogLeNet과 VGG16과 비교해 크게 차이 없었고, 연산량은 확연히 줄어들었음을 알 수 있다.\nSeNet 논문 https://arxiv.org/pdf/1709.01507.pdf 참고 블로그 https://inhovation97.tistory.com/48 https://deep-learning-study.tistory.com/539 모델 나오게 된 배경 feature map의 채널마다의 relationship을 이용하면 성능을 끌어올릴 수 있지 않을까?\n모델 구조 Squeeze \u0026 Excitation\n1. Squeeze(Global Information Embedding)\n기존의 input $X$에 $F_{tr}$라는 연산을 거쳐 $H \\cdot W \\cdot C$의 형태를 가진 feature map($U$)에 대해서 각각의 channel에 대해 squeeze(global average pooling) 작업을 해준다.\n수식으로 표현하면 다음과 같다. $$z_c = F_{sq}(u_c) = \\cfrac{1}{H * W}\\sum_{i=1}^H \\sum_{j=1}^W{u_c(i, j)}$$\n2. Excitation(Adaptive Recalibration)\n앞의 squeeze에서 모아놓은 정보에다가 채널마다의 정보 의존성을 넣는다. 두가지 조건을 만족해야 하는데, flexible해야하고 non mutually exclusive해야 한다.(유연 + 상호의존적) 그러기 위해, fc-relu-fc-sigmoid 순으로 layer를 거쳐준다.\n이 때, reduction ratio(r)만큼으로 bottle neck크기를 설정한다.(16으로 제안)\n수식으로 표현하면 다음과 같다.\n$$s = F_{ex}(z, W) = \\sigma(g(z, W)) = \\sigma(W_2\\delta(W_1z))$$\n마지막으로 원래의 feature map $u_c$에 곱해주어 결론적으로 각각의 feature마다의 중요도? 만큼씩 scaling된다고 볼 수 있다.\n$$\\tilde{x}c = F{scale}(u_c, s_c) = s_cu_c$$\n즉, 짜내서 feature 정보를 embedding하고 반응성에 따라 재교정한다 =\u003e Squeeze \u0026 Excitation\nthink\n어떻게 보면, feature map에 대해서 self-attention하는 것과 비슷한 효과를 내는 것같다.(channel끼리 내적하는 대신에 layer들을 통해 channel끼리의 관계를 파악한다)\nEfficient Net 논문 https://arxiv.org/pdf/1905.11946.pdf 참고 블로그 https://blog.kubwa.co.kr/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-efficientnet-rethinking-model-scaling-for-convolutional-neural-networks-2019-%EA%B0%84%EB%8B%A8%ED%95%9C-%EC%8B%A4%EC%8A%B5%EC%BD%94%EB%93%9C-cbe0e9963ffc 모델 나오게 된 배경 기존에 나온 모델의 size가 과연 무엇이 최적일까\n해당 논문에서 크게 3가지 scaling에 초점을 맞춘다.\ndepth(모델 깊이)\n기존에 ResNet같은 경우 ResNet-18 ~ ResNet-1000 까지 많은 층을 쌓으며 모델의 성능을 끌어올렸음 을 알 수 있다. 그런데, 성능을 보았을 때, 101층짜리와 1000층짜리의 정확도에서 큰 차이가 없었다.\nwidth(채널 수)\n주로 작은 모델에 대해 사용하는 scaling인데, 마찬가지로 3.8과 5.0을 비교해보면 큰 차이가 없다\nresolution(이미지해상도)\n마찬가지로 해상도가 높아질수록 성능은 올라가지만 그 정도가 점점 감소함을 알 수 있다.\n모델 구조 Basline network이 되는 EfficientNet-B0부터 시작하여 오른쪽의 수식에 따라 d, w, r을 바꿔가며 모델을 실험해나간다.\n이 때, MBConv는 Mobile block을 의미하고\n각각의 layer마다 SE 작업을 해주었다.\n3가지 하이퍼파라미터에 대해 기준을 두고 grid search를 한다. 그 값을 기준으로 모델의 전체적인 크기를 늘린다. 즉, 최적의 모양을 먼저잡고 그것을 동일한 비율로 잡아 늘렸다고 보면된다.\n(물론, 각각의 3가지 scale이 독립적이지 않아서 이렇게 늘리는 것이 최선은 아니겠지만, test하는 것에 시간이 너무 오래 소요되어서 차선으로 이 방법을 택한 듯 하다.)\n기존의 비슷한 성능을 내는 network들과 비교했을 때 parameter수와 FLOPs(계산량)수가 현저히 낮고, B7 모델은 sota를 달성했다.\n","wordCount":"1021","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://dobe0715.github.io/posts/classification-cnn-models/"},"publisher":{"@type":"Organization","name":"블로그 홈","logo":{"@type":"ImageObject","url":"https://dobe0715.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dobe0715.github.io accesskey=h title="Home (Alt + H)"><img src=https://dobe0715.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dobe0715.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://dobe0715.github.io/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dobe0715.github.io>Home</a>&nbsp;»&nbsp;<a href=https://dobe0715.github.io/posts/>Posts</a></div><h1 class=post-title>CNN 모델 정리</h1><div class=post-meta>5 min&nbsp;·&nbsp;1021 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/Classification%20CNN%20Models.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><h1 id=inception-netgooglenet>Inception Net(GoogLeNet)<a hidden class=anchor aria-hidden=true href=#inception-netgooglenet>#</a></h1><ul><li><a href=https://arxiv.org/pdf/1409.4842.pdf>https://arxiv.org/pdf/1409.4842.pdf</a></li><li>참고 블로그<ul><li><a href=https://phil-baek.tistory.com/entry/3-GoogLeNet-Going-deeper-with-convolutions-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0>https://phil-baek.tistory.com/entry/3-GoogLeNet-Going-deeper-with-convolutions-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0</a></li></ul></li></ul><h2 id=모델-나오게-된-배경>모델 나오게 된 배경<a hidden class=anchor aria-hidden=true href=#모델-나오게-된-배경>#</a></h2><p><strong>DNN의 성능을 향상시키는 방법 가장 직접적인 방법은 size를 늘리는 것에서 시작</strong></p><p>구체적으로, depth/width를 증가시키면 되는데 이 때 두가지 문제점이 있다.</p><ol><li><p>파라미터 수의 증가<br>파라미터가 늘어날 경우, 이에 비하여 학습데이터가 부족하면 오버피팅에 일어나기 쉽다.</p></li><li><p>컴퓨팅 자원 사용량의 증가<br>만약, 두개의 conv layer가 연속으로 있는 상황에서, 거의 대부분의 가중치가 0이면, 계산에 있어서 버려지는 자원이 많아진다.<br><strong>but, 컴퓨팅자원은 한정적이므로 효율적으로 분배하는 것이 중요</strong></p></li></ol><p><strong>해결방법으론, dense한 Fully Coneected에서, Sparsely Connected(연관있는 애들 끼리) 구조로 바꾸는 것.</strong><br>(대부분의 경량화는 결국 이것이 목표인 듯 하다.)<br></p><p>왼쪽이 sparsely, 오른쪽이 fully</p><p><strong>but, sparsely한 구조를 사용하려면 더 섬세한 작업이 필요하고, 이러한 것은 컴퓨팅의 병렬 작업에 그리 적합하지 않다.</strong></p><p>그래서, Sparse한 효과를 내는 비슷한 구조를 만들어내는 것이 목표이다</p><h2 id=모델-구조>모델 구조<a hidden class=anchor aria-hidden=true href=#모델-구조>#</a></h2><p>filter의 size에 따라 잡아낼 수 있는 local한 의미가 다르기 때문에, 각각이 반응하도록 하고, 이를 통해 sparse한 효과를 낼 수 있다.</p><p>그리고, 병목현상 및 연산량을 줄이기 위해서 각각의 3x3, 5x5 전에 1x1 layer를 추가해주었다.</p><p>max pooling 이후에 1x1한 것은 previous layer의 특징을 보존시키기 위해서라고 생각이 든다.</p><p>어쨋든, GoogLeNet은 이러한 inception module을 여러개 쌓아서 만든 구조이다. 이후의 detail한 부분은 읽어보면 될정도</p><h1 id=resnet>ResNet<a hidden class=anchor aria-hidden=true href=#resnet>#</a></h1><ul><li><p>논문</p><ul><li>resnet : <a href=https://arxiv.org/pdf/1512.03385.pdf>https://arxiv.org/pdf/1512.03385.pdf</a></li><li>resnet ensemble : <a href=https://arxiv.org/pdf/1605.06431.pdf>https://arxiv.org/pdf/1605.06431.pdf</a></li></ul></li><li><p>참고 블로그</p><ul><li>투빅스 : <a href=https://tobigs.gitbook.io/tobigs/deep-learning/computer-vision/resnet-deep-residual-learning-for-image-recognition>https://tobigs.gitbook.io/tobigs/deep-learning/computer-vision/resnet-deep-residual-learning-for-image-recognition</a></li><li><a href=https://blog.kubwa.co.kr/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-deep-residual-learning-for-image-recognition-2015-%EA%B0%84%EB%8B%A8%ED%95%9C-%EC%8B%A4%EC%8A%B5%EC%BD%94%EB%93%9C-a55cb68981b1>https://blog.kubwa.co.kr/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-deep-residual-learning-for-image-recognition-2015-%EA%B0%84%EB%8B%A8%ED%95%9C-%EC%8B%A4%EC%8A%B5%EC%BD%94%EB%93%9C-a55cb68981b1</a></li></ul></li><li><p>참고 유튜브</p><ul><li>혁펜하임 : <a href="https://www.youtube.com/watch?v=Fypk0ec32BU&amp;list=PL_iJu012NOxd_lWvBM8RfXeB7nPYDwafn&amp;index=14&amp;t=0s">https://www.youtube.com/watch?v=Fypk0ec32BU&amp;list=PL_iJu012NOxd_lWvBM8RfXeB7nPYDwafn&amp;index=14&amp;t=0s</a></li></ul></li></ul><h2 id=모델-나오게-된-배경-1>모델 나오게 된 배경<a hidden class=anchor aria-hidden=true href=#모델-나오게-된-배경-1>#</a></h2><p><strong>depth가 단순히 깊어지기만 해서 모델이 좋아지지는 않는다.(순전히 오버피팅이 아닌게, 훈련 셋의 정확도 마저도 낮다.)</strong><br></p><h2 id=모델-구조-1>모델 구조<a hidden class=anchor aria-hidden=true href=#모델-구조-1>#</a></h2><p>아래의 block을 마구마구 쌓은 구조를 갖는다.<br></p><p>논문에서 말하고자하는 바는 다음과 같다.<br>우리가 얻길 원하는 이상적인 함수 $H(x)$가 있다고하면, $F(x)$를 다음과 같이 정의한다.</p><p>$F(x) := H(x) - x$</p><p>이 때, 이상적인 함수가 $H(x) \simeq x$를 만족하면 좋겠다고 이야기함.</p><p><strong>why? 아마도, layer가 깊어지면 깊어질 수록 값들 변화하는 횟수가 많아지는데, 그 변동 폭이 너무 크다면 오버피팅의 위험도 있고, 학습하는 것에 어려움이 있을 것이다.</strong></p><p>즉, identity 함수와 비슷해지도록 학습하는 것이 좋다..!!<br></p><p>가정 : non linear layer들이 점진적으로 복잡한 함수로 접근해나갈 것이다.($H(x) \simeq x$)</p><p>그렇다면, 이러한 상황에서 residual connection을 하면 왜 학습이 잘되는가?</p><p><strong>non linear layer를 포함한 함수 $F(x)$를 가지고, identity mapping을 하는 것보다, zero mapping을 하도록 optimizing하는 것이 더 쉽기 때문이다.</strong></p><p>선형 레이어들의 경우에는 초깃값을 단위행렬 이런거로 맞추면 가능하겠지만, 비선형 레이어의 경우에는 초깃값을 어떻게 설정할지도 의문이고, 그럴 바에야 어차피 초깃값 0 근처로 잡아버리고 그 근처로 optiminze하는 것이 훨씬 쉽다.</p><p><strong>Residual Learning!</strong></p><p>기존의 plain network의 경우에는 층이 깊어졌을 때, error가 증가하는 모습을 볼 수 있다.<br>하지만, resnet의 경우에는 층이 깊어지면 error가 내려간다.</p><h2 id=resnet의-다른-해석앙상블>ResNet의 다른 해석(앙상블)<a hidden class=anchor aria-hidden=true href=#resnet의-다른-해석앙상블>#</a></h2><ul><li>(2016)Residual Networks Behave Like Ensembles of
Relatively Shallow Networks</li></ul><p>이처럼 앙상블로 볼 수 있다는 이야기와, layer들을 몇개 제거해봤을 때,<br>resnet은 성능에 큰 차이가 없으나 vggnet은 거의 제대로된 기능을 하지 못한다.</p><h1 id=mobile-net>Mobile Net<a hidden class=anchor aria-hidden=true href=#mobile-net>#</a></h1><ul><li>논문<ul><li><a href=https://arxiv.org/pdf/1704.04861.pdf>https://arxiv.org/pdf/1704.04861.pdf</a></li></ul></li><li>참고 블로그<ul><li><a href=https://deep-learning-study.tistory.com/532>https://deep-learning-study.tistory.com/532</a></li><li><a href=https://velog.io/@twinjuy/MobileNet-%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0>https://velog.io/@twinjuy/MobileNet-%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0</a></li></ul></li></ul><h2 id=모델-나오게-된-배경-2>모델 나오게 된 배경<a hidden class=anchor aria-hidden=true href=#모델-나오게-된-배경-2>#</a></h2><p><strong>mobile, embedded vision network에 적용하기 위해 모델을 경량화 시켜야 한다.</strong><br>즉, 저용량 메모리 환경에서 accuracy와 latency(깊이)의 균형을 잘 찾기 위함</p><h2 id=모델-구조-2>모델 구조<a hidden class=anchor aria-hidden=true href=#모델-구조-2>#</a></h2><p><strong>Depthwise Separable Convolution</strong></p><p>해당 모델은 Depthwise Convolution을 거치고, Pointwise Convolution을 결합한 것이다.</p><p>(a)를 보면, 일반적인 conv filter이다.<br>kernel size를 $D_K \cdot D_K$이라하고, input channel을 M, output channel을 N이라하고 결과 feature map size를 $D_F \cdot D_F$라 하면 다음의 컴퓨팅 cost가 나온다.<br>$$D_K \cdot D_K \cdot M \cdot N \cdot D_F \cdot D_F$$</p><p>(b)는 depthwise conv filter이다.<br>$D_K \cdot D_K \cdot 1$size의 filter가 이전 layer의 각각의 channel에 conv 연산을 수행해준다. 따라서 컴퓨팅 cost는 다음과 같다.
$$D_K \cdot D_K \cdot M \cdot D_F \cdot D_F$$</p><p>(c)는 pointwise conv filter이다.<br>이전 layer에 대해서 M의 input channel, N의 output channel을 얻도록 해주려 한다. 또한 feature map의 크기가 $D_F \cdot D_F$로 나오고 1x1으로 곱해주므로 다음의 컴퓨팅 cost를 갖는다.
$$M \cdot N \cdot D_F \cdot D_F$$</p><p>최종적으로 classic한 conv layer와의 computation cost를 비교해보면 다음과 같다.<br>$$\cfrac{D_K \cdot D_K \cdot M \cdot D_F \cdot D_F + M \cdot N \cdot D_F \cdot D_F}{D_K \cdot D_K \cdot M \cdot N \cdot D_F \cdot D_F} = \cfrac{1}{N} + \cfrac{1}{D_K^2}$$</p><p>3x3 kernel을 쓰게되면, 기존에 비해 8~9배정도 computation을 덜 하게 된다고 한다.</p><p>여기에 추가적으로 width/resolution Multiplier를 적용해 더욱 경량화를 시킬 수 있었다. 최종적인 식은 다음과 같다.
$$D_K \cdot D_K \cdot \alpha M \cdot \rho D_F \cdot \rho D_F + \alpha M \cdot \alpha N \cdot \rho D_F \cdot \rho D_F$$</p><p>$\alpha$는 0.25, 0.5, 0.75, 1 으로 width multiplier<br>$\rho$는 resolution multiplier<br>각각 $\alpha^2, \rho^2$만큼의 computational cost를 절약시켜줬다.</p><p>정확도는 기존의 GoogLeNet과 VGG16과 비교해 크게 차이 없었고, 연산량은 확연히 줄어들었음을 알 수 있다.</p><h1 id=senet>SeNet<a hidden class=anchor aria-hidden=true href=#senet>#</a></h1><ul><li>논문<ul><li><a href=https://arxiv.org/pdf/1709.01507.pdf>https://arxiv.org/pdf/1709.01507.pdf</a></li></ul></li><li>참고 블로그<ul><li><a href=https://inhovation97.tistory.com/48>https://inhovation97.tistory.com/48</a></li><li><a href=https://deep-learning-study.tistory.com/539>https://deep-learning-study.tistory.com/539</a></li></ul></li></ul><h2 id=모델-나오게-된-배경-3>모델 나오게 된 배경<a hidden class=anchor aria-hidden=true href=#모델-나오게-된-배경-3>#</a></h2><p><strong>feature map의 채널마다의 relationship을 이용하면 성능을 끌어올릴 수 있지 않을까?</strong></p><h2 id=모델-구조-3>모델 구조<a hidden class=anchor aria-hidden=true href=#모델-구조-3>#</a></h2><p><strong>Squeeze & Excitation</strong></p><p><strong>1. Squeeze(Global Information Embedding</strong>)<br>기존의 input $X$에 $F_{tr}$라는 연산을 거쳐 $H \cdot W \cdot C$의 형태를 가진 feature map($U$)에 대해서 각각의 channel에 대해 squeeze(<strong>global average pooling</strong>) 작업을 해준다.</p><p>수식으로 표현하면 다음과 같다.
$$z_c = F_{sq}(u_c) = \cfrac{1}{H * W}\sum_{i=1}^H \sum_{j=1}^W{u_c(i, j)}$$</p><p><strong>2. Excitation(Adaptive Recalibration)</strong><br>앞의 squeeze에서 모아놓은 정보에다가 채널마다의 정보 의존성을 넣는다. 두가지 조건을 만족해야 하는데, flexible해야하고 non mutually exclusive해야 한다.(유연 + 상호의존적) 그러기 위해, fc-relu-fc-sigmoid 순으로 layer를 거쳐준다.<br>이 때, reduction ratio(r)만큼으로 bottle neck크기를 설정한다.(16으로 제안)</p><p>수식으로 표현하면 다음과 같다.<br>$$s = F_{ex}(z, W) = \sigma(g(z, W)) = \sigma(W_2\delta(W_1z))$$</p><p>마지막으로 원래의 feature map $u_c$에 곱해주어 결론적으로 각각의 feature마다의 중요도? 만큼씩 scaling된다고 볼 수 있다.</p><p>$$\tilde{x}<em>c = F</em>{scale}(u_c, s_c) = s_cu_c$$</p><p>즉, 짜내서 feature 정보를 embedding하고 반응성에 따라 재교정한다 => Squeeze & Excitation</p><p><strong>think</strong><br>어떻게 보면, feature map에 대해서 self-attention하는 것과 비슷한 효과를 내는 것같다.(channel끼리 내적하는 대신에 layer들을 통해 channel끼리의 관계를 파악한다)</p><h1 id=efficient-net>Efficient Net<a hidden class=anchor aria-hidden=true href=#efficient-net>#</a></h1><ul><li>논문<ul><li><a href=https://arxiv.org/pdf/1905.11946.pdf>https://arxiv.org/pdf/1905.11946.pdf</a></li></ul></li><li>참고 블로그<ul><li><a href=https://blog.kubwa.co.kr/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-efficientnet-rethinking-model-scaling-for-convolutional-neural-networks-2019-%EA%B0%84%EB%8B%A8%ED%95%9C-%EC%8B%A4%EC%8A%B5%EC%BD%94%EB%93%9C-cbe0e9963ffc>https://blog.kubwa.co.kr/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-efficientnet-rethinking-model-scaling-for-convolutional-neural-networks-2019-%EA%B0%84%EB%8B%A8%ED%95%9C-%EC%8B%A4%EC%8A%B5%EC%BD%94%EB%93%9C-cbe0e9963ffc</a></li></ul></li></ul><h2 id=모델-나오게-된-배경-4>모델 나오게 된 배경<a hidden class=anchor aria-hidden=true href=#모델-나오게-된-배경-4>#</a></h2><p><strong>기존에 나온 모델의 size가 과연 무엇이 최적일까</strong></p><p>해당 논문에서 크게 3가지 scaling에 초점을 맞춘다.<br></p><ol><li><p>depth(모델 깊이)<br>기존에 ResNet같은 경우 ResNet-18 ~ ResNet-1000 까지 많은 층을 쌓으며 모델의 성능을 끌어올렸음 을 알 수 있다. 그런데, 성능을 보았을 때, 101층짜리와 1000층짜리의 정확도에서 큰 차이가 없었다.</p></li><li><p>width(채널 수)<br>주로 작은 모델에 대해 사용하는 scaling인데, 마찬가지로 3.8과 5.0을 비교해보면 큰 차이가 없다</p></li><li><p>resolution(이미지해상도)<br>마찬가지로 해상도가 높아질수록 성능은 올라가지만 그 정도가 점점 감소함을 알 수 있다.</p></li></ol><h2 id=모델-구조-4>모델 구조<a hidden class=anchor aria-hidden=true href=#모델-구조-4>#</a></h2><p>Basline network이 되는 EfficientNet-B0부터 시작하여 오른쪽의 수식에 따라 d, w, r을 바꿔가며 모델을 실험해나간다.</p><p>이 때, MBConv는 Mobile block을 의미하고<br>각각의 layer마다 SE 작업을 해주었다.</p><ol><li>3가지 하이퍼파라미터에 대해 기준을 두고 grid search를 한다.</li><li>그 값을 기준으로 모델의 전체적인 크기를 늘린다.</li></ol><p>즉, 최적의 모양을 먼저잡고 그것을 동일한 비율로 잡아 늘렸다고 보면된다.<br>(물론, 각각의 3가지 scale이 독립적이지 않아서 이렇게 늘리는 것이 최선은 아니겠지만, test하는 것에 시간이 너무 오래 소요되어서 차선으로 이 방법을 택한 듯 하다.)</p><p>기존의 비슷한 성능을 내는 network들과 비교했을 때 parameter수와 FLOPs(계산량)수가 현저히 낮고, B7 모델은 sota를 달성했다.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://dobe0715.github.io/posts/classifier-free-diffusion-guidance/><span class=title>« Prev</span><br><span>Classifier-free Diffusion Guidance review</span></a>
<a class=next href=https://dobe0715.github.io/posts/convnext/><span class=title>Next »</span><br><span>ConvNeXt review</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share CNN 모델 정리 on twitter" href="https://twitter.com/intent/tweet/?text=CNN%20%eb%aa%a8%eb%8d%b8%20%ec%a0%95%eb%a6%ac&amp;url=https%3a%2f%2fdobe0715.github.io%2fposts%2fclassification-cnn-models%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share CNN 모델 정리 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdobe0715.github.io%2fposts%2fclassification-cnn-models%2f&amp;title=CNN%20%eb%aa%a8%eb%8d%b8%20%ec%a0%95%eb%a6%ac&amp;summary=CNN%20%eb%aa%a8%eb%8d%b8%20%ec%a0%95%eb%a6%ac&amp;source=https%3a%2f%2fdobe0715.github.io%2fposts%2fclassification-cnn-models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share CNN 모델 정리 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdobe0715.github.io%2fposts%2fclassification-cnn-models%2f&title=CNN%20%eb%aa%a8%eb%8d%b8%20%ec%a0%95%eb%a6%ac"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share CNN 모델 정리 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdobe0715.github.io%2fposts%2fclassification-cnn-models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share CNN 모델 정리 on whatsapp" href="https://api.whatsapp.com/send?text=CNN%20%eb%aa%a8%eb%8d%b8%20%ec%a0%95%eb%a6%ac%20-%20https%3a%2f%2fdobe0715.github.io%2fposts%2fclassification-cnn-models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share CNN 모델 정리 on telegram" href="https://telegram.me/share/url?text=CNN%20%eb%aa%a8%eb%8d%b8%20%ec%a0%95%eb%a6%ac&amp;url=https%3a%2f%2fdobe0715.github.io%2fposts%2fclassification-cnn-models%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share CNN 모델 정리 on ycombinator" href="https://news.ycombinator.com/submitlink?t=CNN%20%eb%aa%a8%eb%8d%b8%20%ec%a0%95%eb%a6%ac&u=https%3a%2f%2fdobe0715.github.io%2fposts%2fclassification-cnn-models%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://dobe0715.github.io>블로그 홈</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>